\documentclass[a4paper,11pt,twoside]{report}
% THIS FILE SHOULD BE COMPILED BY pdfLaTeX

% ----------------------   PREAMBLE PART ------------------------------

% ------------------------ ENCODING & LANGUAGES ----------------------

\usepackage{fontspec}
\usepackage[polish, english]{babel} % Ostatni język to język GŁÓWNY pracy (English)
\usepackage{bbm}


\usepackage{amsmath, amsfonts, amsthm, latexsym, bm} % MOSTLY MATHEMATICAL SYMBOLS

\usepackage[final]{pdfpages} % INPUTING TITLE PDF PAGE - GENERATE IT FIRST!
%\usepackage[backend=bibtex, style=verbose-trad2]{biblatex}

\usepackage{tabularx, booktabs, xcolor, float} % FOR TABLES

\usepackage{commath} % various commands which can make writing math expressions easier --- documentation available at: https://ctan.gust.org.pl/tex-archive/macros/latex/contrib/commath/commath.pdf

\usepackage[hidelinks]{hyperref} % for hyperlinks, for example, urls, references to equations, entries in a bibliography --- hidelinks option removes rectangles around hiperlinks


% ---------------- MARGINS, INDENTATION, LINESPREAD ------------------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry} % MARGINS


\linespread{1.5}
\allowdisplaybreaks         % ALLOWS BREAKING PAGE IN MATH MODE

\usepackage{indentfirst}    % IT MAKES THE FIRST PARAGRAPH INDENTED; NOT NEEDED
\setlength{\parindent}{5mm} % WIDTH OF AN INDENTATION


%---------------- RUNNING HEAD - CHAPTER NAMES, PAGE NUMBERS ETC. -------------------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
% PAGINATION: LEFT ALIGNMENT ON EVEN PAGES, RIGHT ALIGNMENT ON ODD PAGES
\fancyfoot[LE,RO]{\thepage}
% RIGHT HEADER: zawartość \rightmark do lewego, wewnętrznego (marginesu)
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
% lewa pagina: zawartość \leftmark do prawego, wewnętrznego (marginesu)
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}

% HEAD RULE - IT'S A LINE WHICH SEPARATES HEADER AND FOOTER FROM CONTENT
\renewcommand{\headrulewidth}{0 pt} % 0 MEANS NO RULE, 0.5 MEANS FINE RULE, THE BIGGER VALUE THE THICKER RULE


\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[LE,RO]{\thepage}

  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0.0pt}
}



% --------------------------- CHAPTER HEADERS ---------------------

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt}


\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% ----------------------- TABLE OF CONTENTS SETUP ---------------------------

\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


% THIS MAKES DOTS IN TOC FOR CHAPTERS
\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries \thecontentslabel.\quad}% <numbered-entry-format>
  {\bfseries}% <numberless-entry-format>
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}% <filler-page-format>

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% ---------------------- TABLES AD FIGURES NUMBERING ----------------------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}


% ------------- DEFINING ENVIRONMENTS FOR THEOREMS, DEFINITIONS ETC. ---------------

\makeatletter
\newtheoremstyle{definition}
{3ex}%                           % Space above
{3ex}%                           % Space below
{\upshape}%                      % Body font
{}%                              % Indent amount
{\bfseries}%                     % Theorem head font
{.}%                             % Punctuation after theorem head
{.5em}%                          % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\makeatother

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% --------------------- END OF PREAMBLE PART (MOSTLY) --------------------------





% -------------------------- USER SETTINGS ---------------------------

\newcommand{\tytul}{Dostosowanie biblioteki gips do zadania klasyfikacji przy pomocy analizy dyskryminacyjnej – gipsDA}
\renewcommand{\title}{Adapting the gips library for classification problem utilizing discriminant analysis – gipsDA}
\newcommand{\type}{Engineer} % Master OR Engineer
\newcommand{\supervisor}{MSc Eng. Adam Chojecki\\
    PhD Bartosz Kołodziejek, Assoc. Prof.} % TITLE AND NAME OF THE SUPERVISOR



\begin{document}
\sloppy
\selectlanguage{english}

\includepdf[pages=-]{titlepage-en} % THIS INPUTS THE TITLE PAGE

\null\thispagestyle{empty}\newpage

% ------------------ PAGE WITH SIGNATURES --------------------------------

%\thispagestyle{empty}\newpage
%\null
%
%\vfill
%
%\begin{center}
%\begin{tabular}[t]{ccc}
%............................................. & \hspace*{100pt} & .............................................\\
%supervisor's signature & \hspace*{100pt} & author's signature
%\end{tabular}
%\end{center}
%


% ---------------------------- ABSTRACTS -----------------------------

{  \fontsize{12}{14} \selectfont
\begin{abstract}

\begin{center}
\title
\end{center}
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\\

\noindent \textbf{Keywords:} keyword1, keyword2, ...
\end{abstract}
}

\null\thispagestyle{empty}\newpage


{\selectlanguage{polish} \fontsize{12}{14}\selectfont
\begin{abstract}

\begin{center}
\tytul
\end{center}

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\\

\noindent \textbf{Słowa kluczowe:} słowo klucz 1, słowo klucz 2, zażółć gęślą jaźń...
\end{abstract}
}


%% --------------------------- DECLARATIONS ------------------------------------
%
%%
%%	IT IS NECESSARY OT ATTACH FILLED-OUT AUTORSHIP DEECLRATION. SCAN (IN PDF FORMAT) NEEDS TO BE PLACED IN scans FOLDER AND IT SHOULD BE CALLED, FOR EXAMPLE, DECLARATION_OF_AUTORSHIP.PDF. IF THE FILENAME OR FILEPATH IS DIFFERENT, THE FILEPATH IN THE NEXT COMMAND HAS TO BE ADJUSTED ACCORDINGLY.
%%
%%	command attacging the declarations of autorship
%%
%\includepdf[pages=-]{scans/declaration-of-autorship}
%\null\thispagestyle{empty}\newpage
%
%% optional declaration
%%
%%	command attaching the declaataration on granting a license
%%
%\includepdf[pages=-]{scans/declaration-on-granting-a-license}
%%
%%	.tex corresponding to the above PDF files are present in the 3. declarations folder
%
\null\thispagestyle{empty}\newpage
% ------------------- TABLE OF CONTENTS ---------------------
% \selectlanguage{english} - for English
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}
\newpage % IF YOU HAVE EVEN QUANTITY OD PAGES OF TOC, THEN REMOVE IT OR ADD \null\newpage FOR DOUBLE BLANK PAGE BEFORE INTRODUCTION


% -------------------- THE BODY OF THE THESIS --------------------------------

\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{11}

% ======================================================================
%                          Introduction
% ======================================================================

\chapter{Introduction}

Statistical classification is a fundamental problem in machine learning and data analysis, with applications ranging from genomic sequencing and medical diagnosis to financial forecasting. Among the vast landscape of classification algorithms, generative models such as Linear and Quadratic Discriminant Analysis (LDA and QDA) remain cornerstones of the field due to their rigorous statistical foundations, interpretability, and computational efficiency. However, the modern era of data science frequently confronts these classical methods with high-dimensional datasets—scenarios where the number of features rivals or exceeds the number of observations. In such environments, standard estimation techniques often fail, necessitating the development of more robust, regularized approaches.

This thesis proposes a novel solution to this challenge by integrating the principles of invariant theory with discriminant analysis. Specifically, we explore how identifying and enforcing permutation symmetries within the data can serve as a powerful form of model-based regularization, stabilizing covariance estimation without resorting to arbitrary shrinkage.

This chapter provides the necessary context for the project and is organized into four main sections:
\begin{itemize}
    \item \textbf{Section \ref{sec:motivation}: Motivation and Goal} identifies the specific limitations of classical classifiers in high-dimensional settings, particularly the issues of parameter instability and singularity, and defines the primary objective of this thesis.
    \item \textbf{Section \ref{sec:symmetry}: Why do we expect symmetry?} provides the rationale behind the proposed solution, explaining why permutation invariance is a plausible and valuable assumption in many real-world datasets.
    \item \textbf{Section \ref{sec:da_desc}: General Description of Discriminant Analysis} briefly establishes the theoretical baseline, outlining the mechanics of LDA and QDA that will be modified in our work.
    \item \textbf{Section \ref{sec:contribution}: Contribution} summarizes the specific achievements of this project, including the development of the \texttt{gipsDA} package and the implementation of novel estimators.
\end{itemize}

\section{Motivation and Goal}
\label{sec:motivation}

Classical generative classifiers, such as Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), remain fundamental tools in statistical machine learning\cite{elemstats} due to their interpretability and closed-form solutions.
However, the performance of these methods is critically dependent on the accurate estimation of the class-conditional covariance matrices, denoted as $\boldsymbol{\Sigma}$.
In the era of \("\)Big Data\("\), researchers frequently encounter high-dimensional datasets where the number of features ($p$) is large relative to the number of available observations ($n$).
In such settings, standard estimators often fail, rendering classical methods suboptimal or entirely inapplicable.

The bottleneck of these algorithms lies in the sample covariance matrix, $\hat{\boldsymbol{\Sigma}}$.
For a dataset with $p$ features, the covariance matrix contains $p(p+1)/2$ unique parameters that must be estimated.
This implies that the model complexity grows quadratically with the dimension of the feature space ($\Theta(p^2)$).
This phenomenon leads to two primary challenges:

\begin{enumerate}
    \item \textbf{Parameter Instability:} When $p$ is large, the number of parameters to estimate can easily exceed the information content available in the training data. This results in estimates with high variance, leading to overfitting and poor generalization performance on unseen data.
    \item \textbf{Estimator Singularity:} In the extreme regime where the number of features exceeds the sample size ($p > n$), the sample covariance matrix $\hat{\boldsymbol{\Sigma}}$ becomes rank-deficient. Consequently, it is no longer positive definite and, crucially, not invertible. Since the decision functions for both LDA and QDA require the computation of the precision matrix $\hat{\boldsymbol{\Sigma}}^{-1}$, classical algorithms mathematically break down in these \("\)small $n$, large $p$\("\) scenarios.
\end{enumerate}

To mitigate these issues, regularization techniques are necessary.
While standard approaches often involve scalar shrinkage (e.g., shrinking towards the identity matrix)\cite{friedman1989regularized, ledoit2004well}, this project explores a structural approach.
We posit that by identifying and enforcing permutation symmetries within the data—where certain variables are exchangeable—we can significantly reduce the number of free parameters required to estimate the covariance structure.

\subsection*{Goal of the Thesis}

The primary objective of this project is to develop \texttt{gipsDA}, a novel R library that adapts discriminant analysis for high-dimensional environments.
By integrating the methodology of the \texttt{gips} (Gaussian model Invariant by Permutation Symmetry) package, we aim to construct robust classifiers that impose statistically justified symmetry constraints on the covariance matrix.

Specifically, the goal is to provide a solution that:
\begin{itemize}
    \item Maintains high classification accuracy in high-dimensional settings where standard LDA/QDA overfit.
    \item Remains mathematically valid and computationally stable in the $p > n$ regime by ensuring the estimated matrices are positive definite through symmetry projection.
    \item Offers a user-friendly interface compatible with the standard R ecosystem, bridging the gap between advanced theoretical statistics and practical data science applications.
\end{itemize}

\section{Why do we expect symmetry?}
\label{sec:symmetry}

At first glance, the assumption that distinct features in a dataset share a permutation symmetry—meaning they remain invariant under specific reorderings—may appear overly restrictive\cite{andersson1998symmetry}. In classical multivariate analysis, features are often treated as distinct entities with unique variances and pairwise correlations. However, in the context of high-dimensional data, the assumption of symmetry is not only mathematically convenient but often empirically justified by the underlying nature of the data generation process.

The core justification lies in the concept of \textit{exchangeability}. In many modern datasets, features are not arbitrary measurements but rather collections of comparable units. Consider the following domains where symmetry naturally arises:

\begin{itemize}
    \item \textbf{Genomics and Bioinformatics:} In gene expression data, thousands of genes are measured simultaneously \cite{dudoit2002comparison}. Genes belonging to the same biological pathway or functional group often exhibit similar behavior. It is statistically plausible to assume that the correlation between any pair of genes within such a functional cluster is roughly equivalent, regardless of their specific labels.

    \item \textbf{Medical Diagnostics and Imaging:} Consider the analysis of digitized images for cancer diagnosis, such as the Breast Cancer Wisconsin dataset used in this work\cite{breast}. Features are computed from the characteristics of cell nuclei present in the image (e.g., radius, texture, smoothness) \cite{street1993nuclear}. While each feature measures a distinct geometric property, groups of features derived from similar morphological aspects often display strong, structured correlations. Assuming a symmetric covariance structure among these related biometrics allows the model to capture the general "shape" of malignant cells without overfitting to the noise inherent in individual sample measurements.

    \item \textbf{Industrial Sensor Systems:} In predictive maintenance and fault detection, systems are often monitored by multiple sensors recording analogous physical quantities. A prime example is the Sensorless Drive Diagnosis dataset\cite{dataset_for_sensorless_drive_diagnosis_325}, which involves electric current signals from different phases of a motor \cite{seera2014classification}. In a balanced three-phase system, the physical properties of the phases are designed to be identical. Consequently, the statistical relationship between Phase A and Phase B should theoretically be invariant to the relationship between Phase B and Phase C. Treating these signals as exchangeable via permutation symmetry reflects the physical reality of the hardware design.
\end{itemize}

Furthermore, the approach proposed in this thesis does not require prior knowledge of these specific relationships. Unlike rigid regularization methods that force the covariance matrix towards a specific target (such as the identity matrix), the \texttt{gips} methodology is designed to \textit{discover} the optimal symmetry structure from the data itself\cite{JSSv112i07}.

By allowing the data to reveal its own invariant structure, we adhere to the principle of parsimony (Occam's razor). If a complex covariance matrix with $\Theta(p^2)$ parameters can be accurately approximated by a symmetric structure requiring only $\Theta(p)$ parameters, the latter model is preferred. It reduces the variance of the estimator and mitigates the risk of overfitting, which is the primary enemy in high-dimensional classification. Thus, expecting symmetry is not about imposing an artificial constraint, but rather about recognizing and exploiting the redundancy inherent in large-scale systems.

\section{General description of Discriminant Analysis}
\label{sec:da_desc}
\textcolor{red}{jakis wprowadzenie ze mozna klasyfikacje robic na podstawie tego ze widzimy symetrie w macierzy kowariancji}

\section{Contribution}
\label{sec:contribution}

The central contribution of this thesis is the proposal and implementation of a novel regularization framework for Discriminant Analysis, specifically designed for the "High Dimension, Low Sample Size" (HDLSS) regime where $p \gg n$. In such settings, classical estimators for the covariance matrix become singular and unstable \cite{bickel2004some}.

While numerous modifications to LDA and QDA have been proposed to address this instability—ranging from the scalar shrinkage parameters of Regularized Discriminant Analysis (RDA) \cite{friedman1989regularized} to sparsity-enforcing methods like Penalized LDA \cite{witten2011penalized} or independence assumptions in Diagonal LDA \cite{dudoit2002comparison}—these approaches often impose arbitrary constraints. They typically force the covariance structure towards a diagonal matrix or sparse representation, potentially discarding significant correlations between features.

In contrast, our approach introduces a paradigm shift by utilizing \textit{permutation symmetry} as a form of model-based regularization. Instead of assuming correlations are zero (sparsity) or uniform (scalar shrinkage), we assume that groups of features may be exchangeable. This allows the data to dictate its own complexity through the discovery of invariant structures.

The specific contributions of this work are as follows:

\begin{itemize}
    \item \textbf{Development of the \texttt{gipsDA} Package:} We provide a fully functional R package that integrates the \texttt{gips} methodology with the standard discriminant analysis workflow. The package is designed to be API-compatible with the widely used \texttt{MASS} library, ensuring immediate accessibility for practitioners.

    \item \textbf{Novel Covariance Estimators:} We introduce three distinct classification models that leverage symmetry in different ways:
    \begin{itemize}
        \item \texttt{gipsLDA}: Adapts the homoscedastic assumption of LDA by projecting the pooled covariance matrix onto a symmetric permutation group.
        \item \texttt{gipsQDA}: Allows for full heteroscedasticity, optimizing a unique permutation structure for each class independently.
        \item \texttt{gipsMultQDA}: A hybrid approach that allows for different covariance matrices per class but constrains them to share the \textit{same} underlying permutation symmetry. This required the development of the \texttt{gipsmult} module to optimize a joint posterior probability across multiple groups.
    \end{itemize}

    \item \textbf{Empirical Validation in Data-Scarce Regimes:} Through extensive simulation studies and benchmarks on real-world datasets (e.g., biomedical and industrial sensor data), we demonstrate that imposing symmetry constraints significantly reduces the estimation error and improves classification accuracy compared to standard QDA, particularly when the sample size is critically low relative to the dimensionality.
\end{itemize}

% ======================================================================
%                          Related Work
% ======================================================================

\chapter{Related Work}

This chapter establishes the theoretical framework and reviews the existing literature fundamental to the project.
The discussion is structured around two pivotal domains that form the basis of our proposed solution.

First, we revisit classical statistical machine learning algorithms for classification, specifically focusing on Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA).
These well-established methods serve as the foundation for the predictive models developed in this work\cite{elemstats}.

Second, we introduce the methodology underlying the \texttt{gips} (Gaussian model Invariant by Permutation Symmetry) package.
We explore the mathematical principles of identifying invariant structures within covariance matrices and how this approach can be utilized for parameter estimation in high-dimensional settings\cite{JSSv112i07}.
The integration of these two distinct areas, classical discriminant analysis and permutation symmetry discovery, constitutes the core contribution of this thesis.

\section{Theoretical description of LDA and QDA}

Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are generative classifiers that model the probability density of each class, $g$, using a multivariate Gaussian distribution, $\mathbbm{P}(x|g) = \mathcal{N}(x | \mu_g, \Sigma_g)$. The key difference between them lies in their assumptions regarding the class covariance matrices, $\Sigma_g$.

\begin{itemize}
    \item \textbf{Linear Discriminant Analysis (LDA)} assumes that all classes share a single, common covariance matrix, i.e., $\Sigma_g = \Sigma$ for all $g$. This assumption of homoscedasticity leads to a decision boundary that is a linear function of the input features $x$.
    \item \textbf{Quadratic Discriminant Analysis (QDA)} relaxes this assumption, allowing each class $g$ to have its own distinct covariance matrix, $\Sigma_g$. This assumption of heteroscedasticity results in a decision boundary that is a quadratic function of $x$, providing greater flexibility.
\end{itemize}

Prediction for a new observation $x$ is made by assigning it to the class $g$ that maximizes the posterior probability $\mathbbm{P}(g|x)$. Using Bayes' theorem, this is equivalent to maximizing the discriminant function, $\delta_g(x)$:
\begin{equation}
    \delta_g(x) = -\frac{1}{2} \log |\Sigma_g| - \frac{1}{2} (x - \mu_g)^T \Sigma_g^{-1} (x - \mu_g) + \log \pi_g,
\end{equation}
where $\mu_g$ is the mean vector for class $g$, $\Sigma_g$ is its covariance matrix, and $\pi_g$ is the prior probability of the class. In practice, these parameters are estimated from the training data. For a more detailed treatment of these methods, we refer the reader to "The Elements of Statistical Learning" \cite{elemstats}. Our work focuses on novel estimation techniques for the $\Sigma_g$ matrices.


\section{General description of gips}
\textcolor{red}{Tutaj opis gipsa, na podstawie artykulu itd}

% ======================================================================
%                          Solution Proposal
% ======================================================================

\chapter{Solution Proposal}
The central theme of our proposed solution is the enhancement of classical discriminant analysis methods by introducing a novel approach to estimating covariance matrices.
We modify the standard Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) algorithms by leveraging the methodology of the \texttt{gips} R package\cite{JSSv112i07}.
The core functionality of the \texttt{gips} library is to identify the most probable permutation group under which a given covariance matrix remains approximately invariant.
By imposing such a symmetry constraint, we effectively introduce a form of model-based regularization.
This reduces the number of free parameters that need to be estimated, which can be particularly advantageous when dealing with high-dimensional data or limited sample sizes, potentially leading to more stable and robust classifiers.

\section{gipsmult}
\subsection{Definitions}
\subsubsection{Symmetric group}
Let us set $p \in \mathbb{N}$. Let $\mathfrak{S}_p$ denote the symmetric group i.e. the set of all permutastions on $V = \{1,2, \dots, p\}$
with function composition as the operation.
\subsubsection{Invariance under action of a subgroup of the symmetric group}
Let $K \in \mathbb{R}^{p\times p}$ and $\Gamma \leq \mathfrak{S}_p$. We say that matrix $K$ is invariant under action of subgroup $\Gamma$ i.i.f
\begin{equation}
    \forall_{\sigma \in \Gamma} \forall_{i,j\in V} K_{i,j} = K_{\sigma(i), \sigma(j)}
\end{equation}
For conciseness sake in the following sections we will say that a matrix is \textit{invariant under subgroup} or, if supgroup $\Gamma = \langle \sigma \rangle$
is cyclic that a matrix is \textit{invariant under} $\sigma$.
\subsubsection{Invariant space and cone}
\begin{equation}
    \mathcal{Z}_\Gamma := \left\{S \in Sym(p;\mathbbm{R}) : S \ is \ invariant \ under \ \Gamma \right\},
\end{equation}
\begin{equation}
    \mathcal{P}_{\Gamma}:= Z_\Gamma \cap Sym^+(p;\mathbbm{R}).
\end{equation}

\subsection{\textt{gips} vs \texttt{gipsmult}}
\subsubsection{General comparision}
In the setting from \cite{10.1214/22-AOS2174} a single symmetric SPD matrix $K \in \mathbb{R}^{p\times p}$ is invariant under $\sigma \in \mathfrak{S}$
Let us set $m \in \mathbb{N}$, the model of \texttt{gipsmult} assumes entire set of symmetric SPD matrices $\mathcal{K} = \{K_1, K_2, \dots, K_m\}$ to be invariant under $\sigma$.
\subsubsection{Bayesian procedure}
Under assumption of uniform prior on the set
$
\mathcal{C} := \{ \langle \sigma \rangle : \sigma \in \mathfrak{S}_p \}
$
of cyclic subgroups of $\mathfrak{S}_p$ and Diaconis-Ylvisacker\cite{diaconis1979conjugate} one on the precision
matrix $K$, \cite{JSSv112i07} arrives at the following proportionality of posterior:
\begin{equation}\label{gipsposteriorformula}
    \mathbbm{P}\left(\Gamma = c \mid \mathbbm{X} = \mathbbm{x}\right) \propto \frac{I_c\left(\delta + n, D+U\right)}{I_c\left(\delta, D\right)}
\end{equation}
For more detailed explanation of this result see \cite{JSSv112i07}
\\
Under analogous assumptions the posterior in \texttt{gipsmult} setting is proportional to:
\begin{equation}\label{gipsmultposteriorformula}
    \mathbbm{P}\left(\Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right) \propto \prod_{g=1}^m\frac{I_c\left(\delta + n_g, D+U_g\right)}{I_c\left(\delta, D\right)},
\end{equation}
So the entire logic of Bayesian selection for \texttt{gipsmult} boils down to multiplication of results from \texttt{gips}.\\
We attach full derivation of \eqref{gipsmultposteriorformula} in the appendix A.


\section{Description of machine learning models}
\subsection{gipsQDA}
The \texttt{gipsQDA} model represents the most flexible approach within our framework. In this method, the \texttt{gips} library is applied independently to the data of each class. This process yields a unique estimated covariance matrix, $\hat{\Sigma}_g$, and a unique optimal permutation group, $\hat{c}_g$, for each class $g$.
\begin{equation}
    (\hat{\Sigma}_g, \hat{c}_g) = \texttt{gips}(\text{Data}_g) \quad \text{for each class } g=1, \dots, m
\end{equation}
This model is directly analogous to the classic QDA, as it allows for heteroscedasticity, but with the additional complexity that each class can exhibit its own distinct symmetry structure. Consequently, \texttt{gipsQDA} serves as a general model that contains all other proposed models as special, more constrained cases.

\subsection{gipsLDA}
The \texttt{gipsLDA} model adapts the core idea of LDA—a single, shared covariance matrix—to the \texttt{gips} framework. The general strategy is to first pool the covariance information from all classes into a single matrix, and then use \texttt{gips} to find a common symmetry structure for this pooled matrix. The process is as follows:
\begin{enumerate}
    \item For each class $g$, estimate the sample covariance matrix, $S_g$, using the standard unbiased estimator:
    \begin{equation}
        S_g = \frac{1}{n_g-1} \sum^{n_g}_{i=1} (x_i^{(g)} -\overline{x}^{(g)})(x_i^{(g)} -\overline{x}^{(g)})^T,
    \end{equation}
    where $n_g$ is the number of observations in class $g$, and $x_i^{(g)}$ and $\overline{x}^{(g)}$ are the $i$-th observation and the sample mean of class $g$, respectively.
    \item Combine the individual $S_g$ matrices into a single pooled matrix, $S$, using a specific averaging method.
    \item Supply the pooled matrix $S$ to the \texttt{gips} algorithm to find the optimal permutation group $\hat{c}$ and the projected covariance matrix $\hat{\Sigma}_{\text{gipsLDA}}$.
    \item Use the resulting matrix $\hat{\Sigma}_{\text{gipsLDA}}$ as the shared covariance matrix in the LDA classification framework, assuming each class has a different mean vector $\mu_g$.
\end{enumerate}
We propose two methods for pooling the covariance matrices.

\subsubsection{gipsLDA classic}
This method uses the classic pooled covariance estimator, which is the standard approach in traditional LDA. The pooled matrix $S$ is defined as:
\begin{equation}
    S = \frac{1}{n-m}\sum_{g=1}^m (n_g-1)S_{g}.
\end{equation}
By substituting the formula for $S_g$ from Equation (3), this simplifies to the well-known pooled sample covariance matrix formula:
\begin{equation}
S = \frac{1}{n-m}\sum_{g=1}^m \sum^{n_g}_{i=1} (x_i^{(g)} -\overline{x}^{(g)})(x_i^{(g)} -\overline{x}^{(g)})^T.
\end{equation}
This estimator is an unbiased estimate of the common covariance matrix under the assumption of homoscedasticity.

\subsubsection{gipsLDA weighted average}
This method is named "weighted average" because the contribution of each class's covariance matrix, $S_g$, to the final pooled matrix is weighted by its sample size, $n_g$. However, this weighting scheme gives more influence to smaller classes compared to the \texttt{gipsLDA classic} approach. The pooled matrix $S$ is calculated as:
\begin{equation}
    S = \frac{1}{n}\sum_{g=1}^m n_g S_{g},
\end{equation}
where $n = \sum_{g=1}^m n_g$ is the total number of observations and $m$ is the number of classes.


\subsection{gipsMultQDA}
The \texttt{gipsMultQDA} model is an intermediate approach between the full flexibility of \texttt{gipsQDA} and the strong constraints of \texttt{gipsLDA}.
It is supposed to serve the situation when each class has a unique covariance matrix (like QDA), but all these
matrices share the same underlying permutation symmetry. This is useful for scenarios where classes  differ
in scale or variance but are expected to share a common dependency structure.



\section{Theoretical aspects and pseudocode}

% ======================================================================
%                          Implementation
% ======================================================================

\chapter{Implementation (Package Description)}

\section{Package structure and dependencies}

\section{Documentation}

\section{Usage of S3 classes}

\section{Usage example}
\textcolor{red}{Tutaj chyba tak jak na prezentacji by sie to przedstawilo}

% ======================================================================
%                          Testing
% ======================================================================

\chapter{Testing Methodology}
\label{ch:testing_methodology}

This chapter presents the comprehensive evaluation methodology employed to assess the performance and robustness of the proposed \texttt{gipsDA} framework. To ensure a rigorous validation, the assessment strategy is twofold, encompassing controlled experiments on synthetic data and empirical benchmarking on diverse real-world datasets.

First, we detail the procedures for the \textbf{synthetic data analysis}. This includes a description of the generative algorithms used to simulate high-dimensional data with specific covariance structures, the methodology for constructing performance visualization curves, and the statistical framework applied to quantify the significance of the results.

Subsequently, we outline the approach for \textbf{real-world data benchmarking}. This section describes the necessary data preparation and preprocessing pipelines, explains the visualization techniques used to monitor model convergence, and provides a brief characterization of the specific datasets selected for this study.

In both experimental scenarios, the comparative analysis focuses on the performance of six distinct classification models. These include four variants of the proposed framework:
\begin{itemize}
    \item \texttt{gipsLDA} with weighted averaging (denoted in plots as \texttt{gipsldawa}),
    \item \texttt{gipsLDA} utilizing the classic estimator (\texttt{gipsldacl}),
    \item \texttt{gipsQDA} (\texttt{gipsqda}),
    \item \texttt{gipsMultQDA} (\texttt{gipsmultqda}).
\end{itemize}
These are benchmarked against two baseline models, referred to as \texttt{LDAmod} and \texttt{QDAmod}.
These baselines represent standard Linear and Quadratic Discriminant Analysis, respectively, augmented with a specific regularization technique.
This modification ensures numerical stability by constraining the smallest eigenvalue of the covariance matrix to a minimum threshold of $0.05$, a procedure that will be detailed in subsequent sections.

To quantitatively assess and compare the predictive power of these models, we employ \textbf{Classification Accuracy} as the primary evaluation metric. For a given test dataset consisting of $N$ observations, let $y_i$ denote the true class label of the $i$-th sample, and $\hat{y}_i$ denote the corresponding label predicted by the model. The accuracy is formally defined as:

\begin{equation}
    \text{Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(y_i = \hat{y}_i),
\end{equation}

where $\mathbb{I}(\cdot)$ is the indicator function, taking the value $1$ if the condition $y_i = \hat{y}_i$ is met (i.e., the prediction is correct) and $0$ otherwise. This metric provides a global measure of the model's effectiveness in correctly identifying class membership across the entire test set.

\section{Baseline Regularization Strategy}
\label{sec:baseline_regularization}

Standard estimators for Linear and Quadratic Discriminant Analysis (LDA and QDA) rely on the inversion of the sample covariance matrix, $\hat{\boldsymbol{\Sigma}}$. In high-dimensional settings, particularly when $p > n$, $\hat{\boldsymbol{\Sigma}}$ becomes singular or near-singular, possessing eigenvalues close to or equal to zero. This renders the standard precision matrix calculation unstable or impossible.

To ensure a fair comparison between the proposed \texttt{gips}-based models and the classical approaches, we implemented a modified version of the standard algorithms, denoted as \texttt{LDAmod} and \texttt{QDAmod}. These models incorporate a deterministic regularization step—referred to as \textit{desingularization}—which guarantees that the covariance matrix is strictly positive-definite with a bounded condition number.

\subsection{Mathematical Formulation}

The objective of this procedure is to transform the empirical covariance matrix $\mathbf{A}$ into a regularized matrix $\tilde{\mathbf{A}}$ such that its smallest eigenvalue, $\lambda_{\min}(\tilde{\mathbf{A}})$, is no less than a specific target threshold $\tau$. For the purposes of this study, we set $\tau = 0.05$.

If the smallest eigenvalue of the original matrix, denoted as $\lambda = \lambda_{\min}(\mathbf{A})$, already satisfies $\lambda \ge \tau$, no modification is performed. However, if $\lambda < \tau$, we apply a linear shrinkage towards the identity matrix $\mathbf{I}$. The regularized matrix is defined as:

\begin{equation}
    \tilde{\mathbf{A}} = \frac{\mathbf{A} + s\mathbf{I}}{1 + s},
\end{equation}
where $s$ is a non-negative scalar scaling factor.

\subsection{Derivation of the Scaling Factor}

The parameter $s$ is derived analytically to ensure the new smallest eigenvalue exactly matches the target $\tau$. The derivation proceeds as follows:

\begin{enumerate}
    \item Let the eigenvalues of $\mathbf{A}$ be denoted by $\lambda_i$. The smallest eigenvalue is $\lambda$.
    \item Adding a multiple of the identity matrix shifts the spectrum. The eigenvalues of $\mathbf{A} + s\mathbf{I}$ are $\lambda_i + s$. Consequently, the smallest eigenvalue becomes $\lambda + s$.
    \item Dividing by the scalar $(1+s)$ scales the eigenvalues. Thus, the smallest eigenvalue of the normalized matrix $\tilde{\mathbf{A}}$ is given by:
    \begin{equation}
        \lambda_{\min}(\tilde{\mathbf{A}}) = \frac{\lambda + s}{1 + s}.
    \end{equation}
\end{enumerate}

To enforce the condition $\lambda_{\min}(\tilde{\mathbf{A}}) = \tau$, we solve the following equation for $s$:

\begin{equation}
    \frac{\lambda + s}{1 + s} = \tau.
\end{equation}

Rearranging the terms:
\begin{align*}
    \lambda + s &= \tau(1 + s) \\
    \lambda + s &= \tau + \tau s \\
    s - \tau s &= \tau - \lambda \\
    s(1 - \tau) &= \tau - \lambda.
\end{align*}

This yields the closed-form solution for the scaling factor:
\begin{equation}
    s = \frac{\tau - \lambda}{1 - \tau}.
\end{equation}

This transformation ensures that \texttt{LDAmod} and \texttt{QDAmod} remain numerically solvable even in high-dimensional scenarios where standard implementations would fail due to singularity, providing a robust baseline for benchmarking the performance of \texttt{gipsDA}.

\section{Synthetic data}
\label{sec:synthetic_data}

To rigorously evaluate the performance of the proposed classification models, a comprehensive simulation study was designed. This involved generating synthetic datasets with controlled and well-defined properties, allowing us to systematically explore different assumptions regarding the underlying covariance structures of the classes.

\subsection{Experimental Workflow and Reproducibility}

A critical aspect of our methodology is the separation of the \textit{distribution definition} from the \textit{data sampling} phase. To ensure that performance comparisons across different sample sizes ($n$) reflect true model characteristics rather than random variations in the ground-truth parameters, the simulation proceeds in two distinct stages:

\begin{enumerate}
    \item \textbf{Parameter Fixing:} First, the "ground truth" parameters—specifically the class mean vectors ($\boldsymbol{\mu}_k$) and the true covariance matrices ($\boldsymbol{\Sigma}_k$)—are generated according to specific scenarios (detailed in Section \ref{sec:scenarios}). These parameters are serialized and saved to disk.
    \item \textbf{Monte Carlo Simulation:} Subsequently, for each defined sample size $n$, we conduct a series of independent experiments. In each experiment, training and test datasets are sampled from the distributions defined by the saved parameters.
\end{enumerate}

Consequently, each data point on the resulting performance curves represents the mean accuracy calculated over multiple independent repetitions (e.g., 20 or 50 runs). This approach minimizes the variance of the estimator and ensures that the observed trends are statistically robust.

\subsection{Data Generation Algorithm}

The core algorithm for defining the distributions consists of the following sequential steps:

\begin{enumerate}
    \item \textbf{Mean Vector Sampling:}
    For each of the $k$ classes, a mean vector $\boldsymbol{\mu}_k$ is sampled as a random point from a $p$-dimensional hypercube, $[0, 1]^p$. To prevent fortuitous overlap between classes, a minimum separation constraint is enforced. After sampling the set of $k$ vectors, the Euclidean distance between every pair of distinct means $(\boldsymbol{\mu}_i, \boldsymbol{\mu}_j)$ is calculated. If any pair satisfies $\|\boldsymbol{\mu}_i - \boldsymbol{\mu}_j\| \leq 0.05$, the entire set is discarded, and the sampling process is repeated. This safeguard prevents the generation of trivially difficult scenarios caused by nearly identical class centroids.

    \item \textbf{Covariance Matrix Construction (Spectral Decomposition):}
    To rigorously stress-test the models under varied spectral conditions, we employ a method based on spectral decomposition. This allows for direct control over the eigenvalues and the condition number of the matrix. The procedure is as follows:
    \begin{itemize}
        \item \textbf{Eigenvalue Generation:} A vector of eigenvalues $\boldsymbol{\lambda} = (\lambda_1, \dots, \lambda_p)$ is drawn from a specified distribution $\mathcal{D}$ (e.g., Exponential) and sorted such that $\lambda_1 \ge \dots \ge \lambda_p > 0$. Let $\Lambda = \text{diag}(\boldsymbol{\lambda})$.
        \item \textbf{Random Orthogonal Matrix:} A random matrix $\mathbf{Z} \in \mathbb{R}^{p \times p}$ is generated with entries $Z_{ij} \sim \mathcal{N}(0, 1)$. To obtain a uniformly distributed orthogonal matrix (Haar measure), we perform a QR decomposition: $\mathbf{Z} = \mathbf{Q}'\mathbf{R}$. The matrix $\mathbf{Q}'$ is adjusted by the signs of the diagonal elements of $\mathbf{R}$ to ensure uniqueness:
        \begin{equation}
            \mathbf{Q} = \mathbf{Q}' \cdot \text{diag}\left(\text{sgn}(R_{11}), \dots, \text{sgn}(R_{pp})\right).
        \end{equation}
        \item \textbf{Reconstruction:} The base covariance matrix is constructed as $\boldsymbol{\Sigma}_{\text{raw}} = \mathbf{Q} \Lambda \mathbf{Q}^\top$.
    \end{itemize}

    \item \textbf{Covariance Matrix Projection:}
    For scenarios involving the \texttt{gips} methodology, the raw covariance matrix $\boldsymbol{\Sigma}_{\text{raw}}$ is projected onto a specific permutation group structure. This step imposes the predefined symmetries required to test the specific hypotheses of our thesis (e.g., cyclic symmetry or block symmetry).

    \item \textbf{Separability Control:}
    To ensure a consistent level of classification difficulty across diverse scenarios, a scaling parameter $\psi$ is introduced. This parameter scales the covariance matrix ($\boldsymbol{\Sigma} = \boldsymbol{\Sigma}_{\text{base}} \cdot \psi$), where smaller values of $\psi$ reduce variance, making classes more compact and separable. The optimal value of $\psi$ is determined via an automated iterative search. Starting with an initial $\psi_{\text{init}}$, we iteratively test $\psi_{\text{new}} =\frac{\psi_{\text{init}}}{2^i}$ until a baseline LDA or QDA model achieves a predefined target accuracy on the generated data.

    This target threshold is calculated dynamically to adapt to the number of classes, $K$. We define a fixed signal strength parameter $S = 0.6$ and the baseline random accuracy as $Acc_{\text{base}} =\frac{1}{K}$. The theoretical target test accuracy is defined as:
    \begin{equation}
        Acc_{\text{test}} = Acc_{\text{base}} + (1 - Acc_{\text{base}}) \cdot S.
    \end{equation}
    To account for estimation variance during the calibration phase, the actual stopping criterion for the search (target training accuracy) is set slightly higher, capped at a maximum of 0.90:
    \begin{equation}
        Acc_{\text{target}} = \min(0.90, Acc_{\text{test}} + 0.10).
    \end{equation}
    This calibration guarantees that all scenarios are normalized to a comparable difficulty level before the main evaluation begins.

    \item \textbf{Final Data Generation:}
    Finally, for a given experiment with sample size $n_k$ for class $k$, the observations are sampled from the multivariate normal distribution $\mathcal{N}_p(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$.
\end{enumerate}

\subsection{Data Generation Scenarios}
\label{sec:scenarios}

To systematically evaluate the proposed models, we defined five distinct data generation scenarios. Each scenario corresponds to a specific set of assumptions regarding the covariance structure and symmetry of the underlying classes. These scenarios range from the most constrained (homoscedastic and symmetric) to the most flexible (heteroscedastic and unstructured), effectively creating "ideal" conditions for each of the tested algorithms.

Let $K$ denote the number of classes, $\boldsymbol{\Sigma}_g$ the covariance matrix for class $g$, and $\Gamma$ a permutation group defining the symmetry.

\begin{description}
    \item[Scenario 1: gipsLDA (Homoscedastic / Symmetric)] \hfill \\
    This scenario represents the strictest set of assumptions, targeting the \texttt{gipsLDA} model.
    \begin{itemize}
        \item \textbf{Covariance Structure:} We assume homoscedasticity, meaning all classes share a single, common covariance matrix:
        \[ \boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2 = \dots = \boldsymbol{\Sigma}_K = \boldsymbol{\Sigma}_{\text{common}}. \]
        \item \textbf{Symmetry:} This common matrix is projected onto a specific, non-trivial permutation group $\Gamma$. Consequently, the data exhibits a simplified dependency structure shared globally across the entire dataset.
    \end{itemize}

    \item[Scenario 2: gipsMultQDA (Heteroscedastic / Shared Symmetry)] \hfill \\
    This scenario is designed to test the intermediate flexibility of the \texttt{gipsMultQDA} model.
    \begin{itemize}
        \item \textbf{Covariance Structure:} We assume heteroscedasticity. Each class $g$ possesses a unique covariance matrix $\boldsymbol{\Sigma}_g$ with distinct eigenvalues and variance scales ($\boldsymbol{\Sigma}_i \neq \boldsymbol{\Sigma}_j$ for $i \neq j$).
        \item \textbf{Symmetry:} Crucially, while the matrices differ numerically, they are all constrained by the \textbf{same} underlying permutation group $\Gamma$. This implies that while the magnitude of correlations may vary between classes, the pattern of invariant relationships between features remains constant across the population.
    \end{itemize}

    \item[Scenario 3: gipsQDA (Heteroscedastic / Unique Symmetry)] \hfill \\
    This is the most general and flexible scenario involving symmetry, targeting the \texttt{gipsQDA} model.
    \begin{itemize}
        \item \textbf{Covariance Structure:} The data is fully heteroscedastic; each class has a distinct covariance matrix $\boldsymbol{\Sigma}_g$.
        \item \textbf{Symmetry:} Unlike the previous scenario, there is no shared structural constraint. Each class covariance matrix $\boldsymbol{\Sigma}_g$ is projected onto its own \textbf{unique} permutation group $\Gamma_g$. This models complex environments where the dependency structure of features changes fundamentally depending on the class label.
    \end{itemize}

    \item[Scenario 4: Classic LDA (Homoscedastic / Unstructured)] \hfill \\
    This serves as the baseline for linear classification.
    \begin{itemize}
        \item \textbf{Covariance Structure:} All classes share a single covariance matrix $\boldsymbol{\Sigma}_{\text{common}}$.
        \item \textbf{Symmetry:} No permutation symmetry is imposed (other than the trivial identity group). The covariance matrix is generated from a random spectral distribution without any projection, representing a standard, unstructured multivariate normal distribution.
    \end{itemize}

    \item[Scenario 5: Classic QDA (Heteroscedastic / Unstructured)] \hfill \\
    This serves as the baseline for quadratic classification.
    \begin{itemize}
        \item \textbf{Covariance Structure:} Each class has a unique, distinct covariance matrix $\boldsymbol{\Sigma}_g$.
        \item \textbf{Symmetry:} Similar to Scenario 4, no permutation constraints are applied. Each matrix is independently generated and unstructured. This represents the most difficult setting for estimation when $n$ is small, as the number of parameters to estimate is maximal ($\frac{Kp(p+1)}{2}$).
    \end{itemize}
\end{description}


\section{Real data}
\label{sec:real_world_data}

In addition to synthetic simulations, the proposed models were evaluated on a diverse collection of real-world datasets sourced from public repositories. These datasets span various domains, including medical diagnostics, finance, and industrial quality control, providing a robust test bed for assessing model performance under realistic conditions.

The selected datasets present a wide range of challenges, including high dimensionality, class imbalance, and complex feature dependencies. Furthermore, several datasets contain binary and categorical features. By including these, we consciously accept the violation of the multivariate normality assumption inherent to Discriminant Analysis, allowing us to assess the practical robustness of the \texttt{gipsDA} framework in non-ideal settings.

Below, we provide a detailed description of each dataset and the specific preprocessing pipelines applied to prepare the data for experimentation.

\subsection{Heart Failure Prediction Dataset}

\begin{itemize}
    \item \textbf{Domain:} Medical Diagnostics
    \item \textbf{Description:} Contains clinical features used to predict mortality caused by heart failure. It represents a classic binary classification problem.
    \item \textbf{Target Variable:} \texttt{HeartDisease} (1: Heart Disease, 0: Normal).
    \item \textbf{Dimensions:} 11 features, 918 observations.
    \item \textbf{Class Balance:} Normal (410), Heart Disease (508).
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction} \cite{heart}
\end{itemize}

\textbf{Preprocessing Pipeline:}
Since this dataset contains categorical variables (e.g., \texttt{Sex}, \texttt{ChestPainType}), we applied \textbf{One-Hot Encoding} (dummy encoding), dropping one level per factor to avoid perfect multicollinearity. This transformation introduces binary features, explicitly violating the Gaussian assumption, but allows the models to utilize all available information. Following encoding, we removed 1 feature that exhibited near-zero variance (dominance ratio $> 0.9$).

\subsection{Breast Cancer Wisconsin (Diagnostic)}

\begin{itemize}
    \item \textbf{Domain:} Medical Diagnostics
    \item \textbf{Description:} Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, describing characteristics of the cell nuclei.
    \item \textbf{Target Variable:} \texttt{diagnosis} (M: Malignant, B: Benign).
    \item \textbf{Dimensions:} 30 features, 569 observations.
    \item \textbf{Class Balance:} Benign (357), Malignant (212).
    \item \textbf{Source:} \url{https://archive.ics.uci.edu/dataset/17/breast-cancer-wisconsin-diagnostic} \cite{breast}
\end{itemize}

\textbf{Preprocessing Pipeline:}
The data was already in a clean, numerical format suitable for analysis. No additional encoding, imputation, or feature removal was required.

\subsection{EEG Brainwave Dataset: Feeling Emotions}

\begin{itemize}
    \item \textbf{Domain:} Medical / Neuroscience
    \item \textbf{Description:} Contains statistical features extracted from EEG brainwave signals collected from individuals in different emotional states.
    \item \textbf{Target Variable:} \texttt{emotion} (Positive, Neutral, Negative).
    \item \textbf{Dimensions:} 2548 features, 2132 observations.
    \item \textbf{Class Balance:} Balanced ($\approx 700$ per class).
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/birdy654/eeg-brainwave-dataset-feeling-emotions} \cite{birdy654_eeg_emotions}
\end{itemize}

\textbf{Preprocessing Pipeline:}
Given the extremely high dimensionality ($p=2548$), feature selection was critical.
\begin{enumerate}
    \item \textbf{Low Variance Filtering:} We removed 2 columns where a single value dominated more than 90\% of the observations.
    \item \textbf{Feature Selection:} We trained a Random Forest classifier on the full dataset and selected the \textbf{top 30 features} based on the Gini importance measure. This reduced the dimensionality to a manageable level while preserving the most predictive signals.
\end{enumerate}

\subsection{Credit Card Fraud Detection}

\begin{itemize}
    \item \textbf{Domain:} Rare-Event Detection / Finance
    \item \textbf{Description:} A dataset of credit card transactions aimed at identifying fraudulent activity.
    \item \textbf{Target Variable:} \texttt{Class} (1: Fraud, 0: Legitimate).
    \item \textbf{Dimensions:} 30 features, $\approx 285,000$ observations.
    \item \textbf{Class Balance:} Highly Imbalanced (only 492 fraud cases, $\approx 0.17\%$).
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud} \cite{ulb_creditcard_fraud}
\end{itemize}

\textbf{Preprocessing Pipeline:}
The extreme class imbalance poses a problem for standard classifiers. We applied a strict \textbf{undersampling} strategy to create a balanced subset for training:
\begin{enumerate}
    \item We retained all minority class samples (Fraud).
    \item We sampled the majority class (Legitimate) such that the final dataset composition was 40\% Fraud and 60\% Legitimate.
    \item This resulted in a dataset of 1230 observations.
    \item Finally, we removed 2 collinear features using a correlation cutoff of 0.9.
\end{enumerate}

\subsection{Steel Plates Faults Dataset}

\begin{itemize}
    \item \textbf{Domain:} Industrial Quality Control
    \item \textbf{Description:} Attributes of steel plates used to classify surface defects into 7 distinct fault types.
    \item \textbf{Target Variable:} Fault Type (e.g., \texttt{Pastry}, \texttt{Z\_Scratch}, \texttt{K\_Scatch}).
    \item \textbf{Dimensions:} 27 features, 1941 observations.
    \item \textbf{Class Balance:} Multi-class, ranging from 55 to 673 samples per class.
    \item \textbf{Source:} \url{https://archive.ics.uci.edu/dataset/198/steel-plates-faults} \cite{steel_plates_faults_198}
\end{itemize}

\textbf{Preprocessing Pipeline:}
\begin{enumerate}
    \item \textbf{Infinite Value Handling:} We scanned the dataset for infinite values, replaced them with \texttt{NA}, and subsequently removed rows containing missing values.
    \item \textbf{Scaling Small Values:} We applied the \texttt{fix\_tiny\_values} algorithm (described below) to prevent numerical underflow.
    \item \textbf{Collinearity Removal:} We removed 6 features that exhibited a pairwise correlation greater than 0.9.
\end{enumerate}

\subsection{Sensorless Drive Diagnosis Data Set}

\begin{itemize}
    \item \textbf{Domain:} Industrial Quality Control
    \item \textbf{Description:} Signals from a sensorless drive used to diagnose 11 different operating conditions (faults).
    \item \textbf{Target Variable:} Condition (11 classes).
    \item \textbf{Dimensions:} 48 features, 58,509 observations.
    \item \textbf{Class Balance:} Perfectly Balanced (5,319 per class).
    \item \textbf{Source:} \url{https://archive.ics.uci.edu/dataset/325/sensorless+drive+diagnosis} \cite{dataset_for_sensorless_drive_diagnosis_325}
\end{itemize}

\textbf{Preprocessing Pipeline:}
\begin{enumerate}
    \item \textbf{Scaling Small Values:} Some features contained values with very small magnitudes (e.g., $10^{-5}$), which can cause numerical instability in covariance matrix calculations. We applied a custom \texttt{fix\_tiny\_values} algorithm: for any column with a mean absolute value $< 0.01$, we scaled the column by a factor of $10^k$ to bring the magnitude into the standard range ($[1, 10]$).
    \item \textbf{Collinearity Removal:} We identified and removed 17 highly collinear features using a pairwise correlation threshold of 0.99.
\end{enumerate}

\section{Performance Evaluation and Visualization Strategy}
\label{sec:visualization_methodology}

To construct the performance curves, we employ a systematic evaluation protocol that assesses model accuracy across a range of sample sizes. This process is controlled by a specific set of hyperparameters designed to ensure both statistical reliability and computational feasibility.

\subsection{Sample Size Sampling (The X-axis)}

The performance of the classifiers is evaluated at discrete intervals of the sample size, denoted as $n$. To effectively capture the model behavior in the critical "small $n$" regime (where $p \approx n$ or $p > n$) while covering the convergence behavior at larger sample sizes, we do not sample $n$ linearly. Instead, we employ a \textbf{logarithmic spacing} strategy.

Let $N_{\min}$ denote the lower bound (controlled by the parameter \texttt{lb}, typically set to 16) and $N_{\max}$ denote the upper bound (controlled by the parameter \texttt{ub}). The sequence of sample sizes is generated as follows:

\begin{equation}
    n_i = \left\lfloor \exp\left( \ln(N_{\min}) + \frac{i-1}{G-1} (\ln(N_{\max}) - \ln(N_{\min})) \right) \right\rceil, \quad \text{for } i = 1, \dots, G,
\end{equation}
where $G$ is the \texttt{granularity} parameter (typically set to 10 or more). This results in $G$ distinct evaluation points on the x-axis, densely clustered at the lower end where the regularization impact is most significant.

\subsection{Monte Carlo Repetitions (The Y-axis)}

Each data point plotted on the performance curve represents the \textbf{mean accuracy} calculated over a set of independent experiments, controlled by the parameter \texttt{n\_experiments} (typically 30).

For a specific sample size $n_i$:
\begin{enumerate}
    \item We perform $M$ independent repetitions (where $M = \texttt{n\_experiments}$).
    \item In each repetition $j$:
    \begin{itemize}
        \item \textbf{Synthetic Data:} Fresh training and testing sets are sampled directly from the underlying multivariate normal distributions $\mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$ defined by the saved scenario parameters. This ensures true independence between experiments.
        \item \textbf{Real-World Data:} A subset of size $n_i$ is randomly sampled (without replacement) from the full dataset to form the training set, with the remaining data used for testing.
    \end{itemize}
    \item The data is split according to the \texttt{tr\_ts\_split} ratio (e.g., 0.7 for training).
    \item The accuracy $A_{ij}$ is computed.
\end{enumerate}

\textbf{Handling Model Failures:} A crucial aspect of our benchmarking is robustness. If a model fails to fit (e.g., due to numerical instability or singularity that the regularization failed to handle), the experiment is not discarded. Instead, it is penalized by assigning an accuracy of $0$. The reported value for sample size $n_i$ is the arithmetic mean:
\[ \bar{A}_i = \frac{1}{M} \sum_{j=1}^{M} A_{ij}. \]

\subsection{Optimization Strategy}

The \texttt{gips}-based models require finding the optimal permutation group. The search strategy is determined automatically based on the dimensionality $p$ of the data to balance precision and computation time:

\begin{itemize}
    \item \textbf{Small Dimension ($p \le 9$):} We utilize a \textbf{Brute Force} optimizer. This algorithm exhaustively searches the entire space of permutation groups, guaranteeing the discovery of the global maximum for the posterior probability.
    \item \textbf{High Dimension ($p > 9$):} The permutation space becomes too large for exhaustive search. Here, we employ the \textbf{Metropolis-Hastings} (MH) algorithm, a stochastic Markov Chain Monte Carlo (MCMC) method. The depth of the search is controlled by the \texttt{max\_iter} parameter (typically set to 1000 iterations).
\end{itemize}

Additionally, the \texttt{MAP} parameter controls the estimation logic: if \texttt{TRUE}, the model projects covariance matrices onto the single best permutation found (Maximum A Posteriori); if \texttt{FALSE}, it utilizes Bayesian Model Averaging over the visited permutations.

\subsection{Statistical Significance Testing}
\label{sec:statistical_testing}

To determine whether the observed performance differences between the proposed \texttt{gips}-based models and the baselines are statistically significant rather than artifacts of random sampling, we employ a non-parametric testing framework. Specifically, we utilize a Blocked Permutation Test (also known as a Blocked Randomization Test).

This choice is motivated by the structure of our experiment. Since model performance varies drastically across different sample sizes ($n$), simply aggregating results would violate the exchangeability assumption required for standard tests. The "blocking" factor in our design is the sample size; we must ensure that comparisons are made strictly within the same regime of data availability.

\subsubsection{Hypothesis Definition}

For a given pair of models, Model A (baseline, e.g., \texttt{QDAmod}) and Model B (proposed, e.g., \texttt{gipsQDA}), we define the following hypotheses:
\begin{itemize}
    \item $H_0$ (Null Hypothesis): There is no difference in performance between Model A and Model B. The assignment of accuracy scores to the models is arbitrary within each experimental block.
    \item $H_1$ (Alternative Hypothesis): Model B performs significantly better than Model A (one-sided test).
\end{itemize}

\subsubsection{Test Statistic Construction}

Let $G$ denote the number of sample size levels (granularity). For a specific sample size $n_k$ (where $k=1, \dots, G$), we perform $M$ independent experiments. Let $Acc_{k,j}^{(A)}$ and $Acc_{k,j}^{(B)}$ denote the accuracy of Model A and Model B, respectively, in the $j$-th experiment at sample size $n_k$.

First, we calculate the mean difference in accuracy for each sample size block $k$:
\begin{equation}
    \bar{\delta}_k = \frac{1}{M} \sum_{j=1}^{M} \left( Acc_{k,j}^{(B)} - Acc_{k,j}^{(A)} \right).
\end{equation}

The observed test statistic, $T_{\text{obs}}$, is defined as the grand mean of these block-wise differences:
\begin{equation}
    T_{\text{obs}} = \frac{1}{G} \sum_{k=1}^{G} \bar{\delta}_k.
\end{equation}
This statistic effectively aggregates the performance gain across the entire learning curve, treating each sample size level as equally important.

\subsubsection{Permutation Procedure}

To approximate the null distribution of the test statistic, we perform $B = 5000$ Monte Carlo permutations. The procedure preserves the block structure of the data:

\begin{enumerate}
    \item \textbf{Within-Block Swapping:} For every specific sample size $n_k$ and every experiment $j$, we randomly swap the accuracy values of Model A and Model B with a probability of $0.5$. Formally, we define a Bernoulli variable $S_{k,j} \sim \text{Bern}(0.5)$.
    \[
    (X, Y) =
    \begin{cases}
    (Acc_{k,j}^{(A)}, Acc_{k,j}^{(B)}) & \text{if } S_{k,j} = 0 \\
    (Acc_{k,j}^{(B)}, Acc_{k,j}^{(A)}) & \text{if } S_{k,j} = 1
    \end{cases}
    \]
    \item \textbf{Statistic Recomputation:} Using these permuted labels, we calculate a new permuted statistic $T^*_b$.
    \item \textbf{P-value Calculation:} After $B$ repetitions, the empirical p-value is calculated as the proportion of permuted statistics that are greater than or equal to the observed statistic:
    \begin{equation}
        p\text{-value} = \frac{1}{B} \sum_{b=1}^{B} \mathbb{I}(T^*_b \ge T_{\text{obs}}).
    \end{equation}
\end{enumerate}

A low p-value (typically $< 0.05$) indicates that the observed performance improvement of Model B over Model A is statistically significant and unlikely to have occurred by chance.


% ======================================================================
%                          Experiments
% ======================================================================

\chapter{Experimental Results}

\section{Plots and Tables}

\section{Comments and interpretation of results}

% ======================================================================
%                          Conclusion
% ======================================================================

\chapter{Conclusion and Discussion}

\section{Reflection on the approach}

\section{Future Work}
\textcolor{red}{Tutaj wiem co napisac, troche jak w prezentacji}

% ======================================================================
%                          Rest
% ======================================================================

\chapter{Division of Work}

\begin{table}[H]
        \centering
        \small
        % Zwiększenie odstępów między wierszami dla lepszej czytelności
        \renewcommand{\arraystretch}{1.1}

        \begin{tabularx}{\textwidth}{l X c c}
            \toprule
            \textbf{Category} & \textbf{Task / Responsibility} & \textbf{Antoni} & \textbf{Norbert} \\
            \midrule
            \textbf{Theory} & Mathematical derivation of the \texttt{gipsmult} posterior & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Package} & \texttt{gipsmult} module: Optimization engines (MH, BF) & \textcolor{teal}{$\checkmark$} & \textcolor{gray}{$\times$} \\
            \addlinespace
            \textbf{Package} & \texttt{models} module: S3 methods, API consistency & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Package} & Model serialization (JSON engine) & \textcolor{teal}{$\checkmark$} & \textcolor{gray}{$\times$} \\
            \addlinespace
            \textbf{Testing} & Unit testing framework and "Negative Testing" suite & \textcolor{gray}{$\times$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Experiments} & Synthetic data generation and benchmarking & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Experiments} & Real-world dataset benchmarking & \textcolor{gray}{$\times$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Documentation} & Technical reports, User's Manual, and Thesis Writing & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \bottomrule
        \end{tabularx}
        \caption{Summary of individual and shared contributions.}
    \end{table}

\textcolor{red}{Tutaj podzial pracy tekstu inzynierki czyli tabelka kto ktory rozdzial w pracy napisal itd}

% ------------------------------- BIBLIOGRAPHY ---------------------------
% LEXICOGRAPHICAL ORDER BY AUTHORS' LAST NAMES
% FOR AMBITIOUS ONES - USE BIBTEX


%\begin{thebibliography}{20} % IF YOU HAVE MORE REFERENCES, WRITE THE BIGGER NUMBER
%
%\bibitem[1]{Ktos} A. Author, \emph{Title of a book}, Publisher, year, page--page.
%\bibitem[2]{Innyktos} J. Bobkowski, S. Dobkowski, Title of an article, \emph{Magazine X, No. 7}, year, PAGE--PAGE.
%\bibitem[3]{B} C. Brink, Power structures, \emph{Algebra Universalis 30(2)}, 1993, 177--216.
%\bibitem[4]{H} F. Burris, H. P. Sankappanavar, \emph{A Course of Universal Algebra}, Springer-Verlag, New York, 1981.
%\end{thebibliography}

% Wybór stylu bibliografii (np. plain, abbrv, alpha, unsrt)
\bibliographystyle{plain}

% Wskazanie pliku z bazą danych (bez rozszerzenia .bib)
\bibliography{thesis-en}

\pagenumbering{gobble}
\thispagestyle{empty}

\appendix
\chapter{Derivation of a posteriori distribution in \texttt{gipsmult} model}
We examine m multidimensional gaussian samples $Z^{(1)}_{g},\dots Z_g^{(n_g)}$ under
set $\{\mathcal{K}_g=\kappa_g, \Gamma=c\} $ consisting of i.i.d. random
vectors $\mathcal{N}_p(0, \kappa_g^{-1})$.
Let $\Gamma$ be a discrete random variable uniformly distributed over the set of
$
\mathcal{C} := \{ \langle \sigma \rangle : \sigma \in \mathfrak{S}_p \}
$
cyclic subgroups of $\mathfrak{S}_p$. We assume that each $\mathcal{K}_g$ under set $\Gamma = c$ follows conjugate Diaconis-Ylvisacker a priori distribution \cite{diaconis1979conjugate} defined by its PDF:
\begin{equation}
    f_{\mathcal{K}_g \mid \Gamma = c}(k_g)
  = \frac{1}{I_{c}(\delta, D)}
    \operatorname{Det}(k_g)^{(\delta - 2)/2}
    e^{-\tfrac12 \operatorname{tr}[D^{-1} k_g] }
    \mathbf{1}_{\mathcal{P}_{c}}(k_g),
\end{equation}
where $\delta>1$ and $D\in \mathcal{P}_c$ are hyperparameters and $I_c(\delta, D)$ is a normalizing constant.
We start with standard bayesian reasoning linking the probability of any given permutation conditioned by data with probability of specific data conditioned by given permutation:
\begin{equation}
\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) =
\frac{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right) \mathbbm{P}\left(\Gamma = c \right)}{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right)} \propto \mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right).
\end{equation}
Writing this in more probabilistic terms and conditioning $\mathbbm{X}$ directly on precision matrices we get:
\begin{equation}
\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) \propto f_{\mathbbm{X} \; \mid \; \Gamma = c, \; \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) = \idotsint\limits_{\mathcal{P}_c^m}
    f_{\mathbbm{X} \mid \mathcal{K} = \kappa, \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x})\,
    f_{\mathcal{K} \mid \Gamma = c}(\kappa)
    \, d \kappa
\end{equation}
Let us now tackle the first factor under the integral:
\begin{equation}
    \begin{align}
        f_{\mathbbm{X} \mid \mathcal{K} = \kappa, \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) = \prod_{i=i}^{n}(2\pi)^{-\frac{p}{2}}\left|K_{g_i}\right|^{\frac{1}{2}}
        \exp \left(-\frac{1}{2}x_i^T K_{g_i}x_i\right) \propto \\ \propto \prod_{g=1}^m\left|K_g\right|^{\frac{n_g}{2}}\exp\left(-\frac{1}{2}\sum_{g=1}^m\sum_{j=1}^{n_g}
        x_{g,j}^T K_g x_{g,j}\right)
    \end{align}
\end{equation}
As inside exponential we are summing real numbers we can replace them by their trace:
\begin{equation}
    \begin{align}
        \exp\left(-\frac{1}{2}\sum_{g=1}^m\sum_{j=1}^{n_g}
        x_{g,j}^T K_g x_{g,j}\right) = \exp\left(-\frac{1}{2}\sum_{g=1}^m\sum_{j=1}^{n_g}
        tr\left(x_{g,j}^T K_g x_{g,j}\right)\right) = \\ =
        \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_g\sum_{j=1}^{n_g}
        x_{g,j}x_{g,j}^T\right)\right) = \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_{g}U_g\right)\right)
    \end{align}
\end{equation}
Where:
\begin{equation}
    U_g = \sum_{j=1}^{n_g}
        x_{g,j}x_{g,j}^T
\end{equation}
As for the second factor we assume precission matrices are independent:
\begin{equation}
    f_{\mathcal{K} \mid \Gamma = c}(\kappa) = \prod_{g=1}^m f_{K_g \mid \Gamma = c} (k_g)
\end{equation}
Combining all of these we get:
\begin{equation}
    \begin{align}
        \idotsint\limits_{\mathcal{P}_c^m}
    f_{\mathbbm{X} \mid \mathcal{K} = \kappa, \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x})\,
    f_{\mathcal{K} \mid \Gamma = c}(\kappa)
    \, d \kappa = \\ =\idotsint\limits_{\mathcal{P}_c^m} \prod_{g=1}^m \left|K_g\right|^{\frac{n_g}{2}}
        \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_{g}U_g\right)\right) f_{K_g \mid \Gamma = c}(k_g) \, d \kappa = \\ =
        \prod_{g=1} ^m \int\limits_{\mathcal{P}_c} \left|K_g\right|^{\frac{n_g}{2}}
        \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_{g}U_g\right)\right) f_{K_g \mid \Gamma = c}(k_g) \, dk_g
    \end{align}
\end{equation}
Applying results from \cite{JSSv112i07} for each factor we arrive at the result:
\begin{equation}\label{UltimateAPosterioriFormula}
    \mathbbm{P}\left(\Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right) \propto
    f_{\mathbbm{X} \; \mid \; \Gamma = c, \; \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) \propto
    \prod_{g=1}^m\frac{I_c\left(\delta + n_g, D+U_g\right)}{I_c\left(\delta, D\right)},
\end{equation}






% ----------------------- LIST OF SYMBOLS AND ABBREVIATIONS ------------------
\chapter*{List of symbols and abbreviations}

\begin{tabular}{cl}
nzw. & nadzwyczajny \\
* & star operator \\
$\widetilde{}$ & tilde
\end{tabular}
\\
If you don't need it, delete it.
\thispagestyle{empty}


% ----------------------------  LIST OF FIGURES --------------------------------
\listoffigures
\thispagestyle{empty}
If you don't need it, delete it.


% -----------------------------  LIST OF TABLES --------------------------------
\renewcommand{\listtablename}{Spis tabel}
\listoftables
\thispagestyle{empty}
If you don't need it, delete it.

% -----------------------------  LIST OF APPENDICES ---------------------------
\chapter*{List of appendices}
\begin{enumerate}
\item Appendix 1
\item Appendix 2
\item In case of no appendices, delete this part.
\item texy probabilistyczne
%ogólny ogólnik
%\begin{equation*}
%\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) =
%\frac{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right)}{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right)} \propto \mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right)
%\end{equation*}
%wzór japońcowy
%\begin{equation*}
%I_c\left(\delta, D\right) = \int \limits_{P_c} |k|^{\frac{\delta - 2}{2}} \exp\left(-\frac{1}{2} Tr\left(Dk\right)\right) \; dk
%\end{equation*}
%powiązanie rozkładu z japańcem
%\begin{equation*}
%\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}\right) \propto \frac{I_c\left(\delta + n, D+U\right)}{I_c\left(\delta, D\right)}
%\end{equation*}
%uszczegółowienie ogólnego ogólnika (ogólnik)
%\begin{equation*}
%\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) \propto f_{\mathbbm{X} \; \mid \; \Gamma = c, \; \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) = \overset{m}{\idotsint} f(\mathbbm{x})\, d\mathbbm{x}
%\end{equation*}

\end{enumerate}

\thispagestyle{empty}





\end{document}
