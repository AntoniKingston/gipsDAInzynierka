\documentclass[a4paper,11pt,twoside]{report}
% THIS FILE SHOULD BE COMPILED BY pdfLaTeX

% ----------------------   PREAMBLE PART ------------------------------

% ------------------------ ENCODING & LANGUAGES ----------------------

\usepackage{fontspec}
\usepackage[polish, english]{babel} % Ostatni język to język GŁÓWNY pracy (English)
\usepackage{bbm}



\usepackage{amsmath, amsfonts, amsthm, latexsym, bm, amssymb} % MOSTLY MATHEMATICAL SYMBOLS

\usepackage[final]{pdfpages} % INPUTING TITLE PDF PAGE - GENERATE IT FIRST!
%\usepackage[backend=bibtex, style=verbose-trad2]{biblatex}

\usepackage{tabularx, booktabs, xcolor, float} % FOR TABLES

\usepackage{commath} % various commands which can make writing math expressions easier --- documentation available at: https://ctan.gust.org.pl/tex-archive/macros/latex/contrib/commath/commath.pdf

\usepackage[hidelinks]{hyperref} % for hyperlinks, for example, urls, references to equations, entries in a bibliography --- hidelinks option removes rectangles around hiperlinks

\usepackage{listings}
\usepackage[most]{tcolorbox}

% ---------------- MARGINS, INDENTATION, LINESPREAD ------------------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry} % MARGINS


\linespread{1.5}
\allowdisplaybreaks         % ALLOWS BREAKING PAGE IN MATH MODE

\usepackage{indentfirst}    % IT MAKES THE FIRST PARAGRAPH INDENTED; NOT NEEDED
\setlength{\parindent}{5mm} % WIDTH OF AN INDENTATION


%---------------- RUNNING HEAD - CHAPTER NAMES, PAGE NUMBERS ETC. -------------------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
% PAGINATION: LEFT ALIGNMENT ON EVEN PAGES, RIGHT ALIGNMENT ON ODD PAGES
\fancyfoot[LE,RO]{\thepage}
% RIGHT HEADER: zawartość \rightmark do lewego, wewnętrznego (marginesu)
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
% lewa pagina: zawartość \leftmark do prawego, wewnętrznego (marginesu)
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}

% HEAD RULE - IT'S A LINE WHICH SEPARATES HEADER AND FOOTER FROM CONTENT
\renewcommand{\headrulewidth}{0 pt} % 0 MEANS NO RULE, 0.5 MEANS FINE RULE, THE BIGGER VALUE THE THICKER RULE


\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[LE,RO]{\thepage}

  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0.0pt}
}

% ----------------------- Rcode Setup ---------------------------

% ============================================================
% 1. DEFINICJA KOLORÓW (Nord Style / VS Code)
% ============================================================
\definecolor{codegreen}{rgb}{0.25,0.5,0.35}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{HTML}{F2F2F2}    % Bardzo jasne szare tło
\definecolor{framecolour}{HTML}{2E3440}   % Ciemna ramka (Nord dark)
\definecolor{keywordcolour}{HTML}{81A1C1} % Nord blue

% ============================================================
% 2. STYL LISTINGU (Sam kod R)
% ============================================================
\lstdefinestyle{myRstyle}{
    language=R,
    backgroundcolor=\color{backcolour},
    commentstyle=\itshape\color{codegreen},
    keywordstyle=\bfseries\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize, % Czcionka maszynowa, mniejsza
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=8pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    morekeywords={TRUE, FALSE, NULL, NA, Inf, NaN}
}

% Aktywacja stylu domyślnego
\lstset{style=myRstyle}

% ============================================================
% 3. ŚRODOWISKO "RCode" (Ramka + Listing)
% ============================================================
\newtcblisting[auto counter]{RCode}[2][]{%
    enhanced,                     % Zaawansowane rysowanie
    breakable,                    % POZWALA PRZENOSIĆ KOD NA KOLEJNĄ STRONĘ
    listing engine=listings,      % Silnik listings
    listing only,                 % Tylko kod
    listing options={style=myRstyle},
    title={\textbf{Listing \thetcbcounter:} #2}, % Tytuł: Listing X: Nazwa
    colback=backcolour,           % Tło
    colframe=framecolour,         % Ramka
    coltitle=white,               % Tekst tytułu
    fonttitle=\bfseries\footnotesize,
    sharp corners=downhill,       % Wygląd zakładki
    arc=3mm,
    boxrule=0.5mm,
    drop shadow,                  % Cień
    top=1mm, bottom=1mm,
    #1                            % Opcje dodatkowe
}

% ============================================================
% 4. STYL DLA OUTPUTU Z KONSOLI
% ============================================================
\definecolor{outbg}{HTML}{FAFAFA}      % Prawie białe tło
\definecolor{outframe}{HTML}{707070}   % Szara ramka

\lstdefinestyle{outputStyle}{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{outbg},
    breaklines=true,
    showstringspaces=false,
    tabsize=2,
    frame=none,
    numbers=none,
    columns=fullflexible
}

\newtcblisting{ROutput}[1][]{
    enhanced,
    breakable,                    % Pozwala łamać output na strony
    listing only,
    listing options={style=outputStyle},
    colback=outbg,
    colframe=outframe,
    coltitle=white,
    fonttitle=\bfseries\footnotesize,
    title={Console Output},       % Domyślny tytuł
    arc=2mm,
    boxrule=0.5mm,
    drop shadow,
    #1
}


% --------------------------- CHAPTER HEADERS ---------------------

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt}


\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% ----------------------- TABLE OF CONTENTS SETUP ---------------------------

\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


% THIS MAKES DOTS IN TOC FOR CHAPTERS
\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries \thecontentslabel.\quad}% <numbered-entry-format>
  {\bfseries}% <numberless-entry-format>
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}% <filler-page-format>

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% ---------------------- TABLES AD FIGURES NUMBERING ----------------------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}


% ------------- DEFINING ENVIRONMENTS FOR THEOREMS, DEFINITIONS ETC. ---------------

\makeatletter
\newtheoremstyle{definition}
{3ex}%                           % Space above
{3ex}%                           % Space below
{\upshape}%                      % Body font
{}%                              % Indent amount
{\bfseries}%                     % Theorem head font
{.}%                             % Punctuation after theorem head
{.5em}%                          % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\makeatother

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% --------------------- END OF PREAMBLE PART (MOSTLY) --------------------------





% -------------------------- USER SETTINGS ---------------------------

\newcommand{\tytul}{Dostosowanie biblioteki gips do zadania klasyfikacji przy pomocy analizy dyskryminacyjnej – gipsDA}
\renewcommand{\title}{Adapting the gips library for classification problem utilizing discriminant analysis – gipsDA}
\newcommand{\type}{Engineer} % Master OR Engineer
\newcommand{\supervisor}{MSc Eng. Adam Chojecki\\
    PhD Bartosz Kołodziejek, Assoc. Prof.} % TITLE AND NAME OF THE SUPERVISOR



\begin{document}
\sloppy
\selectlanguage{english}

\includepdf[pages=-]{titlepage-en} % THIS INPUTS THE TITLE PAGE

\null\thispagestyle{empty}\newpage

% ------------------ PAGE WITH SIGNATURES --------------------------------

%\thispagestyle{empty}\newpage
%\null
%
%\vfill
%
%\begin{center}
%\begin{tabular}[t]{ccc}
%............................................. & \hspace*{100pt} & .............................................\\
%supervisor's signature & \hspace*{100pt} & author's signature
%\end{tabular}
%\end{center}
%


% ---------------------------- ABSTRACTS -----------------------------

{  \fontsize{12}{14} \selectfont
\begin{abstract}

\begin{center}
\title
\end{center}
Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\\

\noindent \textbf{Keywords:} keyword1, keyword2, ...
\end{abstract}
}

\null\thispagestyle{empty}\newpage


{\selectlanguage{polish} \fontsize{12}{14}\selectfont
\begin{abstract}

\begin{center}
\tytul
\end{center}

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\\

\noindent \textbf{Słowa kluczowe:} słowo klucz 1, słowo klucz 2, zażółć gęślą jaźń...
\end{abstract}
}


%% --------------------------- DECLARATIONS ------------------------------------
%
%%
%%	IT IS NECESSARY OT ATTACH FILLED-OUT AUTORSHIP DEECLRATION. SCAN (IN PDF FORMAT) NEEDS TO BE PLACED IN scans FOLDER AND IT SHOULD BE CALLED, FOR EXAMPLE, DECLARATION_OF_AUTORSHIP.PDF. IF THE FILENAME OR FILEPATH IS DIFFERENT, THE FILEPATH IN THE NEXT COMMAND HAS TO BE ADJUSTED ACCORDINGLY.
%%
%%	command attacging the declarations of autorship
%%
%\includepdf[pages=-]{scans/declaration-of-autorship}
%\null\thispagestyle{empty}\newpage
%
%% optional declaration
%%
%%	command attaching the declaataration on granting a license
%%
%\includepdf[pages=-]{scans/declaration-on-granting-a-license}
%%
%%	.tex corresponding to the above PDF files are present in the 3. declarations folder
%
\null\thispagestyle{empty}\newpage
% ------------------- TABLE OF CONTENTS ---------------------
% \selectlanguage{english} - for English
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}
\newpage % IF YOU HAVE EVEN QUANTITY OD PAGES OF TOC, THEN REMOVE IT OR ADD \null\newpage FOR DOUBLE BLANK PAGE BEFORE INTRODUCTION


% -------------------- THE BODY OF THE THESIS --------------------------------

\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{11}

% ======================================================================
%                          Introduction
% ======================================================================

\chapter{Introduction}

Statistical classification is a fundamental problem in machine learning and data analysis, with applications ranging from genomic sequencing and medical diagnosis to financial forecasting. Among the vast landscape of classification algorithms, generative models such as Linear and Quadratic Discriminant Analysis (LDA and QDA) remain cornerstones of the field due to their rigorous statistical foundations, interpretability, and computational efficiency. However, the modern era of data science frequently confronts these classical methods with high-dimensional datasets—scenarios where the number of features rivals or exceeds the number of observations. In such environments, standard estimation techniques often fail, necessitating the development of more robust, regularized approaches.

This thesis proposes a novel solution to this challenge by integrating the principles of invariant theory with discriminant analysis. Specifically, we explore how identifying and enforcing permutation symmetries within the data can serve as a powerful form of model-based regularization, stabilizing covariance estimation without resorting to arbitrary shrinkage.

This chapter provides the necessary context for the project and is organized into four main sections:
\begin{itemize}
    \item \textbf{Section \ref{sec:motivation}: Motivation and Goal} identifies the specific limitations of classical classifiers in high-dimensional settings, particularly the issues of parameter instability and singularity, and defines the primary objective of this thesis.
    \item \textbf{Section \ref{sec:symmetry}: Why do we expect symmetry?} provides the rationale behind the proposed solution, explaining why permutation invariance is a plausible and valuable assumption in many real-world datasets.
    \item \textbf{Section \ref{sec:da_desc}: General Description of Discriminant Analysis} briefly establishes the theoretical baseline, outlining the mechanics of LDA and QDA that will be modified in our work.
    \item \textbf{Section \ref{sec:contribution}: Contribution} summarizes the specific achievements of this project, including the development of the \texttt{gipsDA} package and the implementation of novel estimators.
\end{itemize}

\section{Motivation and Goal}
\label{sec:motivation}

Classical generative classifiers, such as Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), remain fundamental tools in statistical machine learning\cite{elemstats} due to their interpretability and closed-form solutions.
However, the performance of these methods is critically dependent on the accurate estimation of the class-conditional covariance matrices, denoted as $\boldsymbol{\Sigma}$.
In the era of \("\)Big Data\("\), researchers frequently encounter high-dimensional datasets where the number of features ($p$) is large relative to the number of available observations ($n$).
In such settings, standard estimators often fail, rendering classical methods suboptimal or entirely inapplicable.

The bottleneck of these algorithms lies in the sample covariance matrix, $\hat{\boldsymbol{\Sigma}}$.
For a dataset with $p$ features, the covariance matrix contains $p(p+1)/2$ unique parameters that must be estimated.
This implies that the model complexity grows quadratically with the dimension of the feature space ($\Theta(p^2)$).
This phenomenon leads to two primary challenges:

\begin{enumerate}
    \item \textbf{Parameter Instability:} When $p$ is large, the number of parameters to estimate can easily exceed the information content available in the training data. This results in estimates with high variance, leading to overfitting and poor generalization performance on unseen data.
    \item \textbf{Estimator Singularity:} In the extreme regime where the number of features exceeds the sample size ($p > n$), the sample covariance matrix $\hat{\boldsymbol{\Sigma}}$ becomes rank-deficient. Consequently, it is no longer positive definite and, crucially, not invertible. Since the decision functions for both LDA and QDA require the computation of the precision matrix $\hat{\boldsymbol{\Sigma}}^{-1}$, classical algorithms mathematically break down in these \("\)small $n$, large $p$\("\) scenarios.
\end{enumerate}

To mitigate these issues, regularization techniques are necessary.
While standard approaches often involve scalar shrinkage (e.g., shrinking towards the identity matrix)\cite{friedman1989regularized, ledoit2004well}, this project explores a structural approach.
We posit that by identifying and enforcing permutation symmetries within the data—where certain variables are exchangeable—we can significantly reduce the number of free parameters required to estimate the covariance structure.

\subsection*{Goal of the Thesis}

The primary objective of this project is to develop \texttt{gipsDA}, a novel R library that adapts discriminant analysis for high-dimensional environments.
By integrating the methodology of the \texttt{gips} (Gaussian model Invariant by Permutation Symmetry) package, we aim to construct robust classifiers that impose statistically justified symmetry constraints on the covariance matrix.

Specifically, the goal is to provide a solution that:
\begin{itemize}
    \item Maintains high classification accuracy in high-dimensional settings where standard LDA/QDA overfit.
    \item Remains mathematically valid and computationally stable in the $p > n$ regime by ensuring the estimated matrices are positive definite through symmetry projection.
    \item Offers a user-friendly interface compatible with the standard R ecosystem, bridging the gap between advanced theoretical statistics and practical data science applications.
\end{itemize}

\section{Why do we expect symmetry?}
\label{sec:symmetry}

At first glance, the assumption that distinct features in a dataset share a permutation symmetry—meaning they remain invariant under specific reorderings—may appear overly restrictive\cite{andersson1998symmetry}. In classical multivariate analysis, features are often treated as distinct entities with unique variances and pairwise correlations. However, in the context of high-dimensional data, the assumption of symmetry is not only mathematically convenient but often empirically justified by the underlying nature of the data generation process.

The core justification lies in the concept of \textit{exchangeability}. In many modern datasets, features are not arbitrary measurements but rather collections of comparable units. Consider the following domains where symmetry naturally arises:

\begin{itemize}
    \item \textbf{Genomics and Bioinformatics:} In gene expression data, thousands of genes are measured simultaneously \cite{dudoit2002comparison}. Genes belonging to the same biological pathway or functional group often exhibit similar behavior. It is statistically plausible to assume that the correlation between any pair of genes within such a functional cluster is roughly equivalent, regardless of their specific labels.

    \item \textbf{Medical Diagnostics and Imaging:} Consider the analysis of digitized images for cancer diagnosis, such as the Breast Cancer Wisconsin dataset used in this work\cite{breast}. Features are computed from the characteristics of cell nuclei present in the image (e.g., radius, texture, smoothness) \cite{street1993nuclear}. While each feature measures a distinct geometric property, groups of features derived from similar morphological aspects often display strong, structured correlations. Assuming a symmetric covariance structure among these related biometrics allows the model to capture the general "shape" of malignant cells without overfitting to the noise inherent in individual sample measurements.

    \item \textbf{Industrial Sensor Systems:} In predictive maintenance and fault detection, systems are often monitored by multiple sensors recording analogous physical quantities. A prime example is the Sensorless Drive Diagnosis dataset\cite{dataset_for_sensorless_drive_diagnosis_325}, which involves electric current signals from different phases of a motor \cite{seera2014classification}. In a balanced three-phase system, the physical properties of the phases are designed to be identical. Consequently, the statistical relationship between Phase A and Phase B should theoretically be invariant to the relationship between Phase B and Phase C. Treating these signals as exchangeable via permutation symmetry reflects the physical reality of the hardware design.
\end{itemize}

Furthermore, the approach proposed in this thesis does not require prior knowledge of these specific relationships. Unlike rigid regularization methods that force the covariance matrix towards a specific target (such as the identity matrix), the \texttt{gips} methodology is designed to \textit{discover} the optimal symmetry structure from the data itself\cite{JSSv112i07}.

By allowing the data to reveal its own invariant structure, we adhere to the principle of parsimony (Occam's razor). If a complex covariance matrix with $\Theta(p^2)$ parameters can be accurately approximated by a symmetric structure requiring only $\Theta(p)$ parameters, the latter model is preferred. It reduces the variance of the estimator and mitigates the risk of overfitting, which is the primary enemy in high-dimensional classification. Thus, expecting symmetry is not about imposing an artificial constraint, but rather about recognizing and exploiting the redundancy inherent in large-scale systems.

\section{General description of Discriminant Analysis}
\label{sec:da_desc}
\textcolor{red}{jakis wprowadzenie ze mozna klasyfikacje robic na podstawie tego ze widzimy symetrie w macierzy kowariancji}

\section{Contribution}
\label{sec:contribution}

The central contribution of this thesis is the proposal and implementation of a novel regularization framework for Discriminant Analysis, specifically designed for the "High Dimension, Low Sample Size" (HDLSS) regime where $p \gg n$. In such settings, classical estimators for the covariance matrix become singular and unstable \cite{bickel2004some}.

While numerous modifications to LDA and QDA have been proposed to address this instability—ranging from the scalar shrinkage parameters of Regularized Discriminant Analysis (RDA) \cite{friedman1989regularized} to sparsity-enforcing methods like Penalized LDA \cite{witten2011penalized} or independence assumptions in Diagonal LDA \cite{dudoit2002comparison}—these approaches often impose arbitrary constraints. They typically force the covariance structure towards a diagonal matrix or sparse representation, potentially discarding significant correlations between features.

In contrast, our approach introduces a paradigm shift by utilizing \textit{permutation symmetry} as a form of model-based regularization. Instead of assuming correlations are zero (sparsity) or uniform (scalar shrinkage), we assume that groups of features may be exchangeable. This allows the data to dictate its own complexity through the discovery of invariant structures.

The specific contributions of this work are as follows:

\begin{itemize}
    \item \textbf{Development of the \texttt{gipsDA} Package:} We provide a fully functional R package that integrates the \texttt{gips} methodology with the standard discriminant analysis workflow. The package is designed to be API-compatible with the widely used \texttt{MASS} library, ensuring immediate accessibility for practitioners.

    \item \textbf{Novel Covariance Estimators:} We introduce three distinct classification models that leverage symmetry in different ways:
    \begin{itemize}
        \item \texttt{gipsLDA}: Adapts the homoscedastic assumption of LDA by projecting the pooled covariance matrix onto a symmetric permutation group.
        \item \texttt{gipsQDA}: Allows for full heteroscedasticity, optimizing a unique permutation structure for each class independently.
        \item \texttt{gipsMultQDA}: A hybrid approach that allows for different covariance matrices per class but constrains them to share the \textit{same} underlying permutation symmetry. This required the development of the \texttt{gipsmult} module to optimize a joint posterior probability across multiple groups.
    \end{itemize}

    \item \textbf{Empirical Validation in Data-Scarce Regimes:} Through extensive simulation studies and benchmarks on real-world datasets (e.g., biomedical and industrial sensor data), we demonstrate that imposing symmetry constraints significantly reduces the estimation error and improves classification accuracy compared to standard QDA, particularly when the sample size is critically low relative to the dimensionality.
\end{itemize}

% ======================================================================
%                          Related Work
% ======================================================================

\chapter{Related Work}

This chapter establishes the theoretical framework and reviews the existing literature fundamental to the project.
The discussion is structured around two pivotal domains that form the basis of our proposed solution.

First, we revisit classical statistical machine learning algorithms for classification, specifically focusing on Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA).
These well-established methods serve as the foundation for the predictive models developed in this work\cite{elemstats}.

Second, we introduce the methodology underlying the \texttt{gips} (Gaussian model Invariant by Permutation Symmetry) package.
We explore the mathematical principles of identifying invariant structures within covariance matrices and how this approach can be utilized for parameter estimation in high-dimensional settings\cite{JSSv112i07}.
The integration of these two distinct areas, classical discriminant analysis and permutation symmetry discovery, constitutes the core contribution of this thesis.

\section{Theoretical description of LDA and QDA}

Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are generative classifiers that model the probability density of each class, $g$, using a multivariate Gaussian distribution, $\mathbbm{P}(x|g) = \mathcal{N}(x | \mu_g, \Sigma_g)$. The key difference between them lies in their assumptions regarding the class covariance matrices, $\Sigma_g$.

\begin{itemize}
    \item \textbf{Linear Discriminant Analysis (LDA)} assumes that all classes share a single, common covariance matrix, i.e., $\Sigma_g = \Sigma$ for all $g$. This assumption of homoscedasticity leads to a decision boundary that is a linear function of the input features $x$.
    \item \textbf{Quadratic Discriminant Analysis (QDA)} relaxes this assumption, allowing each class $g$ to have its own distinct covariance matrix, $\Sigma_g$. This assumption of heteroscedasticity results in a decision boundary that is a quadratic function of $x$, providing greater flexibility.
\end{itemize}

Prediction for a new observation $x$ is made by assigning it to the class $g$ that maximizes the posterior probability $\mathbbm{P}(g|x)$. Using Bayes' theorem, this is equivalent to maximizing the discriminant function, $\delta_g(x)$:
\begin{equation}
    \delta_g(x) = -\frac{1}{2} \log |\Sigma_g| - \frac{1}{2} (x - \mu_g)^T \Sigma_g^{-1} (x - \mu_g) + \log \pi_g,
\end{equation}
where $\mu_g$ is the mean vector for class $g$, $\Sigma_g$ is its covariance matrix, and $\pi_g$ is the prior probability of the class. In practice, these parameters are estimated from the training data. For a more detailed treatment of these methods, we refer the reader to "The Elements of Statistical Learning" \cite{elemstats}. Our work focuses on novel estimation techniques for the $\Sigma_g$ matrices.


\section{General description of gips}
\textcolor{red}{Tutaj opis gipsa, na podstawie artykulu itd}

% ======================================================================
%                          Solution Proposal
% ======================================================================

\chapter{Solution Proposal}
The central theme of our proposed solution is the enhancement of classical discriminant analysis methods by introducing a novel approach to estimating covariance matrices.
We modify the standard Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) algorithms by leveraging the methodology of the \texttt{gips} R package\cite{JSSv112i07}.
The core functionality of the \texttt{gips} library is to identify the most probable permutation group under which a given covariance matrix remains approximately invariant.
By imposing such a symmetry constraint, we effectively introduce a form of model-based regularization.
This reduces the number of free parameters that need to be estimated, which can be particularly advantageous when dealing with high-dimensional data or limited sample sizes, potentially leading to more stable and robust classifiers.

\section{gipsmult}
\subsection{Definitions}
\subsubsection{Symmetric group}
Let us set $p \in \mathbb{N}$. Let $\mathfrak{S}_p$ denote the symmetric group i.e. the set of all permutastions on $V = \{1,2, \dots, p\}$
with function composition as the operation.
\subsubsection{Invariance under action of a subgroup of the symmetric group}
Let $K \in \mathbb{R}^{p\times p}$ and $\Gamma \leq \mathfrak{S}_p$. We say that matrix $K$ is invariant under action of subgroup $\Gamma$ i.i.f
\begin{equation}
    \forall_{\sigma \in \Gamma} \forall_{i,j\in V} K_{i,j} = K_{\sigma(i), \sigma(j)}
\end{equation}
For conciseness sake in the following sections we will say that a matrix is \textit{invariant under subgroup} or, if supgroup $\Gamma = \langle \sigma \rangle$
is cyclic that a matrix is \textit{invariant under} $\sigma$.
\subsubsection{Invariant space and cone}
\begin{equation}
    \mathcal{Z}_\Gamma := \left\{S \in Sym(p;\mathbbm{R}) : S \ is \ invariant \ under \ \Gamma \right\},
\end{equation}
\begin{equation}
    \mathcal{P}_{\Gamma}:= Z_\Gamma \cap Sym^+(p;\mathbbm{R}).
\end{equation}

\subsection{\texttt{gips} vs \texttt{gipsmult}}
\subsubsection{General comparision}
In the setting from \cite{10.1214/22-AOS2174} a single symmetric SPD matrix $K \in \mathbb{R}^{p\times p}$ is invariant under $\sigma \in \mathfrak{S}$
Let us set $m \in \mathbb{N}$, the model of \texttt{gipsmult} assumes an entire set of symmetric SPD matrices $\mathcal{K} = \{K_1, K_2, \dots, K_m\}$ to be invariant under $\sigma$.
\subsubsection{Bayesian procedure}
Under assumption of uniform prior on the set
$
\mathcal{C} := \{ \langle \sigma \rangle : \sigma \in \mathfrak{S}_p \}
$
of cyclic subgroups of $\mathfrak{S}_p$ and Diaconis-Ylvisacker\cite{diaconis1979conjugate} one on the precision
matrix $K$, \cite{JSSv112i07} arrives at the following proportionality of posterior:
\begin{equation}\label{gipsposteriorformula}
    \mathbbm{P}\left(\Gamma = c \mid \mathbbm{X} = \mathbbm{x}\right) \propto \frac{I_c\left(\delta + n, D+U\right)}{I_c\left(\delta, D\right)}
\end{equation}
For more detailed explanation of this result see \cite{JSSv112i07}
\\
Under analogous assumptions the posterior in \texttt{gipsmult} setting is proportional to:
\begin{equation}\label{gipsmultposteriorformula}
    \mathbbm{P}\left(\Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right) \propto \prod_{g=1}^m\frac{I_c\left(\delta + n_g, D+U_g\right)}{I_c\left(\delta, D\right)},
\end{equation}
So the entire logic of Bayesian selection for \texttt{gipsmult} boils down to multiplication of results from \texttt{gips}.\\
We attach full derivation of \eqref{gipsmultposteriorformula} in the appendix A.


\section{Description of machine learning models}
\subsection{MAP vs average weighted by AP distribution}
Each of models described down below:
\begin{enumerate}
    \item Estimates covariance matrix(ces)
    \item Estimates most probable permutation(s) under which real covariance matrices are invariant
    \item Projects matrix estimate(s) onto the space of those invariant under found permutation(s)
\end{enumerate}
MAP option means that our estimate is a single projection:
\begin{equation}
    \hat{\Sigma} = \Sigma_{c^*},
\end{equation}
where
\begin{equation}
    c^*=\operatorname*{argmax}_c \mathbb{P}(\Gamma=c \mid X=x, Y=y).
\end{equation}
Since both \texttt{gips} and \texttt{gipsmult} allow to estimate not only MAP but the entire AP distribution we have an alternative:
\begin{equation}
    \hat{\Sigma}=\sum_{c \in \sigma_P} \mathbb{P}(\Gamma=c \mid X=x, Y=y) \Sigma_c
    \label{eq:weighted_avg_estimator}
\end{equation}
It potentially offers a more robust estimation by incorporating uncertainty about the true underlying symmetry.
It should be noted that formulas above  are exact only when the entire space of permutations can be exhaustively searched,
which is feasible in \texttt{gips(mult)} implementation for $p \le 9$, where $p$ denotes the number of features (columns).
For a larger number of features, the Metropolis-Hastings algorithm, as described in \cite{JSSv112i07},
is used to approximate these estimators, as the permutation space becomes too large to explore completely.

\subsection{gipsQDA}
The \texttt{gipsQDA} model represents the most flexible approach within our framework. In this method, the \texttt{gips} library is applied independently to the data of each class. This process yields a unique estimated covariance matrix, $\hat{\Sigma}_g$, and a unique optimal permutation group, $\hat{c}_g$, for each class $g$.
\begin{equation}
    (\hat{\Sigma}_g, \hat{c}_g) = \texttt{gips}(\text{Data}_g) \quad \text{for each class } g=1, \dots, m
\end{equation}
This model is directly analogous to the classic QDA, as it allows for heteroscedasticity, but with the additional complexity that each class can exhibit its own distinct symmetry structure. Consequently, \texttt{gipsQDA} serves as a general model that contains all other proposed models as special, more constrained cases.

\subsection{gipsLDA}
The \texttt{gipsLDA} model adapts the core idea of LDA—a single, shared covariance matrix—to the \texttt{gips} framework. The general strategy is to first pool the covariance information from all classes into a single matrix, and then use \texttt{gips} to find a common symmetry structure for this pooled matrix. The process is as follows:
\begin{enumerate}
    \item For each class $g$, estimate the sample covariance matrix, $S_g$, using the standard unbiased estimator:
    \begin{equation}
        S_g = \frac{1}{n_g-1} \sum^{n_g}_{i=1} (x_{g,i} -\overline{x}_g)(x_{g,i} -\overline{x}_g)^T,
    \end{equation}
    where $n_g$ is the number of observations in class $g$, and $x_{g,i}$ and $\overline{x}_g$ are the $i$-th observation and the sample mean of class $g$, respectively.
    \item Combine the individual $S_g$ matrices into a single pooled matrix, $S$, using a specific averaging method.
    \item Supply the pooled matrix $S$ to the \texttt{gips} algorithm to find the optimal permutation group $\hat{c}$ and the projected covariance matrix $\hat{\Sigma}_{\text{gipsLDA}}$.
    \item Use the resulting matrix $\hat{\Sigma}_{\text{gipsLDA}}$ as the shared covariance matrix in the LDA classification framework, assuming each class has a different mean vector $\mu_g$.
\end{enumerate}
We propose two methods for pooling the covariance matrices.

\subsubsection{gipsLDA classic}
This method uses the classic pooled covariance estimator, which is the standard approach in traditional LDA. The pooled matrix $S$ is defined as:
\begin{equation}
    S = \frac{1}{n-m}\sum_{g=1}^m (n_g-1)S_{g}.
\end{equation}
By substituting the formula for $S_g$ from Equation (3.10), this simplifies to the well-known pooled sample covariance matrix formula:
\begin{equation}
S = \frac{1}{n-m}\sum_{g=1}^m \sum^{n_g}_{i=1} (x_{g,i} -\overline{x}_g)(x_{g,i} -\overline{x}_g)^T.
\end{equation}
This estimator is an unbiased estimate of the common covariance matrix under the assumption of homoscedasticity.

\subsubsection{gipsLDA weighted average}
This method is named "weighted average" because the contribution of each class's covariance matrix, $S_g$, to the final pooled matrix is weighted by its sample size, $n_g$. However, this weighting scheme gives more influence to smaller classes compared to the \texttt{gipsLDA classic} approach. The pooled matrix $S$ is calculated as:
\begin{equation}
    S = \frac{1}{n}\sum_{g=1}^m n_g S_{g},
\end{equation}
where $n = \sum_{g=1}^m n_g$ is the total number of observations and $m$ is the number of classes.


\subsection{gipsMultQDA}
The \texttt{gipsMultQDA} model is an intermediate approach between the full flexibility of \texttt{gipsQDA} and the strong constraints of \texttt{gipsLDA}.
It is supposed to serve the situation when each class has a unique covariance matrix (like QDA), but all these
matrices share the same underlying permutation symmetry. This is useful for scenarios where classes  differ
in scale or variance but are expected to share a common dependency structure.

\begin{equation}
    (\hat{\Sigma}_g, \hat{c}) = \texttt{gips}(\text{Data}_g) \quad \text{for each class } g=1, \dots, m
\end{equation}

\subsection{Regularization}
It should be also noted that all covariance estimates are regularized after being projected. The exact procedure is discussed in \autoref{sec:baseline_regularization}.






% ======================================================================
%                          Implementation
% ======================================================================
\chapter{Package Description}
The framework of \texttt{gipsDA} was implemented in R. All the necessary code was wrapped as a R package and published at \href{https://cran.r-project.org/}{CRAN}. One can install it with:
\begin{RCode}{R Console}
    install.packages("gipsDA")
\end{RCode}

\begin{ROutput}
    Installing package into ‘/usr/local/lib/R/site-library’
    (as ‘lib’ is unspecified)
    trying URL 'https://cran.r-project.org/src/contrib/gipsDA_0.1.0.tar.gz'
    Content type 'application/x-gzip' length 123456 bytes (120 KB)
    ==================================================
    downloaded 120 KB

    * installing *source* package ‘gipsDA’ ...
    ** package ‘gipsDA’ successfully unpacked and MD5 sums checked
    ** using staged installation
    ** R
    ** byte-compile and prepare package for lazy loading
    ** help
    *** installing help indices
    ** building package indices
    ** installing vignettes
    ** testing if installed package can be loaded from temporary location
    ** testing if installed package can be loaded from final location
    ** testing if installed package keeps a record of temporary installation path
    * DONE (gipsDA)
\end{ROutput}
The documentation and the source code of the package are available at  \href{https://github.com/AntoniKingston/gipsDAInzynierka}{github.com/AntoniKingston/gipsDAInzynierka} and \href{https://antonikingston.github.io/gipsDA/}{https://antonikingston.github.io/gipsDA/} repectively.
\section{Example usage}

\section{Implementation}
Standard R package development practices were followed.
The implementation is organized into two primary
components: the user-facing \texttt{models} module and the backend \texttt{gipsmult} module.
The core design philosophy was to extend the well-established and widely-used
functions of the \texttt{MASS} package.

\subsection{The \texttt{models} Module}
This module contains the primary, user-facing functions: \texttt{gipsLDA}, \texttt{gipsQDA}, and \texttt{gipsMultQDA}. Our implementation strategy was to adopt the complete structure of the original \texttt{MASS} functions to ensure a consistent user experience. The \texttt{gipsLDA} function is a direct modification of \texttt{lda.default()}. The other two models, \texttt{gipsQDA} and its variant \texttt{gipsMultQDA}, share a common foundation, as both are implemented by modifying the \texttt{qda.default()} method. For all three functions, our changes are precisely targeted to replace the standard covariance estimation with our \texttt{gips}-based projection.

\subsubsection{gipsLDA}
This function is designed as a direct, enhanced replacement for its \texttt{MASS::lda} counterpart. The implementation adopts the S3 method dispatch system from \texttt{MASS} to provide a familiar and flexible user interface.

\paragraph{S3 Methods and User Interface}
To ensure consistency with standard R practices, \texttt{gipslda} is an S3 generic function with several methods that handle different input types. The \texttt{gipslda.formula()}, \texttt{gipslda.data.frame()}, and \texttt{gipslda.matrix()} methods are almost identical to their equivalents in the \texttt{MASS} package. Their primary role is to process the input data—handling formulas, subsets, and missing values—before dispatching to the core computational engine, \texttt{gipslda.default()}. This design ensures that users familiar with \texttt{MASS::lda} can use \texttt{gipslda} with no change to their workflow.

\paragraph{The Core Engine: \texttt{gipslda.default()}}
This function contains the main algorithm and is where our modifications to the \texttt{MASS} code are concentrated. It accepts the following arguments:
\begin{description}
    \item[\texttt{x, grouping}] The input matrix of predictors and the vector of class labels.
    \item[\texttt{prior}] A vector of prior probabilities for the classes, defaulting to the class proportions in the training data.
    \item[\texttt{tol}] A tolerance threshold to detect zero-variance columns, defaulting to \texttt{1.0e-4}.
    \item[\texttt{weighted\_avg}] A logical parameter, defaulting to \texttt{FALSE}, which controls the covariance pooling strategy.
    \item[\texttt{MAP}] A logical parameter, defaulting to \texttt{TRUE}, which selects the estimation method (argmax or weighted average).
    \item[\texttt{optimizer}] A character string, defaulting to \texttt{NULL}. If \texttt{NULL}, it is automatically set to \texttt{"BF"} for $p < 10$ and \texttt{"MH"} for $p \ge 10$.
    \item[\texttt{max\_iter}] An integer, defaulting to \texttt{NULL}. If the optimizer is \texttt{"MH"} and this is \texttt{NULL}, it is set to \texttt{100} and a warning is issued.
\end{description}
The function's workflow begins with input validation (checking for finite values, consistent dimensions, empty groups), which is identical to the procedure in \texttt{MASS::lda.default()}. The key modifications occur in two stages:
\begin{enumerate}
    \item \textbf{Covariance Pooling:} The first modification intercepts the calculation of the pooled covariance matrix. The logic proceeds based on the \texttt{weighted\_avg} parameter:
    \begin{itemize}
        \item If \texttt{FALSE} (default), the standard unbiased pooled covariance matrix is computed, as in \texttt{MASS}.
        \item If \texttt{TRUE}, the code first calculates the individual covariance matrix $S_g$ for each class and then combines them using the formula $S = \frac{1}{n}\sum_{g} n_g S_{g}$.
    \end{itemize}
    \item \textbf{Gips Projection:} The resulting pooled covariance matrix is then passed as a single-element list to the \texttt{project\_covs()} helper function. This is the primary injection point of our methodology. The helper function applies the \texttt{gips} optimization and returns the final, projected covariance matrix.
\end{enumerate}
After this step, the remainder of the function proceeds exactly as in \texttt{MASS::lda.default()}, performing Singular Value Decomposition (SVD) and calculating the discriminant function coefficients, but now using the \textit{projected} covariance matrix.

\paragraph{Return Value and Post-Processing}
The function returns an object of class \texttt{gipslda}. This is a \texttt{list} containing the standard components from a \texttt{MASS::lda} object (e.g., \texttt{prior}, \texttt{counts}, \texttt{means}, \texttt{scaling}), plus an additional element, \texttt{optimization\_info}, which stores the output from the \texttt{gips} optimization. The package provides \texttt{predict()}, \texttt{print()}, and \texttt{plot()} methods for \texttt{gipslda} objects, which are also direct adaptations of their \texttt{MASS} counterparts. This ensures that a fitted \texttt{gipslda} object can be used for prediction and visualization without any side effects beyond the standard console or plot output. The \texttt{print.gipslda()} method has been extended to display the contents of the \texttt{optimization\_info} element.

\subsubsection{gipsQDA}
This function is designed as an enhanced replacement for \texttt{MASS::qda}, allowing for class-specific covariance matrices, each with its own unique symmetry structure. The implementation follows the S3 method dispatch system of \texttt{MASS} to maintain a familiar user interface.

\paragraph{S3 Methods and User Interface}
Consistent with standard R practices, \texttt{gipsqda} is an S3 generic function. The \texttt{gipsqda.formula()}, \texttt{gipsqda.data.frame()}, and \texttt{gipsqda.matrix()} methods are nearly identical to their counterparts in the \texttt{MASS} package. Their purpose is to preprocess the input data by handling formulas, subsets, and missing values before passing a clean data matrix and grouping factor to the core computational engine, \texttt{gipsqda.default()}.

\paragraph{The Core Engine: \texttt{gipsqda.default()}}
This function contains the main algorithm and is where our modifications to the \texttt{MASS} code are implemented. It accepts the following arguments:
\begin{description}
    \item[\texttt{x, grouping}] The input matrix of predictors and the vector of class labels.
    \item[\texttt{prior}] A vector of prior probabilities for the classes, defaulting to the class proportions in the training data.
    \item[\texttt{MAP}] A logical parameter, defaulting to \texttt{TRUE}, which selects the estimation method (argmax or weighted average).
    \item[\texttt{optimizer}] A character string, defaulting to \texttt{NULL}. If \texttt{NULL}, it is automatically set to \texttt{"BF"} for $p < 10$ and \texttt{"MH"} for $p \ge 10$.
    \item[\texttt{max\_iter}] An integer, defaulting to \texttt{NULL}. If the optimizer is \texttt{"MH"} and this is \texttt{NULL}, it is set to \texttt{100} and a warning is issued.
\end{description}

The function's workflow begins with input validation inherited from \texttt{MASS::qda.default()}, such as checking for finite values and ensuring that each group has enough observations to estimate a covariance matrix ($n_g > p$). The key modification is introduced within the main \texttt{for} loop that iterates through each class:
\begin{enumerate}
    \item For the current class, an empirical covariance matrix is estimated using \texttt{MASS::cov.mve()}.
    \item \textbf{Gips Projection:} This single covariance matrix is then passed as a single-element list to the \texttt{project\_covs()} helper function. The helper applies the \texttt{gips} optimization to find the optimal symmetry structure for this specific class and returns the projected covariance matrix.
    \item The remainder of the loop proceeds with the standard \texttt{qda} logic, performing Singular Value Decomposition (SVD) on the \textit{projected} matrix to calculate the scaling components and log-determinant for that class.
\end{enumerate}
This process is repeated for every class, resulting in a model where each class has its own individually optimized covariance structure.

\paragraph{Return Value and Post-Processing}
The function returns an object of class \texttt{gipsqda}. This is a \texttt{list} containing the standard components from a \texttt{MASS::qda} object (e.g., \texttt{prior}, \texttt{counts}, \texttt{means}, \texttt{scaling}, \texttt{ldet}), plus an additional element, \texttt{optimization\_info}. The package provides \texttt{predict()} and \texttt{print()} methods for \texttt{gipsqda} objects, which are direct adaptations of their \texttt{MASS} counterparts, ensuring standard functionality for prediction and inspection. The \texttt{print.gipsqda()} method has been extended to display the contents of the \texttt{optimization\_info} element.

\subsubsection{gipsMultQDA}
This function implements the intermediate model, which allows for class-specific covariance matrices but constrains them to share a single, common permutation symmetry. The implementation is a structural modification of \texttt{MASS::qda} and, like the other functions, uses the S3 method dispatch system.

\paragraph{S3 Methods and User Interface}
To maintain a consistent API, \texttt{gipsmultqda} is an S3 generic function. The \texttt{gipsmultqda.formula()}, \texttt{gipsmultqda.data.frame()}, and \texttt{gipsmultqda.matrix()} methods are nearly identical to their counterparts in the \texttt{MASS} package. They handle the initial data processing before dispatching to the core computational engine, \texttt{gipsmultqda.default()}.

\paragraph{The Core Engine: \texttt{gipsmultqda.default()}}
This function contains the main algorithm and is where our modifications to the \texttt{MASS} code are implemented. It accepts the following arguments:
\begin{description}
    \item[\texttt{x, grouping}] The input matrix of predictors and the vector of class labels.
    \item[\texttt{prior}] A vector of prior probabilities for the classes, defaulting to the class proportions in the training data.
    \item[\texttt{MAP}] A logical parameter, defaulting to \texttt{TRUE}, which selects the estimation method (argmax or weighted average).
    \item[\texttt{optimizer}] A character string, defaulting to \texttt{NULL}. If \texttt{NULL}, it is automatically set to \texttt{"BF"} for $p < 10$ and \texttt{"MH"} for $p \ge 10$.
    \item[\texttt{max\_iter}] An integer, defaulting to \texttt{NULL}. If the optimizer is \texttt{"MH"} and this is \texttt{NULL}, it is set to \texttt{100} and a warning is issued.
\end{description}
The function's workflow begins with the same input validation as \texttt{MASS::qda.default()}. The core logic is then executed in three distinct stages:
\begin{enumerate}
    \item \textbf{Covariance Collection:} A preliminary \texttt{for} loop iterates through all classes. In each iteration, it calculates the empirical covariance matrix for that class using \texttt{MASS::cov.mve()} and collects it into a list.
    \item \textbf{Joint Gips Projection:} After the loop, the entire list of covariance matrices is passed to the \texttt{project\_covs()} helper function. This is the primary injection point of the \texttt{gipsmult} methodology. The helper function finds a single, common symmetry structure that is jointly optimal for all classes and returns a list of the projected covariance matrices and the optimization results.
    \item \textbf{SVD and Scaling:} A second \texttt{for} loop then iterates through the classes again. For each class, it takes the corresponding projected matrix from the list returned by the helper function and performs the standard SVD and scaling calculations, as in \texttt{qda.default()}.
\end{enumerate}

\paragraph{Return Value and Post-Processing}
The function returns an object of class \texttt{gipsmultqda}. This is a \texttt{list} containing the standard components from a \texttt{MASS::qda} object (e.g., \texttt{prior}, \texttt{counts}, \texttt{means}, \texttt{scaling}, \texttt{ldet}), plus an additional element, \texttt{optimization\_info}. This element stores the results of the joint optimization, such as the single common MAP permutation or the posterior probabilities. The package provides \texttt{predict()} and \texttt{print()} methods for \texttt{gipsmultqda} objects, which are direct adaptations of their \texttt{MASS} counterparts. The \texttt{print.gipsmultqda()} method has been extended to display the contents of the \texttt{optimization\_info} element.

\subsubsection{Helper Functions}
A utility file (\texttt{models\_utils.R}) contains the core bridge functions that connect the modified \texttt{MASS} code to the \texttt{gips} logic. The implementation was streamlined to use a single, versatile function that handles both estimation strategies.

\begin{itemize}
    \item \texttt{project\_covs(emp\_covs, ns\_obs, MAP, optimizer, max\_iter, tol)}: This is the primary helper function that acts as a unified interface for both the argmax and weighted-average estimation methods. It takes in several arguments to control the process:
    \begin{description}
        \item[\texttt{emp\_covs}] A \texttt{list} of numeric \texttt{matrix} objects, where each matrix is an empirical covariance matrix for a class.
        \item[\texttt{ns\_obs}] A numeric \texttt{vector} containing the number of observations for each corresponding class.
        \item[\texttt{MAP}] A logical scalar. If \texttt{TRUE}, the function finds the single most probable permutation. If \texttt{FALSE}, it calculates the weighted-average projection.
        \item[\texttt{optimizer}] A character string specifying the search algorithm, either \texttt{'BF'} or \texttt{'MH'}. The resolution of the \texttt{'auto'} option is handled by the parent model function.
        \item[\texttt{max\_iter}] An integer specifying the number of iterations for the Metropolis-Hastings optimizer.
        \item[\texttt{tol}] A numeric tolerance threshold used when \texttt{MAP = FALSE}. When calculating the weighted average estimator from Equation~\eqref{eq:weighted_avg_estimator}, permutations with a posterior probability below this threshold are excluded from the summation to improve computational efficiency.
    \end{description}
    The function returns a \texttt{list} containing two named elements:
    \begin{itemize}
        \item \texttt{covs}: A \texttt{list} of numeric \texttt{matrix} objects, containing the final projected covariance matrices. This list has the same length and structure as the input \texttt{emp\_covs}.
        \item \texttt{opt\_info}: Contains information from the optimization process. If \texttt{MAP = TRUE}, this is the optimal permutation object. If \texttt{MAP = FALSE}, this is a named numeric \texttt{vector} of the posterior probabilities used for weighting.
    \end{itemize}

    \item \texttt{project\_matrix\_multiperm(emp\_cov, probs)}: This is a lower-level utility that implements the weighted-average projection for a single matrix. It takes in two arguments:
    \begin{description}
        \item[\texttt{emp\_cov}] A single numeric \texttt{matrix}.
        \item[\texttt{probs}] A named numeric \texttt{vector} where the names are permutations and the values are their posterior probabilities.
    \end{description}
    The function returns a single numeric \texttt{matrix} of the same dimensions as the input \texttt{emp\_cov}, representing the final weighted-average covariance matrix.
    \item \texttt{serialize\_for\_json(x)}: A function converting gipsDA classifier object to format eligible for saving to a json file, it takes in a single argument:
    \begin{description}
        \item[\texttt{x}] A gipsDA classifier (an object of class gipslda, gipsqda or gipsmultqda).
    \end{description}
    The function returns a list with possibly more lists nested inside.
    \item \texttt{deserialize\_from\_json(x)}: A function deserializing data loaded from json to create a gipsDA model object, it takes in a single argument:
    \begin{description}
        \item[\texttt{x}] A list constituting serialized model data.
    \end{description}
    The function returns a list which elements are gipsDA components with proper R metadata.
    \item \texttt{gipsDA\_to\_json(obj, file)}: A function saving gipsDA object to a json file, it takes in 2 arguments:
    \begin{description}
        \item[\texttt{obj}] A gipsDA object.
        \item[\texttt{filename}] Path where object is to be saved.
    \end{description}
    The function does not return anything.
    \item \texttt{gipsDA\_from\_json(file, classname)}: A function loading a gipsDA object from a json file, it takes in 2 arguments:
    \begin{description}
        \item[\texttt{obj}] Path where object is to be saved.
        \item[\texttt{classname}] Class of the object to be loaded.
    \end{description}
    The function returns an object of gipslda, gipqda or gipsmultqda class.
    \item \texttt{recursive\_length(x)}: A helper function to serialize\_for\_json(), it takis in a single argument:
    \begin{description}
        \item[\texttt{x}] Any R object.
    \end{description}
    The function returns the number of atomic elements in an object
    \item \texttt{desingularize(A, target)}: A function regularizing a square matrix so that the module of it's smallest eigenvalue is the target, it takes in 2 arguments:
    \begin{description}
        \item[\texttt{A}] A square nonsigular matrix.
        \item[\texttt{target}] Desired module of the smaller eigenvalue.
    \end{description}
    The function returns a square regularized matrix.
\end{itemize}
\section{Documentation}




\textcolor{red}{Tutaj chyba tak jak na prezentacji by sie to przedstawilo}

% ======================================================================
%                          Testing
% ======================================================================

\chapter{Testing Methodology}
\label{ch:testing_methodology}

This chapter presents the comprehensive evaluation methodology employed to assess the performance and robustness of the proposed \texttt{gipsDA} framework. To ensure a rigorous validation, the assessment strategy is twofold, encompassing controlled experiments on synthetic data and empirical benchmarking on diverse real-world datasets.

First, we detail the procedures for the \textbf{synthetic data analysis}. This includes a description of the generative algorithms used to simulate high-dimensional data with specific covariance structures, the methodology for constructing performance visualization curves, and the statistical framework applied to quantify the significance of the results.

Subsequently, we outline the approach for \textbf{real-world data benchmarking}. This section describes the necessary data preparation and preprocessing pipelines, explains the visualization techniques used to monitor model convergence, and provides a brief characterization of the specific datasets selected for this study.

In both experimental scenarios, the comparative analysis focuses on the performance of six distinct classification models. These include four variants of the proposed framework:
\begin{itemize}
    \item \texttt{gipsLDA} with weighted averaging (denoted in plots as \texttt{gipsldawa}),
    \item \texttt{gipsLDA} utilizing the classic estimator (\texttt{gipsldacl}),
    \item \texttt{gipsQDA} (\texttt{gipsqda}),
    \item \texttt{gipsMultQDA} (\texttt{gipsmultqda}).
\end{itemize}
These are benchmarked against two baseline models, referred to as \texttt{LDAmod} and \texttt{QDAmod}.
These baselines represent standard Linear and Quadratic Discriminant Analysis, respectively, augmented with a specific regularization technique.
This modification ensures numerical stability by constraining the smallest eigenvalue of the covariance matrix to a minimum threshold of $0.05$, a procedure that will be detailed in subsequent sections.

To quantitatively assess and compare the predictive power of these models, we employ \textbf{Classification Accuracy} as the primary evaluation metric. For a given test dataset consisting of $N$ observations, let $y_i$ denote the true class label of the $i$-th sample, and $\hat{y}_i$ denote the corresponding label predicted by the model. The accuracy is formally defined as:

\begin{equation}
    \text{Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(y_i = \hat{y}_i),
\end{equation}

where $\mathbb{I}(\cdot)$ is the indicator function, taking the value $1$ if the condition $y_i = \hat{y}_i$ is met (i.e., the prediction is correct) and $0$ otherwise. This metric provides a global measure of the model's effectiveness in correctly identifying class membership across the entire test set.

\section{Baseline Regularization Strategy}
\label{sec:baseline_regularization}

Standard estimators for Linear and Quadratic Discriminant Analysis (LDA and QDA) rely on the inversion of the sample covariance matrix, $\hat{\boldsymbol{\Sigma}}$. In high-dimensional settings, particularly when $p > n$, $\hat{\boldsymbol{\Sigma}}$ becomes singular or near-singular, possessing eigenvalues close to or equal to zero. This renders the standard precision matrix calculation unstable or impossible.

To ensure a fair comparison between the proposed \texttt{gips}-based models and the classical approaches, we implemented a modified version of the standard algorithms, denoted as \texttt{LDAmod} and \texttt{QDAmod}. These models incorporate a deterministic regularization step—referred to as \textit{desingularization}—which guarantees that the covariance matrix is strictly positive-definite with a bounded condition number.

\subsection{Mathematical Formulation}

The objective of this procedure is to transform the empirical covariance matrix $\mathbf{A}$ into a regularized matrix $\tilde{\mathbf{A}}$ such that its smallest eigenvalue, $\lambda_{\min}(\tilde{\mathbf{A}})$, is no less than a specific target threshold $\tau$. For the purposes of this study, we set $\tau = 0.05$.

If the smallest eigenvalue of the original matrix, denoted as $\lambda = \lambda_{\min}(\mathbf{A})$, already satisfies $\lambda \ge \tau$, no modification is performed. However, if $\lambda < \tau$, we apply a linear shrinkage towards the identity matrix $\mathbf{I}$. The regularized matrix is defined as:

\begin{equation}
    \tilde{\mathbf{A}} = \frac{\mathbf{A} + s\mathbf{I}}{1 + s},
\end{equation}
where $s$ is a non-negative scalar scaling factor.

\subsection{Derivation of the Scaling Factor}

The parameter $s$ is derived analytically to ensure the new smallest eigenvalue exactly matches the target $\tau$. The derivation proceeds as follows:

\begin{enumerate}
    \item Let the eigenvalues of $\mathbf{A}$ be denoted by $\lambda_i$. The smallest eigenvalue is $\lambda$.
    \item Adding a multiple of the identity matrix shifts the spectrum. The eigenvalues of $\mathbf{A} + s\mathbf{I}$ are $\lambda_i + s$. Consequently, the smallest eigenvalue becomes $\lambda + s$.
    \item Dividing by the scalar $(1+s)$ scales the eigenvalues. Thus, the smallest eigenvalue of the normalized matrix $\tilde{\mathbf{A}}$ is given by:
    \begin{equation}
        \lambda_{\min}(\tilde{\mathbf{A}}) = \frac{\lambda + s}{1 + s}.
    \end{equation}
\end{enumerate}

To enforce the condition $\lambda_{\min}(\tilde{\mathbf{A}}) = \tau$, we solve the following equation for $s$:

\begin{equation}
    \frac{\lambda + s}{1 + s} = \tau.
\end{equation}

Rearranging the terms:
\begin{align*}
    \lambda + s &= \tau(1 + s) \\
    \lambda + s &= \tau + \tau s \\
    s - \tau s &= \tau - \lambda \\
    s(1 - \tau) &= \tau - \lambda.
\end{align*}

This yields the closed-form solution for the scaling factor:
\begin{equation}
    s = \frac{\tau - \lambda}{1 - \tau}.
\end{equation}

This transformation ensures that \texttt{LDAmod} and \texttt{QDAmod} remain numerically solvable even in high-dimensional scenarios where standard implementations would fail due to singularity, providing a robust baseline for benchmarking the performance of \texttt{gipsDA}.

\section{Synthetic data}
\label{sec:synthetic_data}

To rigorously evaluate the performance of the proposed classification models, a comprehensive simulation study was designed. This involved generating synthetic datasets with controlled and well-defined properties, allowing us to systematically explore different assumptions regarding the underlying covariance structures of the classes.

\subsection{Experimental Workflow and Reproducibility}

A critical aspect of our methodology is the separation of the \textit{distribution definition} from the \textit{data sampling} phase. To ensure that performance comparisons across different sample sizes ($n$) reflect true model characteristics rather than random variations in the ground-truth parameters, the simulation proceeds in two distinct stages:

\begin{enumerate}
    \item \textbf{Parameter Fixing:} First, the "ground truth" parameters—specifically the class mean vectors ($\boldsymbol{\mu}_k$) and the true covariance matrices ($\boldsymbol{\Sigma}_k$)—are generated according to specific scenarios (detailed in Section \ref{sec:scenarios}). These parameters are serialized and saved to disk.
    \item \textbf{Monte Carlo Simulation:} Subsequently, for each defined sample size $n$, we conduct a series of independent experiments. In each experiment, training and test datasets are sampled from the distributions defined by the saved parameters.
\end{enumerate}

Consequently, each data point on the resulting performance curves represents the mean accuracy calculated over multiple independent repetitions (e.g., 20 or 50 runs). This approach minimizes the variance of the estimator and ensures that the observed trends are statistically robust.

\subsection{Data Generation Algorithm}

The core algorithm for defining the distributions consists of the following sequential steps:

\begin{enumerate}
    \item \textbf{Mean Vector Sampling:}
    For each of the $k$ classes, a mean vector $\boldsymbol{\mu}_k$ is sampled as a random point from a $p$-dimensional hypercube, $[0, 1]^p$. To prevent fortuitous overlap between classes, a minimum separation constraint is enforced. After sampling the set of $k$ vectors, the Euclidean distance between every pair of distinct means $(\boldsymbol{\mu}_i, \boldsymbol{\mu}_j)$ is calculated. If any pair satisfies $\|\boldsymbol{\mu}_i - \boldsymbol{\mu}_j\| \leq 0.05$, the entire set is discarded, and the sampling process is repeated. This safeguard prevents the generation of trivially difficult scenarios caused by nearly identical class centroids.

    \item \textbf{Covariance Matrix Construction (Spectral Decomposition):}
    To rigorously stress-test the models under varied spectral conditions, we employ a method based on spectral decomposition. This allows for direct control over the eigenvalues and the condition number of the matrix. The procedure is as follows:
    \begin{itemize}
        \item \textbf{Eigenvalue Generation:} A vector of eigenvalues $\boldsymbol{\lambda} = (\lambda_1, \dots, \lambda_p)$ is drawn from a specified distribution $\mathcal{D}$ (e.g., Exponential) and sorted such that $\lambda_1 \ge \dots \ge \lambda_p > 0$. Let $\Lambda = \text{diag}(\boldsymbol{\lambda})$.
        \item \textbf{Random Orthogonal Matrix:} A random matrix $\mathbf{Z} \in \mathbb{R}^{p \times p}$ is generated with entries $Z_{ij} \sim \mathcal{N}(0, 1)$. To obtain a uniformly distributed orthogonal matrix (Haar measure), we perform a QR decomposition: $\mathbf{Z} = \mathbf{Q}'\mathbf{R}$. The matrix $\mathbf{Q}'$ is adjusted by the signs of the diagonal elements of $\mathbf{R}$ to ensure uniqueness:
        \begin{equation}
            \mathbf{Q} = \mathbf{Q}' \cdot \text{diag}\left(\text{sgn}(R_{11}), \dots, \text{sgn}(R_{pp})\right).
        \end{equation}
        \item \textbf{Reconstruction:} The base covariance matrix is constructed as $\boldsymbol{\Sigma}_{\text{raw}} = \mathbf{Q} \Lambda \mathbf{Q}^\top$.
    \end{itemize}

    \item \textbf{Covariance Matrix Projection:}
    For scenarios involving the \texttt{gips} methodology, the raw covariance matrix $\boldsymbol{\Sigma}_{\text{raw}}$ is projected onto a specific permutation group structure. This step imposes the predefined symmetries required to test the specific hypotheses of our thesis (e.g., cyclic symmetry or block symmetry).

    \item \textbf{Separability Control:}
    To ensure a consistent level of classification difficulty across diverse scenarios, a scaling parameter $\psi$ is introduced. This parameter scales the covariance matrix ($\boldsymbol{\Sigma} = \boldsymbol{\Sigma}_{\text{base}} \cdot \psi$), where smaller values of $\psi$ reduce variance, making classes more compact and separable. The optimal value of $\psi$ is determined via an automated iterative search. Starting with an initial $\psi_{\text{init}}$, we iteratively test $\psi_{\text{new}} =\frac{\psi_{\text{init}}}{2^i}$ until a baseline LDA or QDA model achieves a predefined target accuracy on the generated data.

    This target threshold is calculated dynamically to adapt to the number of classes, $K$. We define a fixed signal strength parameter $S = 0.6$ and the baseline random accuracy as $Acc_{\text{base}} =\frac{1}{K}$. The theoretical target test accuracy is defined as:
    \begin{equation}
        Acc_{\text{test}} = Acc_{\text{base}} + (1 - Acc_{\text{base}}) \cdot S.
    \end{equation}
    To account for estimation variance during the calibration phase, the actual stopping criterion for the search (target training accuracy) is set slightly higher, capped at a maximum of 0.90:
    \begin{equation}
        Acc_{\text{target}} = \min(0.90, Acc_{\text{test}} + 0.10).
    \end{equation}
    This calibration guarantees that all scenarios are normalized to a comparable difficulty level before the main evaluation begins.

    \item \textbf{Final Data Generation:}
    Finally, for a given experiment with sample size $n_k$ for class $k$, the observations are sampled from the multivariate normal distribution $\mathcal{N}_p(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$.
\end{enumerate}

\subsection{Data Generation Scenarios}
\label{sec:scenarios}

To systematically evaluate the proposed models, we defined five distinct data generation scenarios. Each scenario corresponds to a specific set of assumptions regarding the covariance structure and symmetry of the underlying classes. These scenarios range from the most constrained (homoscedastic and symmetric) to the most flexible (heteroscedastic and unstructured), effectively creating "ideal" conditions for each of the tested algorithms.

Let $K$ denote the number of classes, $\boldsymbol{\Sigma}_g$ the covariance matrix for class $g$, and $\Gamma$ a permutation group defining the symmetry.

\begin{description}
    \item[Scenario 1: gipsLDA (Homoscedastic / Symmetric)] \hfill \\
    This scenario represents the strictest set of assumptions, targeting the \texttt{gipsLDA} model.
    \begin{itemize}
        \item \textbf{Covariance Structure:} We assume homoscedasticity, meaning all classes share a single, common covariance matrix:
        \[ \boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2 = \dots = \boldsymbol{\Sigma}_K = \boldsymbol{\Sigma}_{\text{common}}. \]
        \item \textbf{Symmetry:} This common matrix is projected onto a specific, non-trivial permutation group $\Gamma$. Consequently, the data exhibits a simplified dependency structure shared globally across the entire dataset.
    \end{itemize}

    \item[Scenario 2: gipsMultQDA (Heteroscedastic / Shared Symmetry)] \hfill \\
    This scenario is designed to test the intermediate flexibility of the \texttt{gipsMultQDA} model.
    \begin{itemize}
        \item \textbf{Covariance Structure:} We assume heteroscedasticity. Each class $g$ possesses a unique covariance matrix $\boldsymbol{\Sigma}_g$ with distinct eigenvalues and variance scales ($\boldsymbol{\Sigma}_i \neq \boldsymbol{\Sigma}_j$ for $i \neq j$).
        \item \textbf{Symmetry:} Crucially, while the matrices differ numerically, they are all constrained by the \textbf{same} underlying permutation group $\Gamma$. This implies that while the magnitude of correlations may vary between classes, the pattern of invariant relationships between features remains constant across the population.
    \end{itemize}

    \item[Scenario 3: gipsQDA (Heteroscedastic / Unique Symmetry)] \hfill \\
    This is the most general and flexible scenario involving symmetry, targeting the \texttt{gipsQDA} model.
    \begin{itemize}
        \item \textbf{Covariance Structure:} The data is fully heteroscedastic; each class has a distinct covariance matrix $\boldsymbol{\Sigma}_g$.
        \item \textbf{Symmetry:} Unlike the previous scenario, there is no shared structural constraint. Each class covariance matrix $\boldsymbol{\Sigma}_g$ is projected onto its own \textbf{unique} permutation group $\Gamma_g$. This models complex environments where the dependency structure of features changes fundamentally depending on the class label.
    \end{itemize}

    \item[Scenario 4: Classic LDA (Homoscedastic / Unstructured)] \hfill \\
    This serves as the baseline for linear classification.
    \begin{itemize}
        \item \textbf{Covariance Structure:} All classes share a single covariance matrix $\boldsymbol{\Sigma}_{\text{common}}$.
        \item \textbf{Symmetry:} No permutation symmetry is imposed (other than the trivial identity group). The covariance matrix is generated from a random spectral distribution without any projection, representing a standard, unstructured multivariate normal distribution.
    \end{itemize}

    \item[Scenario 5: Classic QDA (Heteroscedastic / Unstructured)] \hfill \\
    This serves as the baseline for quadratic classification.
    \begin{itemize}
        \item \textbf{Covariance Structure:} Each class has a unique, distinct covariance matrix $\boldsymbol{\Sigma}_g$.
        \item \textbf{Symmetry:} Similar to Scenario 4, no permutation constraints are applied. Each matrix is independently generated and unstructured. This represents the most difficult setting for estimation when $n$ is small, as the number of parameters to estimate is maximal ($\frac{Kp(p+1)}{2}$).
    \end{itemize}
\end{description}


\section{Real data}
\label{sec:real_world_data}

In addition to synthetic simulations, the proposed models were evaluated on a diverse collection of real-world datasets sourced from public repositories. These datasets span various domains, including medical diagnostics, finance, and industrial quality control, providing a robust test bed for assessing model performance under realistic conditions.

The selected datasets present a wide range of challenges, including high dimensionality, class imbalance, and complex feature dependencies. Furthermore, several datasets contain binary and categorical features. By including these, we consciously accept the violation of the multivariate normality assumption inherent to Discriminant Analysis, allowing us to assess the practical robustness of the \texttt{gipsDA} framework in non-ideal settings.

Below, we provide a detailed description of each dataset and the specific preprocessing pipelines applied to prepare the data for experimentation.

\subsection{Heart Failure Prediction Dataset}

\begin{itemize}
    \item \textbf{Domain:} Medical Diagnostics
    \item \textbf{Description:} Contains clinical features used to predict mortality caused by heart failure. It represents a classic binary classification problem.
    \item \textbf{Target Variable:} \texttt{HeartDisease} (1: Heart Disease, 0: Normal).
    \item \textbf{Dimensions:} 11 features, 918 observations.
    \item \textbf{Class Balance:} Normal (410), Heart Disease (508).
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction} \cite{heart}
\end{itemize}

\textbf{Preprocessing Pipeline:}
Since this dataset contains categorical variables (e.g., \texttt{Sex}, \texttt{ChestPainType}), we applied \textbf{One-Hot Encoding} (dummy encoding), dropping one level per factor to avoid perfect multicollinearity. This transformation introduces binary features, explicitly violating the Gaussian assumption, but allows the models to utilize all available information. Following encoding, we removed 1 feature that exhibited near-zero variance (dominance ratio $> 0.9$).

\subsection{Breast Cancer Wisconsin (Diagnostic)}

\begin{itemize}
    \item \textbf{Domain:} Medical Diagnostics
    \item \textbf{Description:} Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, describing characteristics of the cell nuclei.
    \item \textbf{Target Variable:} \texttt{diagnosis} (M: Malignant, B: Benign).
    \item \textbf{Dimensions:} 30 features, 569 observations.
    \item \textbf{Class Balance:} Benign (357), Malignant (212).
    \item \textbf{Source:} \url{https://archive.ics.uci.edu/dataset/17/breast-cancer-wisconsin-diagnostic} \cite{breast}
\end{itemize}

\textbf{Preprocessing Pipeline:}
The data was already in a clean, numerical format suitable for analysis. No additional encoding, imputation, or feature removal was required.

\subsection{EEG Brainwave Dataset: Feeling Emotions}

\begin{itemize}
    \item \textbf{Domain:} Medical / Neuroscience
    \item \textbf{Description:} Contains statistical features extracted from EEG brainwave signals collected from individuals in different emotional states.
    \item \textbf{Target Variable:} \texttt{emotion} (Positive, Neutral, Negative).
    \item \textbf{Dimensions:} 2548 features, 2132 observations.
    \item \textbf{Class Balance:} Balanced ($\approx 700$ per class).
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/birdy654/eeg-brainwave-dataset-feeling-emotions} \cite{birdy654_eeg_emotions}
\end{itemize}

\textbf{Preprocessing Pipeline:}
Given the extremely high dimensionality ($p=2548$), feature selection was critical.
\begin{enumerate}
    \item \textbf{Low Variance Filtering:} We removed 2 columns where a single value dominated more than 90\% of the observations.
    \item \textbf{Feature Selection:} We trained a Random Forest classifier on the full dataset and selected the \textbf{top 30 features} based on the Gini importance measure. This reduced the dimensionality to a manageable level while preserving the most predictive signals.
\end{enumerate}

\subsection{Credit Card Fraud Detection}

\begin{itemize}
    \item \textbf{Domain:} Rare-Event Detection / Finance
    \item \textbf{Description:} A dataset of credit card transactions aimed at identifying fraudulent activity.
    \item \textbf{Target Variable:} \texttt{Class} (1: Fraud, 0: Legitimate).
    \item \textbf{Dimensions:} 30 features, $\approx 285,000$ observations.
    \item \textbf{Class Balance:} Highly Imbalanced (only 492 fraud cases, $\approx 0.17\%$).
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud} \cite{ulb_creditcard_fraud}
\end{itemize}

\textbf{Preprocessing Pipeline:}
The extreme class imbalance poses a problem for standard classifiers. We applied a strict \textbf{undersampling} strategy to create a balanced subset for training:
\begin{enumerate}
    \item We retained all minority class samples (Fraud).
    \item We sampled the majority class (Legitimate) such that the final dataset composition was 40\% Fraud and 60\% Legitimate.
    \item This resulted in a dataset of 1230 observations.
    \item Finally, we removed 2 collinear features using a correlation cutoff of 0.9.
\end{enumerate}

\subsection{Steel Plates Faults Dataset}

\begin{itemize}
    \item \textbf{Domain:} Industrial Quality Control
    \item \textbf{Description:} Attributes of steel plates used to classify surface defects into 7 distinct fault types.
    \item \textbf{Target Variable:} Fault Type (e.g., \texttt{Pastry}, \texttt{Z\_Scratch}, \texttt{K\_Scatch}).
    \item \textbf{Dimensions:} 27 features, 1941 observations.
    \item \textbf{Class Balance:} Multi-class, ranging from 55 to 673 samples per class.
    \item \textbf{Source:} \url{https://archive.ics.uci.edu/dataset/198/steel-plates-faults} \cite{steel_plates_faults_198}
\end{itemize}

\textbf{Preprocessing Pipeline:}
\begin{enumerate}
    \item \textbf{Infinite Value Handling:} We scanned the dataset for infinite values, replaced them with \texttt{NA}, and subsequently removed rows containing missing values.
    \item \textbf{Scaling Small Values:} We applied the \texttt{fix\_tiny\_values} algorithm (described below) to prevent numerical underflow.
    \item \textbf{Collinearity Removal:} We removed 6 features that exhibited a pairwise correlation greater than 0.9.
\end{enumerate}

\subsection{Sensorless Drive Diagnosis Data Set}

\begin{itemize}
    \item \textbf{Domain:} Industrial Quality Control
    \item \textbf{Description:} Signals from a sensorless drive used to diagnose 11 different operating conditions (faults).
    \item \textbf{Target Variable:} Condition (11 classes).
    \item \textbf{Dimensions:} 48 features, 58,509 observations.
    \item \textbf{Class Balance:} Perfectly Balanced (5,319 per class).
    \item \textbf{Source:} \url{https://archive.ics.uci.edu/dataset/325/sensorless+drive+diagnosis} \cite{dataset_for_sensorless_drive_diagnosis_325}
\end{itemize}

\textbf{Preprocessing Pipeline:}
\begin{enumerate}
    \item \textbf{Scaling Small Values:} Some features contained values with very small magnitudes (e.g., $10^{-5}$), which can cause numerical instability in covariance matrix calculations. We applied a custom \texttt{fix\_tiny\_values} algorithm: for any column with a mean absolute value $< 0.01$, we scaled the column by a factor of $10^k$ to bring the magnitude into the standard range ($[1, 10]$).
    \item \textbf{Collinearity Removal:} We identified and removed 17 highly collinear features using a pairwise correlation threshold of 0.99.
\end{enumerate}

\section{Performance Evaluation and Visualization Strategy}
\label{sec:visualization_methodology}

To construct the performance curves, we employ a systematic evaluation protocol that assesses model accuracy across a range of sample sizes. This process is controlled by a specific set of hyperparameters designed to ensure both statistical reliability and computational feasibility.

\subsection{Sample Size Sampling (The X-axis)}

The performance of the classifiers is evaluated at discrete intervals of the sample size, denoted as $n$. To effectively capture the model behavior in the critical "small $n$" regime (where $p \approx n$ or $p > n$) while covering the convergence behavior at larger sample sizes, we do not sample $n$ linearly. Instead, we employ a \textbf{logarithmic spacing} strategy.

Let $N_{\min}$ denote the lower bound (controlled by the parameter \texttt{lb}, typically set to 16) and $N_{\max}$ denote the upper bound (controlled by the parameter \texttt{ub}). The sequence of sample sizes is generated as follows:

\begin{equation}
    n_i = \left\lfloor \exp\left( \ln(N_{\min}) + \frac{i-1}{G-1} (\ln(N_{\max}) - \ln(N_{\min})) \right) \right\rceil, \quad \text{for } i = 1, \dots, G,
\end{equation}
where $G$ is the \texttt{granularity} parameter (typically set to 10 or more). This results in $G$ distinct evaluation points on the x-axis, densely clustered at the lower end where the regularization impact is most significant.

\subsection{Monte Carlo Repetitions (The Y-axis)}

Each data point plotted on the performance curve represents the \textbf{mean accuracy} calculated over a set of independent experiments, controlled by the parameter \texttt{n\_experiments} (typically 30).

For a specific sample size $n_i$:
\begin{enumerate}
    \item We perform $M$ independent repetitions (where $M = \texttt{n\_experiments}$).
    \item In each repetition $j$:
    \begin{itemize}
        \item \textbf{Synthetic Data:} Fresh training and testing sets are sampled directly from the underlying multivariate normal distributions $\mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$ defined by the saved scenario parameters. This ensures true independence between experiments.
        \item \textbf{Real-World Data:} A subset of size $n_i$ is randomly sampled (without replacement) from the full dataset to form the training set, with the remaining data used for testing.
    \end{itemize}
    \item The data is split according to the \texttt{tr\_ts\_split} ratio (e.g., 0.7 for training).
    \item The accuracy $A_{ij}$ is computed.
\end{enumerate}

\textbf{Handling Model Failures:} A crucial aspect of our benchmarking is robustness. If a model fails to fit (e.g., due to numerical instability or singularity that the regularization failed to handle), the experiment is not discarded. Instead, it is penalized by assigning an accuracy of $0$. The reported value for sample size $n_i$ is the arithmetic mean:
\[ \bar{A}_i = \frac{1}{M} \sum_{j=1}^{M} A_{ij}. \]

\subsection{Optimization Strategy}

The \texttt{gips}-based models require finding the optimal permutation group. The search strategy is determined automatically based on the dimensionality $p$ of the data to balance precision and computation time:

\begin{itemize}
    \item \textbf{Small Dimension ($p \le 9$):} We utilize a \textbf{Brute Force} optimizer. This algorithm exhaustively searches the entire space of permutation groups, guaranteeing the discovery of the global maximum for the posterior probability.
    \item \textbf{High Dimension ($p > 9$):} The permutation space becomes too large for exhaustive search. Here, we employ the \textbf{Metropolis-Hastings} (MH) algorithm, a stochastic Markov Chain Monte Carlo (MCMC) method. The depth of the search is controlled by the \texttt{max\_iter} parameter (typically set to 1000 iterations).
\end{itemize}

Additionally, the \texttt{MAP} parameter controls the estimation logic: if \texttt{TRUE}, the model projects covariance matrices onto the single best permutation found (Maximum A Posteriori); if \texttt{FALSE}, it utilizes Bayesian Model Averaging over the visited permutations.

\subsection{Statistical Significance Testing}
\label{sec:statistical_testing}

To determine whether the observed performance differences between the proposed \texttt{gips}-based models and the baselines are statistically significant rather than artifacts of random sampling, we employ a non-parametric testing framework. Specifically, we utilize a Blocked Permutation Test (also known as a Blocked Randomization Test).

This choice is motivated by the structure of our experiment. Since model performance varies drastically across different sample sizes ($n$), simply aggregating results would violate the exchangeability assumption required for standard tests. The "blocking" factor in our design is the sample size; we must ensure that comparisons are made strictly within the same regime of data availability.

\subsubsection{Hypothesis Definition}

For a given pair of models, Model A (baseline, e.g., \texttt{QDAmod}) and Model B (proposed, e.g., \texttt{gipsQDA}), we define the following hypotheses:
\begin{itemize}
    \item $H_0$ (Null Hypothesis): There is no difference in performance between Model A and Model B. The assignment of accuracy scores to the models is arbitrary within each experimental block.
    \item $H_1$ (Alternative Hypothesis): Model B performs significantly better than Model A (one-sided test).
\end{itemize}

\subsubsection{Test Statistic Construction}

Let $G$ denote the number of sample size levels (granularity). For a specific sample size $n_k$ (where $k=1, \dots, G$), we perform $M$ independent experiments. Let $Acc_{k,j}^{(A)}$ and $Acc_{k,j}^{(B)}$ denote the accuracy of Model A and Model B, respectively, in the $j$-th experiment at sample size $n_k$.

First, we calculate the mean difference in accuracy for each sample size block $k$:
\begin{equation}
    \bar{\delta}_k = \frac{1}{M} \sum_{j=1}^{M} \left( Acc_{k,j}^{(B)} - Acc_{k,j}^{(A)} \right).
\end{equation}

The observed test statistic, $T_{\text{obs}}$, is defined as the grand mean of these block-wise differences:
\begin{equation}
    T_{\text{obs}} = \frac{1}{G} \sum_{k=1}^{G} \bar{\delta}_k.
\end{equation}
This statistic effectively aggregates the performance gain across the entire learning curve, treating each sample size level as equally important.

\subsubsection{Permutation Procedure}

To approximate the null distribution of the test statistic, we perform $B = 5000$ Monte Carlo permutations. The procedure preserves the block structure of the data:

\begin{enumerate}
    \item \textbf{Within-Block Swapping:} For every specific sample size $n_k$ and every experiment $j$, we randomly swap the accuracy values of Model A and Model B with a probability of $0.5$. Formally, we define a Bernoulli variable $S_{k,j} \sim \text{Bern}(0.5)$.
    \[
    (X, Y) =
    \begin{cases}
    (Acc_{k,j}^{(A)}, Acc_{k,j}^{(B)}) & \text{if } S_{k,j} = 0 \\
    (Acc_{k,j}^{(B)}, Acc_{k,j}^{(A)}) & \text{if } S_{k,j} = 1
    \end{cases}
    \]
    \item \textbf{Statistic Recomputation:} Using these permuted labels, we calculate a new permuted statistic $T^*_b$.
    \item \textbf{P-value Calculation:} After $B$ repetitions, the empirical p-value is calculated as the proportion of permuted statistics that are greater than or equal to the observed statistic:
    \begin{equation}
        p\text{-value} = \frac{1}{B} \sum_{b=1}^{B} \mathbb{I}(T^*_b \ge T_{\text{obs}}).
    \end{equation}
\end{enumerate}

A low p-value (typically $< 0.05$) indicates that the observed performance improvement of Model B over Model A is statistically significant and unlikely to have occurred by chance.


% ======================================================================
%                          Experiments
% ======================================================================

\chapter{Experimental Results}
\label{ch:results}

This chapter presents the empirical evaluation of the \texttt{gipsDA} framework. The results are divided into two main parts: controlled simulations on synthetic data and benchmarking on real-world datasets.

\section{Synthetic Data Experiments}

In this section, we analyze the performance of the proposed classification models against the regularized baselines. To ensure clarity when interpreting the results, we first establish the mapping between the theoretical model names and the labels used in the visualization plots.

\subsection{Model Nomenclature}

The comparative analysis involves six distinct models. In the generated plots and subsequent discussions, the following abbreviations are used:

\begin{itemize}
    \item \textbf{\texttt{gipsldawa}}: The \texttt{gipsLDA} model utilizing the \textbf{weighted average} estimator (Bayesian Model Averaging).
    \item \textbf{\texttt{gipsldacl}}: The \texttt{gipsLDA} model utilizing the \textbf{classic} estimator (unbiased pooled covariance).
    \item \textbf{\texttt{gipsqda}}: The \texttt{gipsQDA} model, where each class has a unique covariance structure.
    \item \textbf{\texttt{gipsmultqda}}: The \texttt{gipsMultQDA} model, where classes share a common permutation symmetry.
    \item \textbf{\texttt{lda}}: The baseline Linear Discriminant Analysis model, modified with the eigenvalue regularization described in Section \ref{sec:baseline_regularization} (\texttt{LDAmod}).
    \item \textbf{\texttt{qda}}: The baseline Quadratic Discriminant Analysis model, modified with eigenvalue regularization (\texttt{QDAmod}).
\end{itemize}

\subsection{Experimental Scope and Hyperparameters}

We conducted an extensive simulation study comprising over 100 independent large-scale experiments. These experiments explored a vast grid of hyperparameters, including varying dimensions ($p$), numbers of classes ($k$), eigenvalue distributions (Exponential, Log-Normal, Chi-squared), and optimization strategies (MAP vs. BMA). From this extensive collection, we selected \textbf{six representative configurations} (parameter sets) for detailed discussion in this chapter. These selections best illustrate the specific strengths and limitations of the proposed methods.

The optimization strategy for the \texttt{gips}-based models was strictly dependent on the problem dimensionality $p$:
\begin{itemize}
    \item \textbf{For $p=5$:} The permutation space is sufficiently small ($5! = 120$) to allow for an exhaustive search. Therefore, the \textbf{Brute Force} optimizer was employed to guarantee finding the global maximum of the posterior probability.
    \item \textbf{For $p=10$:} The permutation space becomes computationally intractable ($10! \approx 3.6 \times 10^6$). In these cases, the stochastic \textbf{Metropolis-Hastings} algorithm was used, constrained by a fixed \texttt{max\_iter} parameter to balance accuracy and runtime.
\end{itemize}

\subsection{Permutation Selection Protocol}

A critical component of the data generation process is the selection of the "ground truth" permutation symmetries imposed on the covariance matrices. We defined specific pools of permutations for $p=5$ and $p=10$, ranging from simple structures (e.g., sparse transpositions) to complex ones (e.g., full cycles).

The selection logic depends on the target scenario:

\begin{enumerate}
    \item \textbf{Shared Symmetry Scenarios (gipsLDA, gipsMultQDA):}
    In these scenarios, the data generation assumes a single permutation structure is shared across all classes. We select \textbf{one} specific permutation type from the pool—either \texttt{sparse\_transposition}, \texttt{dense\_transposition}, or \texttt{full\_cycle}—and apply it to generate the covariance matrices for all groups.

    \item \textbf{Unique Symmetry Scenario (gipsQDA):}
    This scenario assumes that each class possesses a distinct symmetry structure. If the number of classes is $k$, we select the first $k$ permutations from the pre-defined list.
    \begin{itemize}
        \item For example, if $k=5$, we take the first 5 permutations from the $p$-specific list.
        \item If $k=10$, we utilize all 10 available permutations in the list.
    \end{itemize}
\end{enumerate}

The specific permutation lists used for generation are defined as follows:

\begin{itemize}
    \item \textbf{For $p=5$:}
    \begin{enumerate}
        \item $(1,2)$ [sparse transposition]
        \item $(1,2)(3,4)$ [dense transposition]
        \item $(1,2,3,4,5)$ [full cycle]
        \item $(1,3)(2,4)$
        \item $(1,2,3)(4,5)$
        \item ... (and so on up to 10 distinct permutations).
    \end{enumerate}
    \item \textbf{For $p=10$:}
    \begin{enumerate}
        \item $(1,2)(5,6)(8,9)$ [sparse transposition]
        \item $(1,2)(3,4)(5,6)(7,8)(9,10)$ [dense transposition]
        \item $(1,2,\dots,10)$ [full cycle]
        \item $(1,3)(2,4)(7,9)$
        \item ... (and so on up to 10 distinct permutations).
    \end{enumerate}
\end{itemize}

Below, we present the results for the selected parameter configurations.
For each configuration, a composite figure displays five subplots corresponding to the five fundamental data generation scenarios defined in Chapter \ref{ch:testing_methodology}.

% ------------------------------------------------------------------------
% SIMULATION SETS
% ------------------------------------------------------------------------
\subsection{Simulation Set 1: High Dimension and 10 Classes}
\label{sec:sim_set_1}

\textbf{Configuration:} $p=10$, $k=10$, Distribution: Chi-squared, MAP: \texttt{FALSE}, Main Permutation: Dense Transposition.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_10_10_chisq_dense_trans_FALSE}
    \caption{Learning curves for Simulation Set 1. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_1}
\end{figure}

\textbf{Observations:}
In this high-dimensional setting, we observe that... \textcolor{red}{[Tu wpiszemy szybkie wnioski: np. czy gipsQDA wygrywa w swoim scenariuszu? Czy gipsLDA jest stabilne?].}

\begin{table}[H]
    \centering
    \begin{tabular}{lllc}
        \toprule
        \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{p-value} \\
        \midrule
        qda & LDAmod & gipsLDA classic & $0.0274$ \\
        qda & QDAmod & gipsMultQDA & $0.0032$ \\
        gipsqda & QDAmod & gipsMultQDA & $0$ \\
        gipsmultqda & QDAmod & gipsMultQDA & $0$ \\
        lda & LDAmod & gipsLDA classic & $0.0204$ \\
        lda & QDAmod & gipsMultQDA & $0.002$ \\
        gipslda & LDAmod & gipsLDA classic & $4*10^{-4}$ \\
        gipslda & QDAmod & gipsMultQDA & $2*10^{-4}$ \\
        \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results (Blocked Permutation Test) for Simulation Set 1. The table presents p-values for specific model pairs deemed most relevant for the analysis.}
    \label{tab:stat_sim_1}
\end{table}

\textbf{Statistical Analysis:}
The table confirms that the advantage of \texttt{gipsQDA} in heteroscedastic scenarios is statistically significant. However, for the homoscedastic case...


\subsection{Simulation Set 2: High Dimension and Binary Classification}
\label{sec:sim_set_2}

\textbf{Configuration:} $p=10$, $k=2$, Distribution: Chi-squared, MAP: \texttt{FALSE}, Main Permutation: Dense Transposition.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_10_2_chisq_dense_trans_FALSE}
    \caption{Learning curves for Simulation Set 2. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_2}
\end{figure}

\textbf{Observations:}
In this high-dimensional setting, we observe that... \textcolor{red}{[Tu wpiszemy szybkie wnioski: np. czy gipsQDA wygrywa w swoim scenariuszu? Czy gipsLDA jest stabilne?].}

\begin{table}[H]
    \centering
    \begin{tabular}{lllc}
        \toprule
        \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{p-value} \\
        \midrule
        qda & LDAmod & gipsLDA classic & $0$ \\
        qda & LDAmod & gipsLDA classic & $0.0104$ \\
        qda & QDAmod & gipsMultQDA & $0$ \\
        gipsqda & LDAmod & gipsLDA classic & $4*10^{-4}$ \\
        gipsqda & LDAmod & gipsLDA weighted average & $0$ \\
        gipsqda & gipsLDA classic & gipsLDA weighted average & $0.0386$ \\
        gipsqda & QDAmod & gipsMultQDA & $4*10^{-4}$ \\
        gipsmultqda & LDAmod & gipsLDA classic & $0$ \\
        gipsmultqda & LDAmod & gipsLDA weighted average & $0$ \\
        gipsmultqda & QDAmod & gipsMultQDA & $0.001$ \\
        lda & LDAmod & gipsLDA classic & $0$ \\
        lda & LDAmod & gipsLDA weighted average & $0$ \\
        lda & QDAmod & gipsMultQDA & $0$ \\
        gipslda & LDAmod & gipsLDA classic & $0$ \\
        gipslda & LDAmod & gipsLDA weighted average & $0$ \\
        gipslda & QDAmod & gipsMultQDA & $0$ \\
        \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results (Blocked Permutation Test) for Simulation Set 2. The table presents p-values for specific model pairs deemed most relevant for the analysis.}
    \label{tab:stat_sim_2}
\end{table}

\textbf{Statistical Analysis:}
The table confirms that the advantage of \texttt{gipsQDA} in heteroscedastic scenarios is statistically significant. However, for the homoscedastic case...


\subsection{Simulation Set 3: Low Dimension and 10 Classes}
\label{sec:sim_set_3}

\textbf{Configuration:} $p=5$, $k=10$, Distribution: Log-Normal, MAP: \texttt{TRUE}, Main Permutation: Full Cycle.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_5_10_lnorm_full_cycle_TRUE}
    \caption{Learning curves for Simulation Set 3. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_3}
\end{figure}

\textbf{Observations:}
In this high-dimensional setting, we observe that... \textcolor{red}{[Tu wpiszemy szybkie wnioski: np. czy gipsQDA wygrywa w swoim scenariuszu? Czy gipsLDA jest stabilne?].}

\begin{table}[H]
    \centering
    \begin{tabular}{lllc}
        \toprule
        \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{p-value} \\
        \midrule
        gipslda & QDAmod & gipsQDA & $0.0092$ \\
        gipslda & LDAmod & gipsMultQDA & $0$ \\
        \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results (Blocked Permutation Test) for Simulation Set 3. The table presents p-values for specific model pairs deemed most relevant for the analysis.}
    \label{tab:stat_sim_3}
\end{table}

\textbf{Statistical Analysis:}
The table confirms that the advantage of \texttt{gipsQDA} in heteroscedastic scenarios is statistically significant. However, for the homoscedastic case...


\subsection{Simulation Set 4: Low Dimension and Binary Classification}
\label{sec:sim_set_4}

\textbf{Configuration:} $p=5$, $k=2$, Distribution: Chi-squared, MAP: \texttt{False}, Main Permutation: Dense Transposition.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_5_2_chisq_dense_trans_FALSE}
    \caption{Learning curves for Simulation Set 4. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_4}
\end{figure}

\textbf{Observations:}
In this high-dimensional setting, we observe that... \textcolor{red}{[Tu wpiszemy szybkie wnioski: np. czy gipsQDA wygrywa w swoim scenariuszu? Czy gipsLDA jest stabilne?].}

\begin{table}[H]
    \centering
    \begin{tabular}{lllc}
        \toprule
        \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{p-value} \\
        \midrule
        qda & LDAmod & gipsLDA weighted average & $0.0028$ \\
        qda & gipsLDA classic & gipsLDA weighted average & $0.0306$ \\
        gipsqda & LDAmod & gipsLDA weighted average & $0.0244$ \\
        gipsqda & gipsLDA classic & gipsLDA weighted average & $0.046$ \\
        gipsqda & QDAmod & gipsQDA & $0.0374$ \\
        gipsqda & QDAmod & gipsMultQDA & $0.004$ \\
        \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results (Blocked Permutation Test) for Simulation Set 4. The table presents p-values for specific model pairs deemed most relevant for the analysis.}
    \label{tab:stat_sim_4}
\end{table}

\textbf{Statistical Analysis:}
The table confirms that the advantage of \texttt{gipsQDA} in heteroscedastic scenarios is statistically significant. However, for the homoscedastic case...


\subsection{Simulation Set 5: Low Dimension and 5 Classes}
\label{sec:sim_set_5}

\textbf{Configuration:} $p=5$, $k=5$, Distribution: Exponential, MAP: \texttt{False}, Main Permutation: Sparse Transposition.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_5_5_exp_sparse_trans_FALSE}
    \caption{Learning curves for Simulation Set 5. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_5}
\end{figure}

\textbf{Observations:}
In this high-dimensional setting, we observe that... \textcolor{red}{[Tu wpiszemy szybkie wnioski: np. czy gipsQDA wygrywa w swoim scenariuszu? Czy gipsLDA jest stabilne?].}

\begin{table}[H]
    \centering
    \begin{tabular}{lllc}
        \toprule
        \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{p-value} \\
        \midrule
        gipsmultqda & LDAmod & gipsLDA classic & $0.0338$ \\
        \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results (Blocked Permutation Test) for Simulation Set 5. The table presents p-values for specific model pairs deemed most relevant for the analysis.}
    \label{tab:stat_sim_5}
\end{table}

\textbf{Statistical Analysis:}
The table confirms that the advantage of \texttt{gipsQDA} in heteroscedastic scenarios is statistically significant. However, for the homoscedastic case...


\section{Real-World Data Results}

In this section, we evaluate the models on the six real-world datasets described in Chapter \ref{ch:testing_methodology}.
Unlike the synthetic experiments, the "true" covariance structure is unknown, and the data may violate Gaussian assumptions.

Each subsection presents the learning curve for a specific dataset, followed by a statistical comparison of the best-performing \texttt{gips} model against its relevant baseline.

% ========================================================================
% SZABLON DLA DANYCH REALNYCH
% ========================================================================

\subsection{Heart Failure Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/heart_plot}
    \caption{Classification accuracy on the Heart Failure dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:real_heart}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & $4*10^{-4}$ \\
        LDAmod vs gipsLDA weighted average & $0.0402$ \\
        QDAmod vs gipsQDA & $0.0386$ \\
        QDAmod vs gipsMultQDA & $0.0018$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison for Heart Failure dataset.}
\end{table}

\textbf{Analysis:}
The \texttt{gipsLDA classic} model demonstrates remarkable stability for very small sample sizes ($n < 50$)...


\subsection{Breast Cancer Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/breast_plot}
    \caption{Classification accuracy on the Breast Cancer dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:breast}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & $0.0208$ \\
        LDAmod vs gipsLDA weighted average & $0.8234$ \\
        QDAmod vs gipsQDA & $0.0054$ \\
        QDAmod vs gipsMultQDA & $6*10^{-4}$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison for Breast Cancer dataset.}
\end{table}

\textbf{Analysis:}
The \texttt{gipsLDA classic} model demonstrates remarkable stability for very small sample sizes ($n < 50$)...

\subsection{Sensorless Drive Diagnosis Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/sensorless_plot}
    \caption{Classification accuracy on the Sensorless Drive Diagnosis dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:sensorless}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & $0.9218$ \\
        LDAmod vs gipsLDA weighted average & $0.952$ \\
        QDAmod vs gipsQDA & $0.9874$ \\
        QDAmod vs gipsMultQDA & $0.9904$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison for Sensorless Drive Diagnosis dataset.}
\end{table}

\textbf{Analysis:}
The \texttt{gipsLDA classic} model demonstrates remarkable stability for very small sample sizes ($n < 50$)...

\subsection{Credit Card Fraud Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/creditcard_plot}
    \caption{Classification accuracy on the Credit Card Fraud dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:creditcard}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & $0.0062$ \\
        LDAmod vs gipsLDA weighted average & $1$ \\
        QDAmod vs gipsQDA & $1$ \\
        QDAmod vs gipsMultQDA & $1$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison for Credit Card Fraud dataset.}
\end{table}

\textbf{Analysis:}
The \texttt{gipsLDA classic} model demonstrates remarkable stability for very small sample sizes ($n < 50$)...


\subsection{EEG Brainwave Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/emotions_plot}
    \caption{Classification accuracy on the EEG Brainwave dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:emotions}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & $0.038$ \\
        LDAmod vs gipsLDA weighted average & $1$ \\
        QDAmod vs gipsQDA & $0.3908$ \\
        QDAmod vs gipsMultQDA & $0.9274$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison for EEG Brainwave dataset.}
\end{table}

\textbf{Analysis:}
The \texttt{gipsLDA classic} model demonstrates remarkable stability for very small sample sizes ($n < 50$)...


\subsection{Steel Plates Faults Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/steel_plot}
    \caption{Classification accuracy on the Steel Plates Faults dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:steel}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & $0.2888$ \\
        LDAmod vs gipsLDA weighted average & $1$ \\
        QDAmod vs gipsQDA & $1$ \\
        QDAmod vs gipsMultQDA & $1$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison for Steel Plates Faults dataset.}
\end{table}

\textbf{Analysis:}
The \texttt{gipsLDA classic} model demonstrates remarkable stability for very small sample sizes ($n < 50$)...



\section{Summary of Findings}
\textcolor{red}{Do napisania od nowa}
Based on both synthetic and real-world experiments, we can draw the following conclusions:
\begin{itemize}
    \item \textbf{Regularization Efficacy:} In scenarios where $n \approx p$ or $n < p$, \texttt{gipsQDA} and \texttt{gipsMultQDA} consistently outperform the unstructured \texttt{QDAmod}, proving that permutation symmetry is a powerful regularizer.
    \item \textbf{Model Stability:} The \texttt{gipsLDA classic} estimator shows superior stability compared to the weighted average variant, which tends to underperform in...
    \item \textbf{Real-World Applicability:} Even with non-Gaussian features (e.g., Heart Failure), the structural constraints imposed by \texttt{gips} do not hinder predictive performance and often improve data efficiency.
\end{itemize}

% ======================================================================
%                          Conclusion
% ======================================================================

\chapter{Conclusion and Discussion}

\section{Reflection on the approach}

\section{Future Work}
\textcolor{red}{Tutaj wiem co napisac, troche jak w prezentacji}

% ======================================================================
%                          Rest
% ======================================================================

\chapter{Division of Work}

\begin{table}[H]
        \centering
        \small
        % Zwiększenie odstępów między wierszami dla lepszej czytelności
        \renewcommand{\arraystretch}{1.1}

        \begin{tabularx}{\textwidth}{l X c c}
            \toprule
            \textbf{Category} & \textbf{Task / Responsibility} & \textbf{Antoni} & \textbf{Norbert} \\
            \midrule
            \textbf{Theory} & Mathematical derivation of the \texttt{gipsmult} posterior & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Package} & \texttt{gipsmult} module: Optimization engines (MH, BF) & \textcolor{teal}{$\checkmark$} & \textcolor{gray}{$\times$} \\
            \addlinespace
            \textbf{Package} & \texttt{models} module: S3 methods, API consistency & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Package} & Model serialization (JSON engine) & \textcolor{teal}{$\checkmark$} & \textcolor{gray}{$\times$} \\
            \addlinespace
            \textbf{Testing} & Unit testing framework and "Negative Testing" suite & \textcolor{gray}{$\times$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Experiments} & Synthetic data generation and benchmarking & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Experiments} & Real-world dataset benchmarking & \textcolor{gray}{$\times$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Documentation} & Technical reports, User's Manual, and Thesis Writing & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \bottomrule
        \end{tabularx}
        \caption{Summary of individual and shared contributions.}
    \end{table}

\textcolor{red}{Tutaj podzial pracy tekstu inzynierki czyli tabelka kto ktory rozdzial w pracy napisal itd}

% ------------------------------- BIBLIOGRAPHY ---------------------------
% LEXICOGRAPHICAL ORDER BY AUTHORS' LAST NAMES
% FOR AMBITIOUS ONES - USE BIBTEX


%\begin{thebibliography}{20} % IF YOU HAVE MORE REFERENCES, WRITE THE BIGGER NUMBER
%
%\bibitem[1]{Ktos} A. Author, \emph{Title of a book}, Publisher, year, page--page.
%\bibitem[2]{Innyktos} J. Bobkowski, S. Dobkowski, Title of an article, \emph{Magazine X, No. 7}, year, PAGE--PAGE.
%\bibitem[3]{B} C. Brink, Power structures, \emph{Algebra Universalis 30(2)}, 1993, 177--216.
%\bibitem[4]{H} F. Burris, H. P. Sankappanavar, \emph{A Course of Universal Algebra}, Springer-Verlag, New York, 1981.
%\end{thebibliography}

% Wybór stylu bibliografii (np. plain, abbrv, alpha, unsrt)
\bibliographystyle{plain}

% Wskazanie pliku z bazą danych (bez rozszerzenia .bib)
\bibliography{thesis-en}

\pagenumbering{gobble}
\thispagestyle{empty}

\appendix
\chapter{Derivation of a posteriori distribution in \texttt{gipsmult} model}
We examine m multidimensional gaussian samples $Z^{(1)}_{g},\dots Z_g^{(n_g)}$ under
set $\{\mathcal{K}_g=\kappa_g, \Gamma=c\} $ consisting of i.i.d. random
vectors $\mathcal{N}_p(0, \kappa_g^{-1})$.
Let $\Gamma$ be a discrete random variable uniformly distributed over the set of
$
\mathcal{C} := \{ \langle \sigma \rangle : \sigma \in \mathfrak{S}_p \}
$
cyclic subgroups of $\mathfrak{S}_p$. We assume that each $\mathcal{K}_g$ under set $\Gamma = c$ follows conjugate Diaconis-Ylvisacker a priori distribution \cite{diaconis1979conjugate} defined by its PDF:
\begin{equation}
    f_{\mathcal{K}_g \mid \Gamma = c}(k_g)
  = \frac{1}{I_{c}(\delta, D)}
    \operatorname{Det}(k_g)^{(\delta - 2)/2}
    e^{-\tfrac12 \operatorname{tr}[D^{-1} k_g] }
    \mathbf{1}_{\mathcal{P}_{c}}(k_g),
\end{equation}
where $\delta>1$ and $D\in \mathcal{P}_c$ are hyperparameters and $I_c(\delta, D)$ is a normalizing constant.
We start with standard bayesian reasoning linking the probability of any given permutation conditioned by data with probability of specific data conditioned by given permutation:
\begin{equation}
\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) =
\frac{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right) \mathbbm{P}\left(\Gamma = c \right)}{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right)} \propto \mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right).
\end{equation}
Writing this in more probabilistic terms and conditioning $\mathbbm{X}$ directly on precision matrices we get:
\begin{equation}
\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) \propto f_{\mathbbm{X} \; \mid \; \Gamma = c, \; \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) = \idotsint\limits_{\mathcal{P}_c^m}
    f_{\mathbbm{X} \mid \mathcal{K} = \kappa, \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x})\,
    f_{\mathcal{K} \mid \Gamma = c}(\kappa)
    \, d \kappa
\end{equation}
Let us now tackle the first factor under the integral:
\begin{equation}
    \begin{align}
        f_{\mathbbm{X} \mid \mathcal{K} = \kappa, \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) = \prod_{i=i}^{n}(2\pi)^{-\frac{p}{2}}\left|K_{g_i}\right|^{\frac{1}{2}}
        \exp \left(-\frac{1}{2}x_i^T K_{g_i}x_i\right) \propto \\ \propto \prod_{g=1}^m\left|K_g\right|^{\frac{n_g}{2}}\exp\left(-\frac{1}{2}\sum_{g=1}^m\sum_{j=1}^{n_g}
        x_{g,j}^T K_g x_{g,j}\right)
    \end{align}
\end{equation}
As inside exponential we are summing real numbers we can replace them by their trace:
\begin{equation}
    \begin{align}
        \exp\left(-\frac{1}{2}\sum_{g=1}^m\sum_{j=1}^{n_g}
        x_{g,j}^T K_g x_{g,j}\right) = \exp\left(-\frac{1}{2}\sum_{g=1}^m\sum_{j=1}^{n_g}
        tr\left(x_{g,j}^T K_g x_{g,j}\right)\right) = \\ =
        \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_g\sum_{j=1}^{n_g}
        x_{g,j}x_{g,j}^T\right)\right) = \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_{g}U_g\right)\right)
    \end{align}
\end{equation}
Where:
\begin{equation}
    U_g = \sum_{j=1}^{n_g}
        x_{g,j}x_{g,j}^T
\end{equation}
As for the second factor we assume precission matrices are independent:
\begin{equation}
    f_{\mathcal{K} \mid \Gamma = c}(\kappa) = \prod_{g=1}^m f_{K_g \mid \Gamma = c} (k_g)
\end{equation}
Combining all of these we get:
\begin{equation}
    \begin{align}
        \idotsint\limits_{\mathcal{P}_c^m}
    f_{\mathbbm{X} \mid \mathcal{K} = \kappa, \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x})\,
    f_{\mathcal{K} \mid \Gamma = c}(\kappa)
    \, d \kappa = \\ =\idotsint\limits_{\mathcal{P}_c^m} \prod_{g=1}^m \left|K_g\right|^{\frac{n_g}{2}}
        \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_{g}U_g\right)\right) f_{K_g \mid \Gamma = c}(k_g) \, d \kappa = \\ =
        \prod_{g=1} ^m \int\limits_{\mathcal{P}_c} \left|K_g\right|^{\frac{n_g}{2}}
        \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_{g}U_g\right)\right) f_{K_g \mid \Gamma = c}(k_g) \, dk_g
    \end{align}
\end{equation}
Applying results from \cite{JSSv112i07} for each factor we arrive at the result:
\begin{equation}\label{UltimateAPosterioriFormula}
    \mathbbm{P}\left(\Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right) \propto
    f_{\mathbbm{X} \; \mid \; \Gamma = c, \; \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) \propto
    \prod_{g=1}^m\frac{I_c\left(\delta + n_g, D+U_g\right)}{I_c\left(\delta, D\right)},
\end{equation}






% ----------------------- LIST OF SYMBOLS AND ABBREVIATIONS ------------------
\chapter*{List of symbols and abbreviations}

\begin{tabular}{cl}
nzw. & nadzwyczajny \\
* & star operator \\
$\widetilde{}$ & tilde
\end{tabular}
\\
If you don't need it, delete it.
\thispagestyle{empty}


% ----------------------------  LIST OF FIGURES --------------------------------
\listoffigures
\thispagestyle{empty}
If you don't need it, delete it.


% -----------------------------  LIST OF TABLES --------------------------------
\renewcommand{\listtablename}{Spis tabel}
\listoftables
\thispagestyle{empty}
If you don't need it, delete it.

% -----------------------------  LIST OF APPENDICES ---------------------------
\chapter*{List of appendices}
\begin{enumerate}
\item Appendix 1
\item Appendix 2
\item In case of no appendices, delete this part.
\item texy probabilistyczne
%ogólny ogólnik
%\begin{equation*}
%\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) =
%\frac{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right)}{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right)} \propto \mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right)
%\end{equation*}
%wzór japońcowy
%\begin{equation*}
%I_c\left(\delta, D\right) = \int \limits_{P_c} |k|^{\frac{\delta - 2}{2}} \exp\left(-\frac{1}{2} Tr\left(Dk\right)\right) \; dk
%\end{equation*}
%powiązanie rozkładu z japańcem
%\begin{equation*}
%\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}\right) \propto \frac{I_c\left(\delta + n, D+U\right)}{I_c\left(\delta, D\right)}
%\end{equation*}
%uszczegółowienie ogólnego ogólnika (ogólnik)
%\begin{equation*}
%\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) \propto f_{\mathbbm{X} \; \mid \; \Gamma = c, \; \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) = \overset{m}{\idotsint} f(\mathbbm{x})\, d\mathbbm{x}
%\end{equation*}

\end{enumerate}

\thispagestyle{empty}





\end{document}
