\documentclass[a4paper,11pt,twoside]{report}
% THIS FILE SHOULD BE COMPILED BY pdfLaTeX

% ----------------------   PREAMBLE PART ------------------------------

% ------------------------ ENCODING & LANGUAGES ----------------------

\usepackage{fontspec}
\usepackage[polish, english]{babel} % Ostatni język to język GŁÓWNY pracy (English)
\usepackage{bbm}



\usepackage{amsmath, amsfonts, amsthm, latexsym, bm, amssymb} % MOSTLY MATHEMATICAL SYMBOLS

\usepackage[final]{pdfpages} % INPUTING TITLE PDF PAGE - GENERATE IT FIRST!
%\usepackage[backend=bibtex, style=verbose-trad2]{biblatex}

\usepackage{tabularx, booktabs, xcolor, float, multirow, siunitx} % FOR TABLES

\usepackage{commath} % various commands which can make writing math expressions easier --- documentation available at: https://ctan.gust.org.pl/tex-archive/macros/latex/contrib/commath/commath.pdf

\usepackage[hidelinks]{hyperref} % for hyperlinks, for example, urls, references to equations, entries in a bibliography --- hidelinks option removes rectangles around hiperlinks

\usepackage{listings}
\usepackage[most]{tcolorbox}

\usepackage{tikz}

% ---------------- MARGINS, INDENTATION, LINESPREAD ------------------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry} % MARGINS


\linespread{1.5}
\allowdisplaybreaks         % ALLOWS BREAKING PAGE IN MATH MODE

\usepackage{indentfirst}    % IT MAKES THE FIRST PARAGRAPH INDENTED; NOT NEEDED
\setlength{\parindent}{5mm} % WIDTH OF AN INDENTATION


%---------------- RUNNING HEAD - CHAPTER NAMES, PAGE NUMBERS ETC. -------------------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
% PAGINATION: LEFT ALIGNMENT ON EVEN PAGES, RIGHT ALIGNMENT ON ODD PAGES
\fancyfoot[LE,RO]{\thepage}
% RIGHT HEADER: zawartość \rightmark do lewego, wewnętrznego (marginesu)
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
% lewa pagina: zawartość \leftmark do prawego, wewnętrznego (marginesu)
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}

% HEAD RULE - IT'S A LINE WHICH SEPARATES HEADER AND FOOTER FROM CONTENT
\renewcommand{\headrulewidth}{0 pt} % 0 MEANS NO RULE, 0.5 MEANS FINE RULE, THE BIGGER VALUE THE THICKER RULE


\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[LE,RO]{\thepage}

  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0.0pt}
}

% ----------------------- Rcode Setup ---------------------------

% ============================================================
% 1. DEFINICJA KOLORÓW (Nord Style / VS Code)
% ============================================================
\definecolor{codegreen}{rgb}{0.25,0.5,0.35}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{HTML}{F2F2F2}    % Bardzo jasne szare tło
\definecolor{framecolour}{HTML}{2E3440}   % Ciemna ramka (Nord dark)
\definecolor{keywordcolour}{HTML}{81A1C1} % Nord blue

% ============================================================
% 2. STYL LISTINGU (Sam kod R)
% ============================================================
\lstdefinestyle{myRstyle}{
    language=R,
    backgroundcolor=\color{backcolour},
    commentstyle=\itshape\color{codegreen},
    keywordstyle=\bfseries\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize, % Czcionka maszynowa, mniejsza
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=8pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    morekeywords={TRUE, FALSE, NULL, NA, Inf, NaN}
}

% Aktywacja stylu domyślnego
\lstset{style=myRstyle}

% ============================================================
% 3. ŚRODOWISKO "RCode" (Ramka + Listing)
% ============================================================
\newtcblisting[auto counter]{RCode}[2][]{%
    enhanced,                     % Zaawansowane rysowanie
    breakable,                    % POZWALA PRZENOSIĆ KOD NA KOLEJNĄ STRONĘ
    listing engine=listings,      % Silnik listings
    listing only,                 % Tylko kod
    listing options={style=myRstyle},
    title={\textbf{Listing \thetcbcounter:} #2}, % Tytuł: Listing X: Nazwa
    colback=backcolour,           % Tło
    colframe=framecolour,         % Ramka
    coltitle=white,               % Tekst tytułu
    fonttitle=\bfseries\footnotesize,
    sharp corners=downhill,       % Wygląd zakładki
    arc=3mm,
    boxrule=0.5mm,
    drop shadow,                  % Cień
    top=1mm, bottom=1mm,
    #1                            % Opcje dodatkowe
}

% ============================================================
% 4. STYL DLA OUTPUTU Z KONSOLI
% ============================================================
\definecolor{outbg}{HTML}{FAFAFA}      % Prawie białe tło
\definecolor{outframe}{HTML}{707070}   % Szara ramka

\lstdefinestyle{outputStyle}{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{outbg},
    breaklines=true,
    showstringspaces=false,
    tabsize=2,
    frame=none,
    numbers=none,
    columns=fullflexible
}

\newtcblisting{ROutput}[1][]{
    enhanced,
    breakable,                    % Pozwala łamać output na strony
    listing only,
    listing options={style=outputStyle},
    colback=outbg,
    colframe=outframe,
    coltitle=white,
    fonttitle=\bfseries\footnotesize,
    title={Console Output},       % Domyślny tytuł
    arc=2mm,
    boxrule=0.5mm,
    drop shadow,
    #1
}


% --------------------------- CHAPTER HEADERS ---------------------

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt}


\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% ----------------------- TABLE OF CONTENTS SETUP ---------------------------

\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


% THIS MAKES DOTS IN TOC FOR CHAPTERS
\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries \thecontentslabel.\quad}% <numbered-entry-format>
  {\bfseries}% <numberless-entry-format>
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}% <filler-page-format>

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% ---------------------- TABLES AD FIGURES NUMBERING ----------------------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}


% ------------- DEFINING ENVIRONMENTS FOR THEOREMS, DEFINITIONS ETC. ---------------

\makeatletter
\newtheoremstyle{definition}
{3ex}%                           % Space above
{3ex}%                           % Space below
{\upshape}%                      % Body font
{}%                              % Indent amount
{\bfseries}%                     % Theorem head font
{.}%                             % Punctuation after theorem head
{.5em}%                          % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\makeatother

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% --------------------- END OF PREAMBLE PART (MOSTLY) --------------------------





% -------------------------- USER SETTINGS ---------------------------

\newcommand{\tytul}{Dostosowanie biblioteki gips do zadania klasyfikacji przy pomocy analizy dyskryminacyjnej – gipsDA}
\renewcommand{\title}{Adapting the gips library for classification problem utilizing discriminant analysis – gipsDA}
\newcommand{\type}{Engineer} % Master OR Engineer
\newcommand{\supervisor}{MSc Eng. Adam Chojecki\\
    PhD Bartosz Kołodziejek, Assoc. Prof.} % TITLE AND NAME OF THE SUPERVISOR



\begin{document}
\sloppy
\selectlanguage{english}

\includepdf[pages=-]{titlepage-en} % THIS INPUTS THE TITLE PAGE

\null\thispagestyle{empty}\newpage

% ------------------ PAGE WITH SIGNATURES --------------------------------

%\thispagestyle{empty}\newpage
%\null
%
%\vfill
%
%\begin{center}
%\begin{tabular}[t]{ccc}
%............................................. & \hspace*{100pt} & .............................................\\
%supervisor's signature & \hspace*{100pt} & author's signature
%\end{tabular}
%\end{center}
%


% ---------------------------- ABSTRACTS -----------------------------

{  \fontsize{12}{14} \selectfont
\begin{abstract}

\begin{center}
\title
\end{center}
Statistical classification is a fundamental problem in machine learning and data analysis, with applications ranging from genomic sequencing and medical diagnosis to financial forecasting. Among the vast landscape of classification algorithms, generative models such as Linear and Quadratic Discriminant Analysis (LDA and QDA) remain cornerstones of the field due to their rigorous statistical foundations, interpretability, and computational efficiency. However, nowadays  these classical methods are frequently confronted with high-dimensional datasets—scenarios where the number of features rivals or exceeds the number of observations. In such environments, standard estimation techniques often fail, necessitating the development of more robust approaches. \\

We propose a novel solution to this challenge by integrating the the concept of invariance under action of a subgroup of the symmetric group with discriminant analysis. Specifically, we explore how identifying and enforcing permutation symmetries within the data can serve as a powerful form of model-based regularization, stabilizing covariance estimation without resorting to arbitrary shrinkage. \\
\\
\noindent \textbf{Keywords:} classification, discriminant analysis, high-dimensional, regularization, permutation symmetries, covariance, estimation
\end{abstract}
}

\null\thispagestyle{empty}\newpage


{\selectlanguage{polish} \fontsize{12}{14}\selectfont
\begin{abstract}

\begin{center}
\tytul
\end{center}
Statystyczna klasyfikacja jest podstawowym zagadnieniem uczenia maszynowego i analizy danych, jej zastosowania sięgają od sekwencjonowania genomu, przez diagnozę medyczną, po prognozowanie finansowe. Wśród szerokiej gamy algorytmów klasyfikacji, modele generatywne takie jak Liniowa i Kwadratowa Analiza Dyskryminacyjna (LDA i QDA), ze względu na swoje rygorystyczne podstawy statystyczne, interpretowalność i wydajność obliczeniową, pozostają punktem odniesienia dla tej dziedziny. Jednak obecnie te klasyczne metody często zmagają się ze zbiorami danych o wysokiej wymiarowości — scenariuszami, w których liczba cech dorównuje lub przewyższa liczbę obserwacji. W takich okolicznościach standardowe techniki estymacji często zawodzą, co wymaga opracowania bardziej spolegliwych podejść. \\


Proponujemy nowatorskie rozwiązanie tego wyzwania poprzez połączenie koncepcji niezmienniczości pod wpływem działania podgrupy grupy symetrycznej z analizą dyskryminacyjną. W szczególności badamy, w jaki sposób wykrywanie i narzucanie symetrii permutacyjnych w danych może służyć jako skuteczna forma regularyzacji, stabilizując estymację kowariancji bez uciekania się do arbitralnych kroków. \\
\\
\noindent \textbf{Słowa kluczowe:} klasyfikacja, analiza dyskryminacyjna, wielowymiarowy, regularyzacja, symetrie permutacyjne, kowariancja, estymacja
\end{abstract}
}


%% --------------------------- DECLARATIONS ------------------------------------
%
%%
%%	IT IS NECESSARY OT ATTACH FILLED-OUT AUTORSHIP DEECLRATION. SCAN (IN PDF FORMAT) NEEDS TO BE PLACED IN scans FOLDER AND IT SHOULD BE CALLED, FOR EXAMPLE, DECLARATION_OF_AUTORSHIP.PDF. IF THE FILENAME OR FILEPATH IS DIFFERENT, THE FILEPATH IN THE NEXT COMMAND HAS TO BE ADJUSTED ACCORDINGLY.
%%
%%	command attacging the declarations of autorship
%%
%\includepdf[pages=-]{scans/declaration-of-autorship}
%\null\thispagestyle{empty}\newpage
%
%% optional declaration
%%
%%	command attaching the declaataration on granting a license
%%
%\includepdf[pages=-]{scans/declaration-on-granting-a-license}
%%
%%	.tex corresponding to the above PDF files are present in the 3. declarations folder
%
\null\thispagestyle{empty}\newpage
% ------------------- TABLE OF CONTENTS ---------------------
% \selectlanguage{english} - for English
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}
\newpage % IF YOU HAVE EVEN QUANTITY OD PAGES OF TOC, THEN REMOVE IT OR ADD \null\newpage FOR DOUBLE BLANK PAGE BEFORE INTRODUCTION


% -------------------- THE BODY OF THE THESIS --------------------------------

\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{11}

% ======================================================================
%                          Introduction
% ======================================================================

\chapter{Introduction}

The chapter provides the necessary context for the project and is organized into three sections:
\begin{itemize}
    \item \textbf{Section \ref{sec:motivation}: Motivation and Goal} identifies the specific limitations of classical classifiers in high-dimensional settings, particularly the issues of parameter instability and singularity, and defines the primary objective of this thesis.
    \item \textbf{Section \ref{sec:symmetry}: Potential causes of Symmetries} provides the rationale behind the proposed solution, explaining why permutation invariance is a plausible and valuable assumption in many real-world datasets.
%    \item \textbf{Section \ref{sec:da_desc}: General Description of Discriminant Analysis} briefly establishes the theoretical baseline, outlining the mechanics of LDA and QDA that will be modified in our work.
    \item \textbf{Section \ref{sec:contribution}: Contribution} summarizes the specific achievements of this project, including the development of the \texttt{gipsDA} package and the implementation of novel estimators.
\end{itemize}

\section{Motivation and Goal}
\label{sec:motivation}

Classical generative classifiers, such as Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), remain fundamental tools in statistical machine learning\cite{elemstats} due to their interpretability and closed-form solutions.
However, the performance of these methods is critically dependent on the accurate estimation of the class-conditional covariance matrices, denoted as $\boldsymbol{\Sigma}$.
Nowadays it is not uncommon for researchers to frequently encounter high-dimensional datasets where the number of features ($p$) is large relative to the number of available observations ($n$).
In such settings, standard estimators often fail, rendering classical methods suboptimal or entirely inapplicable.

The bottleneck of these algorithms lies in the sample covariance matrix, $\hat{\boldsymbol{\Sigma}}$.
For a dataset with $p$ features, the covariance matrix contains $p(p+1)/2$ unique parameters that must be estimated.
This implies that the model complexity grows quadratically with the dimension of the feature space ($\Theta(p^2)$).
This phenomenon leads to two primary challenges:

\begin{enumerate}
    \item \textbf{Parameter Instability:} When $p$ is large, the number of parameters to estimate can easily exceed the information content available in the training data. This results in estimates with high variance, leading to overfitting and poor generalization performance on unseen data.
    \item \textbf{Estimator Singularity:} In the extreme regime where the number of features exceeds the sample size ($p > n$), the sample covariance matrix $\hat{\boldsymbol{\Sigma}}$ becomes rank-deficient. Consequently, it is no longer positive definite and, crucially, not invertible. Since the decision functions for both LDA and QDA require the computation of the precision matrix $\hat{\boldsymbol{\Sigma}}^{-1}$, classical algorithms mathematically break down in these \("\)small $n$, large $p$\("\) scenarios.
\end{enumerate}

To mitigate these issues, regularization techniques are necessary.
While standard approaches often involve scalar shrinkage (e.g., shrinking towards the identity matrix)\cite{friedman1989regularized, ledoit2004well}, this project explores a structural approach.
We posit that by identifying and enforcing permutation symmetries within the data—where certain variables are exchangeable—we can significantly reduce the number of free parameters required to estimate the covariance structure.

\subsection*{Goal of the Thesis}

The primary objective of this project is to develop \texttt{gipsDA}, a novel R library that adapts discriminant analysis for high-dimensional environments.
By integrating the methodology of the \texttt{gips}\cite{JSSv112i07} (Gaussian model Invariant by Permutation Symmetry) package, we aim to construct robust classifiers that impose statistically justified symmetry constraints on the covariance matrix.

Specifically, the goal is to provide a solution that:
\begin{itemize}
    \item Maintains high classification accuracy in high-dimensional settings where standard LDA/QDA overfit.
    \item Remains mathematically valid and computationally stable in the $p > n$ regime by ensuring the estimated matrices are positive definite through symmetry projection.
    \item Offers a user-friendly interface compatible with the standard R ecosystem, bridging the gap between advanced theoretical statistics and practical data science applications.
\end{itemize}

\section{Potential causes of Symmetries}
\label{sec:symmetry}

At first glance, the assumption that distinct features in a dataset share a permutation symmetry—meaning they remain invariant under specific reorderings—may appear overly restrictive\cite{andersson1998symmetry}. In classical multivariate analysis, features are often treated as distinct entities with unique variances and pairwise correlations. However, in the context of high-dimensional data, the assumption of symmetry is not only mathematically convenient but often empirically justified by the underlying nature of the data itself

The core may be the concept of \textit{exchangeability}. In many datasets, features are not arbitrary measurements but rather collections of comparable units. Consider the following domains where symmetry naturally arises:

\begin{itemize}
    \item \textbf{Genomics and Bioinformatics:} In gene expression data, thousands of genes are measured simultaneously \cite{dudoit2002comparison}. Genes belonging to the same biological pathway or functional group often exhibit similar behavior. It is statistically plausible to assume that the correlation between any pair of genes within such a functional cluster is roughly equivalent, regardless of their specific labels.

    \item \textbf{Medical Diagnostics and Imaging:} Consider the analysis of digitized images for cancer diagnosis, such as the Breast Cancer Wisconsin dataset used in this work\cite{breast}. Features are computed from the characteristics of cell nuclei present in the image (e.g., radius, texture, smoothness) \cite{street1993nuclear}. While each feature measures a distinct geometric property, groups of features derived from similar morphological aspects often display strong, structured correlations. Assuming a symmetric covariance structure among these related biometrics allows the model to capture the general "shape" of malignant cells without overfitting to the noise inherent in individual sample measurements.

    \item \textbf{Industrial Sensor Systems:} In predictive maintenance and fault detection, systems are often monitored by multiple sensors recording analogous physical quantities. A prime example is the Sensorless Drive Diagnosis dataset\cite{dataset_for_sensorless_drive_diagnosis_325}, which involves electric current signals from different phases of a motor \cite{seera2014classification}. In a balanced three-phase system, the physical properties of the phases are designed to be identical. Consequently, the statistical relationship between Phase A and Phase B should theoretically be invariant to the relationship between Phase B and Phase C. Treating these signals as exchangeable via permutation symmetry reflects the physical reality of the hardware design.
\end{itemize}

Furthermore, the approach proposed in this thesis does not require prior knowledge of these specific relationships. Unlike rigid regularization methods that force the covariance matrix towards a specific target (such as the identity matrix), the \texttt{gips} methodology is designed to \textit{discover} the optimal symmetry structure from the data itself\cite{JSSv112i07}.

By allowing the data to reveal its own invariant structure, we adhere to the principle of parsimony (Occam's razor). If a covariance matrix with $\Theta(p^2)$ parameters can be accurately approximated by a symmetric structure requiring only $\Theta(p)$ parameters, the latter model is preferred. It reduces the variance of the estimator and mitigates the risk of overfitting, which is the primary enemy in high-dimensional classification. Thus, expecting symmetry is not about imposing an artificial constraint, but rather about recognizing and exploiting the redundancy inherent in large-scale systems.

%\section{General description of Discriminant Analysis}
%\label{sec:da_desc}
%\textcolor{red}{jakis wprowadzenie ze mozna klasyfikacje robic na podstawie tego ze widzimy symetrie w macierzy kowariancji}

\section{Contribution}
\label{sec:contribution}

The central contribution of this thesis is the proposal and implementation of a novel regularization framework for Discriminant Analysis, specifically designed for the "High Dimension, Low Sample Size" (HDLSS) regime where $p \gg n$. In such settings, classical estimators for the covariance matrix become singular and unstable \cite{bickel2004some}.

While numerous modifications to LDA and QDA have been proposed to address this instability—ranging from the scalar shrinkage parameters of Regularized Discriminant Analysis (RDA) \cite{friedman1989regularized} to sparsity-enforcing methods like Penalized LDA \cite{witten2011penalized} or independence assumptions in Diagonal LDA \cite{dudoit2002comparison}—these approaches often impose arbitrary constraints. They typically force the covariance structure towards a diagonal matrix or sparse representation, potentially discarding significant correlations between features.

In contrast, our approach introduces a paradigm shift by utilizing \textit{permutation symmetry} as a form of model-based regularization. Instead of assuming correlations are zero (sparsity) or uniform (scalar shrinkage), we assume that groups of features may be exchangeable. This allows the data to dictate its own complexity through the discovery of invariant structures.

The specific contributions of this work are as follows:

\begin{itemize}
    \item \textbf{Development of the \texttt{gipsDA} Package:} We provide a fully functional R package that integrates the \texttt{gips} methodology with the standard discriminant analysis workflow. The package is designed to be API-compatible with the widely used \texttt{MASS} library, ensuring immediate accessibility for practitioners.

%    \item \textbf{Novel Covariance Estimators:} We introduce three distinct classification models that leverage symmetry in different ways:
%    \begin{itemize}
%        \item \texttt{gipsLDA}: Adapts the homoscedastic assumption of LDA by projecting the pooled covariance matrix onto a symmetric permutation group.
%        \item \texttt{gipsQDA}: Allows for full heteroscedasticity, optimizing a unique permutation structure for each class independently.
%        \item \texttt{gipsMultQDA}: A hybrid approach that allows for different covariance matrices per class but constrains them to share the \textit{same} underlying permutation symmetry. This required the development of the \texttt{gipsmult} module to optimize a joint posterior probability across multiple groups.
%    \end{itemize}

    \item \textbf{Empirical Validation in Data-Scarce Regimes:} Through extensive simulation studies and benchmarks on real-world datasets (e.g., biomedical and industrial sensor data), we examine to what degree imposing symmetry constraints reduces the estimation error and improves classification accuracy compared to standard QDA, particularly when the sample size is critically low relative to the number of dimensions.
\end{itemize}

% ======================================================================
%                          Related Work
% ======================================================================

\chapter{Related Work}

This chapter establishes the theoretical framework and reviews the existing literature fundamental to the project.
The discussion is structured around two pivotal domains that form the basis of our proposed solution.

First, we revisit classical statistical machine learning algorithms for classification, specifically focusing on Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA).
These well-established methods serve as the foundation for the predictive models developed in this work\cite{elemstats}.

Second, we introduce the methodology underlying the \texttt{gips} (Gaussian model Invariant by Permutation Symmetry) package.
We explore the mathematical principles of identifying invariant structures within covariance matrices and how this approach can be utilized for parameter estimation in high-dimensional settings\cite{JSSv112i07}.
The integration of these two distinct areas, classical discriminant analysis and permutation symmetry discovery, constitutes the core contribution of this thesis.

\section{Theoretical description of LDA and QDA}

Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) are generative classifiers that model the probability density of each class, $g$, using a multivariate Gaussian distribution, $\mathbbm{P}(x|g) = \mathcal{N}(x | \mu_g, \Sigma_g)$. The key difference between them lies in their assumptions regarding the class covariance matrices, $\Sigma_g$.

\begin{itemize}
    \item \textbf{Linear Discriminant Analysis (LDA)} assumes that all classes share a single, common covariance matrix, i.e., $\Sigma_g = \Sigma$ for all $g$. This assumption of homoscedasticity leads to a decision boundary that is a linear function of the input features $x$.
    \item \textbf{Quadratic Discriminant Analysis (QDA)} relaxes this assumption, allowing each class $g$ to have its own distinct covariance matrix, $\Sigma_g$. This assumption of heteroscedasticity results in a decision boundary that is a quadratic function of $x$, providing greater flexibility.
\end{itemize}

Prediction for a new observation $x$ is made by assigning it to the class $g$ that maximizes the posterior probability $\mathbbm{P}(g|x)$. Using Bayes' theorem, this is equivalent to maximizing the discriminant function, $\delta_g(x)$:
\begin{equation}
    \delta_g(x) = -\frac{1}{2} \log |\Sigma_g| - \frac{1}{2} (x - \mu_g)^T \Sigma_g^{-1} (x - \mu_g) + \log \pi_g,
\end{equation}
where $\mu_g$ is the mean vector for class $g$, $\Sigma_g$ is its covariance matrix, and $\pi_g$ is the prior probability of the class. In practice, these parameters are estimated from the training data. For a more detailed treatment of these methods, we refer the reader to "The Elements of Statistical Learning" \cite{elemstats}. Our work focuses on novel estimation techniques for the $\Sigma_g$ matrices.


\section{General description of gips}
The \texttt{gips} package provides a framework for identifying and
exploiting hidden permutation symmetries in Gaussian random vectors.
This approach is particularly effective for high-dimensional data $(p>n)$,
where the empirical covariance matrix is often singular. By assuming
the distribution is invariant under a permutation subgroup $\Gamma$,
the methodology imposes equality constraints on the covariance matrix such that
$K_{i,j} = K_{\sigma(i), \sigma(j)}$ for all $\sigma \in \Gamma$ This reduces the number of free parameters through a form of parameter sharing \cite{JSSv112i07}.




% ======================================================================
%                          Solution Proposal
% ======================================================================

\chapter{Solution Proposal}
The central theme of our proposed solution is the enhancement of classical discriminant analysis methods by introducing a novel approach to estimating covariance matrices.
We modify the standard Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) algorithms by leveraging the methodology of the \texttt{gips} R package\cite{JSSv112i07}.
The core functionality of the \texttt{gips} library is to identify the most probable permutation group under which a given covariance matrix remains approximately invariant.
By imposing such a symmetry constraint, we effectively introduce a form of model-based regularization.
This reduces the number of free parameters that need to be estimated, which can be particularly advantageous when dealing with high-dimensional data or limited sample sizes, potentially leading to more stable and robust classifiers.

\section{gipsmult}
%\section{The \texttt{gipsmult} Methodology}
\label{sec:gipsmult_theory}

The core theoretical contribution of this work involves extending the capabilities of the \texttt{gips} framework to handle more complex data structures. While the original algorithm is designed to identify the optimal permutation symmetry for a \textit{single} covariance matrix, many statistical scenarios involve multiple distinct groups of data that may exhibit related structural properties.

The \texttt{gipsmult} methodology addresses the problem of estimating a shared permutation symmetry $\Gamma$ across $m$ independent groups. In this setting, we allow each group $g$ to have its own distinct covariance matrix $\boldsymbol{\Sigma}_g$—permitting differences in eigenvalues and variance scales—under the strict constraint that all such matrices remain invariant under the \textit{same} permutation group.

This section establishes the mathematical rigor behind this generalization. We begin by introducing the fundamental definitions of permutation-invariant spaces. Subsequently, we present the Bayesian framework used for estimation, contrasting the probabilistic model of the single-group case with our proposed multi-group derivation, and finally arriving at the formula for the joint posterior probability.

\subsection{Definitions}
\subsubsection{Symmetric group}
Let us set $p \in \mathbb{N}$. Let $\mathfrak{S}_p$ denote the symmetric group i.e. the set of all permutastions on $V = \{1,2, \dots, p\}$
with function composition as the operation.
\subsubsection{Invariance under action of a subgroup of the symmetric group}
Let $K \in \mathbb{R}^{p\times p}$ and $\Gamma \leq \mathfrak{S}_p$. We say that matrix $K$ is invariant under action of subgroup $\Gamma$ iif
\begin{equation}
    \forall_{\sigma \in \Gamma} \forall_{i,j\in V} K_{i,j} = K_{\sigma(i), \sigma(j)}
\end{equation}.
For conciseness sake in the following sections we will say that a matrix is \textit{invariant under subgroup} or, if supgroup $\Gamma = \langle \sigma \rangle$
is cyclic that a matrix is \textit{invariant under} $\sigma$.
Below we attach some visual examples to explain the concept better:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/matrix_id}
    \caption{Matrix invariant under () permutation - no constraints}
    \label{fig:matid}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/matrix_1256}
    \caption{Matrix invariant under (1256) permutation - loose constraints}
    \label{fig:mat1256}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/matrix_12345678}
    \caption{Matrix invariant under (12345678) permutation - tight constraints}
    \label{fig:mat12345678}
\end{figure}
\subsubsection{Invariant space and cone}
\begin{equation}
    \mathcal{Z}_\Gamma := \left\{S \in Sym(p;\mathbbm{R}) : S \ is \ invariant \ under \ \Gamma \right\},
\end{equation}
\begin{equation}
    \mathcal{P}_{\Gamma}:= Z_\Gamma \cap Sym^+(p;\mathbbm{R}).
\end{equation}

\subsection{\texttt{gips} vs \texttt{gipsmult}}
\subsubsection{General comparision}
In the setting from \cite{10.1214/22-AOS2174} a \textbf{single} symmetric SPD matrix $K \in \mathbb{R}^{p\times p}$ is invariant under $\sigma \in \mathfrak{S}$.
Let us set $m \in \mathbb{N}$, the model of \texttt{gipsmult} assumes an \textbf{entire set} of symmetric SPD matrices $\mathcal{K} = \{K_1, K_2, \dots, K_m\}$ to be invariant under $\sigma$.
\subsubsection{Bayesian procedure}
Under assumption of uniform prior on the set
$
\mathcal{C} := \{ \langle \sigma \rangle : \sigma \in \mathfrak{S}_p \}
$
of cyclic subgroups of $\mathfrak{S}_p$ and Diaconis-Ylvisacker\cite{diaconis1979conjugate} one on the precision
matrix $K$, \cite{JSSv112i07} arrives at the following proportionality of posterior:
\begin{equation}\label{gipsposteriorformula}
    \mathbbm{P}\left(\Gamma = c \mid \mathbbm{X} = \mathbbm{x}\right) \propto \frac{I_c\left(\delta + n, D+U\right)}{I_c\left(\delta, D\right)}.
\end{equation}
For more detailed explanation of this result see \cite{JSSv112i07}.
\\
Under analogous assumptions the posterior in \texttt{gipsmult} setting is proportional to:
\begin{equation}\label{gipsmultposteriorformula}
    \mathbbm{P}\left(\Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right) \propto \prod_{g=1}^m\frac{I_c\left(\delta + n_g, D+U_g\right)}{I_c\left(\delta, D\right)},
\end{equation}
So the entire logic of Bayesian selection for \texttt{gipsmult} boils down to multiplication of results from \texttt{gips}.\\
We attach full derivation of \eqref{gipsmultposteriorformula} in the appendix A.



\section{Description of machine learning models}
Building upon the theoretical framework of permutation-invariant covariance estimation established in the previous section, we now turn to the practical application of these concepts in supervised learning. The primary objective of this section is to define a new family of classifiers that integrate the \texttt{gips} methodology into the classical frameworks of Linear and Quadratic Discriminant Analysis.

The core innovation lies in replacing the standard empirical covariance estimators with structured estimators that enforce permutation symmetry. This approach serves as a form of model-based regularization, designed to improve stability and predictive performance in high-dimensional settings. However, the implementation of these constraints requires a fundamental choice regarding the statistical estimation strategy: whether to rely on a single, optimal symmetry structure or to incorporate uncertainty through a Bayesian approach. Therefore, before detailing the specific classification architectures, we first define the two primary estimation strategies employed throughout this work.
\subsection{MAP vs average weighted by AP distribution}
Each of models described down below:
\begin{enumerate}
    \item Estimates covariance matrix(ces)
    \item Estimates most probable permutation(s) under which real covariance matrices are invariant
    \item Projects matrix estimate(s) onto the space of those invariant under found permutation(s)
\end{enumerate}
MAP option means that our estimate is a single projection:
\begin{equation}
    \hat{\Sigma} = \Sigma_{c^*},
\end{equation}
where
\begin{equation}
    c^*=\operatorname*{argmax}_{c \in \mathfrak{S}_p} \mathbb{P}(\Gamma=c \mid X=x, Y=y).
\end{equation}
Since both \texttt{gips} and \texttt{gipsmult} allow to estimate not only MAP but the entire AP distribution we have an alternative:
\begin{equation}
    \hat{\Sigma}=\sum_{c \in \mathfrak{S}_p} \mathbb{P}(\Gamma=c \mid X=x, Y=y) \Sigma_c.
    \label{eq:weighted_avg_estimator}
\end{equation}
It potentially offers a more robust estimation by incorporating uncertainty about the true underlying symmetry.
It should be noted that formulas above  are exact only when the entire space of permutations can be exhaustively searched,
which is feasible in \texttt{gips(mult)} implementation for $p \le 9$, where $p$ denotes the number of features (columns).
For a larger number of features, the Metropolis-Hastings algorithm, as described in \cite{JSSv112i07},
is used to approximate these estimators, as the permutation space becomes too large to explore completely.

\subsection{gipsQDA}
The \texttt{gipsQDA} model represents the most flexible approach within our framework. In this method, the \texttt{gips} library is applied independently to the data of each class. This process yields a unique estimated covariance matrix, $\hat{\Sigma}_g$, and a unique optimal permutation group, $\hat{c}_g$, for each class $g$:
\begin{equation}
    (\hat{\Sigma}_g, \hat{c}_g) = \texttt{gips}(\text{Data}_g) \quad \text{for each class } g=1, \dots, m.
\end{equation}
This model is directly analogous to the classic QDA, as it allows for heteroscedasticity, but with the additional complexity that each class can exhibit its own distinct symmetry structure. Consequently, \texttt{gipsQDA} serves as a general model that contains all other proposed models as special, more constrained cases.

\subsection{gipsLDA}
The \texttt{gipsLDA} model adapts the core idea of LDA—a single, shared covariance matrix—to the \texttt{gips} framework. The general strategy is to first pool the covariance information from all classes into a single matrix, and then use \texttt{gips} to find a common symmetry structure for this pooled matrix. The process is as follows:
\begin{enumerate}
    \item For each class $g$, estimate the sample covariance matrix, $S_g$, using the standard unbiased estimator:
    \begin{equation}
        S_g = \frac{1}{n_g-1} \sum^{n_g}_{i=1} (x_{g,i} -\overline{x}_g)(x_{g,i} -\overline{x}_g)^T,
    \end{equation}
    where $n_g$ is the number of observations in class $g$, and $x_{g,i}$ and $\overline{x}_g$ are the $i$-th observation and the sample mean of class $g$, respectively.
    \item Combine the individual $S_g$ matrices into a single pooled matrix, $S$, using a specific averaging method.
    \item Supply the pooled matrix $S$ to the \texttt{gips} algorithm to find the optimal permutation group $\hat{c}$ and the projected covariance matrix $\hat{\Sigma}_{\text{gipsLDA}}$.
    \item Use the resulting matrix $\hat{\Sigma}_{\text{gipsLDA}}$ as the shared covariance matrix in the LDA classification framework, assuming each class has a different mean vector $\mu_g$.
\end{enumerate}
We propose two methods for pooling the covariance matrices.

\subsubsection{gipsLDA classic}
This method uses the classic pooled covariance estimator, which is the standard approach in traditional LDA. The pooled matrix $S$ is defined as:
\begin{equation}
    S = \frac{1}{n-m}\sum_{g=1}^m (n_g-1)S_{g}.
\end{equation}
By substituting the formula for $S_g$ from Equation (3.10), this simplifies to the well-known pooled sample covariance matrix formula:
\begin{equation}
S = \frac{1}{n-m}\sum_{g=1}^m \sum^{n_g}_{i=1} (x_{g,i} -\overline{x}_g)(x_{g,i} -\overline{x}_g)^T.
\end{equation}
This estimator is an unbiased estimate of the common covariance matrix under the assumption of homoscedasticity.

\subsubsection{gipsLDA weighted average}
This method is named "weighted average" because the contribution of each class's covariance matrix, $S_g$, to the final pooled matrix is weighted by its sample size, $n_g$. However, this weighting scheme gives more influence to smaller classes compared to the \texttt{gipsLDA classic} approach. The pooled matrix $S$ is calculated as:
\begin{equation}
    S = \frac{1}{n}\sum_{g=1}^m n_g S_{g},
\end{equation}
where $n = \sum_{g=1}^m n_g$ is the total number of observations and $m$ is the number of classes.


\subsection{gipsMultQDA}
The \texttt{gipsMultQDA} model is an intermediate approach between the full flexibility of \texttt{gipsQDA} and the strong constraints of \texttt{gipsLDA}.
It is supposed to serve the situation when each class has a unique covariance matrix (like QDA), but all these
matrices share the same underlying permutation symmetry. This is useful for scenarios where classes  differ
in scale or variance but are expected to share a common dependency structure:

\begin{equation}
    (\hat{\Sigma}_g, \hat{c}) = \texttt{gips}(\text{Data}_g) \quad \text{for each class } g=1, \dots, m.
\end{equation}

\subsection{Relationship between the models}
\begin{figure}[ht]
\centering
\begin{tikzpicture}

% Outer set
\draw[fill=blue,  fill opacity=0.15] (0,0) ellipse (6cm and 4cm);
\node [above=30mm] {$\texttt{gipsQDA}$};

% Middle set
\draw[fill=green,fill opacity=0.15] (0,0) ellipse (4cm and 2.6cm);
\node [above=15mm] {$\texttt{gipsmultQDA}$};

% Inner set
\draw[fill=red,  fill opacity=0.15] (0,0) ellipse (2cm and 1.2cm);
\node at (0,0) {$\texttt{gipsLDA}$};

\end{tikzpicture}
\caption{The diagram shows that \texttt{gipsLDA} is a specific case of \texttt{gipsmultQDA}, which itself is a specific case of \texttt{gipsQDA}.}
\label{fig:nested-sets}
\end{figure}

\subsection{Regularization}
It should be also noted that all covariance estimates are passed to regularizing function after being projected. The exact procedure is discussed in \autoref{ch:baseline_regularization}.






% ======================================================================
%                          Implementation
% ======================================================================
\chapter{Package Description}
The framework of \texttt{gipsDA} was implemented in R. All the necessary code was wrapped as an R package and published at \href{https://cran.r-project.org/}{CRAN}. One can install it with:
\begin{RCode}{Installing gipsDA}
    install.packages("gipsDA")
\end{RCode}


The source code and the documentation of the package are available at  \href{https://github.com/AntoniKingston/gipsDAInzynierka}{github.com/AntoniKingston/gipsDAInzynierka} and \href{https://antonikingston.github.io/gipsDA/}{https://antonikingston.github.io/gipsDA/} repectively.
\section{Example usage}
After installation one can import package in the standard R way and call its functions. In this case gipsqda() is used to build a classifier on widely known \textit{Iris} dataset\cite{iris_53}.
\begin{RCode}{Training gipsQDA}
library(gipsDA)
# MAP = FALSE uses the weighted posterior estimator
fit_qda <- gipsqda(Species ~ ., data = iris, MAP = FALSE)

print(fit_qda)
\end{RCode}

\begin{ROutput}
Call:
gipsqda(Species ~ ., data = iris, MAP = FALSE)

Prior probabilities of groups:
    setosa versicolor  virginica
 0.3333333  0.3333333  0.3333333

Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
setosa            5.006       3.428        1.462       0.246
versicolor        5.936       2.770        4.260       1.326
virginica         6.588       2.974        5.552       2.026

Permutations with their estimated probabilities:
[[1]]
      (1,2)          ()
0.992587770 0.007412159

[[2]]
       ()
0.9999999

[[3]]
(1,3)(2,4)         ()      (2,4)      (1,3)
0.69108586 0.17759118 0.10792401 0.02339895
\end{ROutput}

As there are 3 classes in the dataset optimization info is a list of length 3.


\section{Implementation}
Standard R package development practices were followed.
The implementation is organized into two primary
components: the user-facing \texttt{models} module and the backend \texttt{gipsmult} module.
The core design philosophy was to extend the well-established and widely-used
functions of the \texttt{MASS} package.

\subsection{The \texttt{models} Module}
This module contains the primary, user-facing functions: \texttt{gipsLDA}, \texttt{gipsQDA}, and \texttt{gipsMultQDA}. Our implementation strategy was to adopt the complete structure of the original \texttt{MASS} functions to ensure a consistent user experience. The \texttt{gipsLDA} function is a direct modification of \texttt{lda.default()}. The other two models, \texttt{gipsQDA} and its variant \texttt{gipsMultQDA}, share a common foundation, as both are implemented by modifying the \texttt{qda.default()} method. For all three functions, our changes are precisely targeted to replace the standard covariance estimation with our \texttt{gips}-based projection.

\subsubsection{gipsLDA}
This function is designed as a direct, enhanced replacement for its \texttt{MASS::lda} counterpart. The implementation adopts the S3 method dispatch system from \texttt{MASS} to provide a familiar and flexible user interface.

\paragraph{S3 Methods and User Interface}
To ensure consistency with standard R practices, \texttt{gipslda} is an S3 generic function with several methods that handle different input types. The \texttt{gipslda.formula()}, \texttt{gipslda.data.frame()}, and \texttt{gipslda.matrix()} methods are almost identical to their equivalents in the \texttt{MASS} package. Their primary role is to process the input data—handling formulas, subsets, and missing values—before dispatching to the core computational engine, \texttt{gipslda.default()}. This design ensures that users familiar with \texttt{MASS::lda} can use \texttt{gipslda} with no change to their workflow.

\paragraph{The Core Engine: \texttt{gipslda.default()}}
This function contains the main algorithm and is where our modifications to the \texttt{MASS} code are concentrated. It accepts the following arguments:
\begin{description}
    \item[\texttt{x, grouping}] The input matrix of predictors and the vector of class labels.
    \item[\texttt{prior}] A vector of prior probabilities for the classes, defaulting to the class proportions in the training data.
    \item[\texttt{tol}] A tolerance threshold to detect zero-variance columns, defaulting to \texttt{1.0e-4}.
    \item[\texttt{weighted\_avg}] A logical parameter, defaulting to \texttt{FALSE}, which controls the covariance pooling strategy.
    \item[\texttt{MAP}] A logical parameter, defaulting to \texttt{TRUE}, which selects the estimation method (argmax or weighted average).
    \item[\texttt{optimizer}] A character string, defaulting to \texttt{NULL}. If \texttt{NULL}, it is automatically set to \texttt{"BF"} for $p < 10$ and \texttt{"MH"} for $p \ge 10$.
    \item[\texttt{max\_iter}] An integer, defaulting to \texttt{NULL}. If the optimizer is \texttt{"MH"} and this is \texttt{NULL}, it is set to \texttt{100} and a warning is issued.
\end{description}
The function's workflow begins with input validation (checking for finite values, consistent dimensions, empty groups), which is identical to the procedure in \texttt{MASS::lda.default()}. The key modifications occur in two stages:
\begin{enumerate}
    \item \textbf{Covariance Pooling:} The first modification intercepts the calculation of the pooled covariance matrix. The logic proceeds based on the \texttt{weighted\_avg} parameter:
    \begin{itemize}
        \item If \texttt{FALSE} (default), the standard unbiased pooled covariance matrix is computed, as in \texttt{MASS}.
        \item If \texttt{TRUE}, the code first calculates the individual covariance matrix $S_g$ for each class and then combines them using the formula $S = \frac{1}{n}\sum_{g} n_g S_{g}$.
    \end{itemize}
    \item \textbf{Gips Projection:} The resulting pooled covariance matrix is then passed as a single-element list to the \texttt{project\_covs()} helper function. This is the primary injection point of our methodology. The helper function applies the \texttt{gips} optimization and returns the final, projected covariance matrix.
\end{enumerate}
After this step, the remainder of the function proceeds exactly as in \texttt{MASS::lda.default()}, performing Singular Value Decomposition (SVD) and calculating the discriminant function coefficients, but now using the \textit{projected} covariance matrix.

\paragraph{Return Value and Post-Processing}
The function returns an object of class \texttt{gipslda}. This is a \texttt{list} containing the standard components from a \texttt{MASS::lda} object (e.g., \texttt{prior}, \texttt{counts}, \texttt{means}, \texttt{scaling}), plus an additional element, \texttt{optimization\_info}, which stores the output from the \texttt{gips} optimization. The package provides \texttt{predict()}, \texttt{print()}, and \texttt{plot()} methods for \texttt{gipslda} objects, which are also direct adaptations of their \texttt{MASS} counterparts. This ensures that a fitted \texttt{gipslda} object can be used for prediction and visualization without any side effects beyond the standard console or plot output. The \texttt{print.gipslda()} method has been extended to display the contents of the \texttt{optimization\_info} element.

\subsubsection{gipsQDA}
This function is designed as an enhanced replacement for \texttt{MASS::qda}, allowing for class-specific covariance matrices, each with its own unique symmetry structure. The implementation follows the S3 method dispatch system of \texttt{MASS} to maintain a familiar user interface.

\paragraph{S3 Methods and User Interface}
Consistent with standard R practices, \texttt{gipsqda} is an S3 generic function. The \texttt{gipsqda.formula()}, \texttt{gipsqda.data.frame()}, and \texttt{gipsqda.matrix()} methods are nearly identical to their counterparts in the \texttt{MASS} package. Their purpose is to preprocess the input data by handling formulas, subsets, and missing values before passing a clean data matrix and grouping factor to the core computational engine, \texttt{gipsqda.default()}.

\paragraph{The Core Engine: \texttt{gipsqda.default()}}
This function contains the main algorithm and is where our modifications to the \texttt{MASS} code are implemented. It accepts the following arguments:
\begin{description}
    \item[\texttt{x, grouping}] The input matrix of predictors and the vector of class labels.
    \item[\texttt{prior}] A vector of prior probabilities for the classes, defaulting to the class proportions in the training data.
    \item[\texttt{MAP}] A logical parameter, defaulting to \texttt{TRUE}, which selects the estimation method (argmax or weighted average).
    \item[\texttt{optimizer}] A character string, defaulting to \texttt{NULL}. If \texttt{NULL}, it is automatically set to \texttt{"BF"} for $p < 10$ and \texttt{"MH"} for $p \ge 10$.
    \item[\texttt{max\_iter}] An integer, defaulting to \texttt{NULL}. If the optimizer is \texttt{"MH"} and this is \texttt{NULL}, it is set to \texttt{100} and a warning is issued.
\end{description}

The function's workflow begins with input validation inherited from \texttt{MASS::qda.default()}, such as checking for finite values and ensuring that each group has enough observations to estimate a covariance matrix ($n_g > p$). The key modification is introduced within the main \texttt{for} loop that iterates through each class:
\begin{enumerate}
    \item For the current class, an empirical covariance matrix is estimated using \texttt{MASS::cov.mve()}.
    \item \textbf{Gips Projection:} This single covariance matrix is then passed as a single-element list to the \texttt{project\_covs()} helper function. The helper applies the \texttt{gips} optimization to find the optimal symmetry structure for this specific class and returns the projected covariance matrix.
    \item The remainder of the loop proceeds with the standard \texttt{qda} logic, performing Singular Value Decomposition (SVD) on the \textit{projected} matrix to calculate the scaling components and log-determinant for that class.
\end{enumerate}
This process is repeated for every class, resulting in a model where each class has its own individually optimized covariance structure.

\paragraph{Return Value and Post-Processing}
The function returns an object of class \texttt{gipsqda}. This is a \texttt{list} containing the standard components from a \texttt{MASS::qda} object (e.g., \texttt{prior}, \texttt{counts}, \texttt{means}, \texttt{scaling}, \texttt{ldet}), plus an additional element, \texttt{optimization\_info}. The package provides \texttt{predict()} and \texttt{print()} methods for \texttt{gipsqda} objects, which are direct adaptations of their \texttt{MASS} counterparts, ensuring standard functionality for prediction and inspection. The \texttt{print.gipsqda()} method has been extended to display the contents of the \texttt{optimization\_info} element.

\subsubsection{gipsMultQDA}
This function implements the intermediate model, which allows for class-specific covariance matrices but constrains them to share a single, common permutation symmetry. The implementation is a structural modification of \texttt{MASS::qda} and, like the other functions, uses the S3 method dispatch system.

\paragraph{S3 Methods and User Interface}
To maintain a consistent API, \texttt{gipsmultqda} is an S3 generic function. The \texttt{gipsmultqda.formula()}, \texttt{gipsmultqda.data.frame()}, and \texttt{gipsmultqda.matrix()} methods are nearly identical to their counterparts in the \texttt{MASS} package. They handle the initial data processing before dispatching to the core computational engine, \texttt{gipsmultqda.default()}.

\paragraph{The Core Engine: \texttt{gipsmultqda.default()}}
This function contains the main algorithm and is where our modifications to the \texttt{MASS} code are implemented. It accepts the following arguments:
\begin{description}
    \item[\texttt{x, grouping}] The input matrix of predictors and the vector of class labels.
    \item[\texttt{prior}] A vector of prior probabilities for the classes, defaulting to the class proportions in the training data.
    \item[\texttt{MAP}] A logical parameter, defaulting to \texttt{TRUE}, which selects the estimation method (argmax or weighted average).
    \item[\texttt{optimizer}] A character string, defaulting to \texttt{NULL}. If \texttt{NULL}, it is automatically set to \texttt{"BF"} for $p < 10$ and \texttt{"MH"} for $p \ge 10$.
    \item[\texttt{max\_iter}] An integer, defaulting to \texttt{NULL}. If the optimizer is \texttt{"MH"} and this is \texttt{NULL}, it is set to \texttt{100} and a warning is issued.
\end{description}
The function's workflow begins with the same input validation as \texttt{MASS::qda.default()}. The core logic is then executed in three distinct stages:
\begin{enumerate}
    \item \textbf{Covariance Collection:} A preliminary \texttt{for} loop iterates through all classes. In each iteration, it calculates the empirical covariance matrix for that class using \texttt{MASS::cov.mve()} and collects it into a list.
    \item \textbf{Joint Gips Projection:} After the loop, the entire list of covariance matrices is passed to the \texttt{project\_covs()} helper function. This is the primary injection point of the \texttt{gipsmult} methodology. The helper function finds a single, common symmetry structure that is jointly optimal for all classes and returns a list of the projected covariance matrices and the optimization results.
    \item \textbf{SVD and Scaling:} A second \texttt{for} loop then iterates through the classes again. For each class, it takes the corresponding projected matrix from the list returned by the helper function and performs the standard SVD and scaling calculations, as in \texttt{qda.default()}.
\end{enumerate}

\paragraph{Return Value and Post-Processing}
The function returns an object of class \texttt{gipsmultqda}. This is a \texttt{list} containing the standard components from a \texttt{MASS::qda} object (e.g., \texttt{prior}, \texttt{counts}, \texttt{means}, \texttt{scaling}, \texttt{ldet}), plus an additional element, \texttt{optimization\_info}. This element stores the results of the joint optimization, such as the single common MAP permutation or the posterior probabilities. The package provides \texttt{predict()} and \texttt{print()} methods for \texttt{gipsmultqda} objects, which are direct adaptations of their \texttt{MASS} counterparts. The \texttt{print.gipsmultqda()} method has been extended to display the contents of the \texttt{optimization\_info} element.

\subsubsection{Helper Functions}
A utility file (\texttt{models\_utils.R}) contains the core bridge functions that connect the modified \texttt{MASS} code to the \texttt{gips} logic. The implementation was streamlined to use a single, versatile function that handles both estimation strategies.

\begin{itemize}
    \item \texttt{project\_covs(emp\_covs, ns\_obs, MAP, optimizer, max\_iter, tol)}: This is the primary helper function that acts as a unified interface for both the argmax and weighted-average estimation methods. It takes in several arguments to control the process:
    \begin{description}
        \item[\texttt{emp\_covs}] A \texttt{list} of numeric \texttt{matrix} objects, where each matrix is an empirical covariance matrix for a class.
        \item[\texttt{ns\_obs}] A numeric \texttt{vector} containing the number of observations for each corresponding class.
        \item[\texttt{MAP}] A logical scalar. If \texttt{TRUE}, the function finds the single most probable permutation. If \texttt{FALSE}, it calculates the weighted-average projection, it defaults to \texttt{TRUE}.
        \item[\texttt{optimizer}] A character string specifying the search algorithm, either \texttt{'BF'} or \texttt{'MH'}. The resolution of the \texttt{'auto'} option is handled by the parent model function.
        \item[\texttt{max\_iter}] An integer specifying the number of iterations for the Metropolis-Hastings optimizer.
        \item[\texttt{tol}] A numeric tolerance threshold used when \texttt{MAP = FALSE}. When calculating the weighted average estimator from Equation~\eqref{eq:weighted_avg_estimator}, permutations with a posterior probability below this threshold are excluded from the summation to improve computational efficiency, it defaults to $0.001$.
    \end{description}
    The function returns a \texttt{list} containing two named elements:
    \begin{itemize}
        \item \texttt{covs}: A \texttt{list} of numeric \texttt{matrix} objects, containing the final projected covariance matrices. This list has the same length and structure as the input \texttt{emp\_covs}.
        \item \texttt{opt\_info}: Contains information from the optimization process. If \texttt{MAP = TRUE}, this is the optimal permutation object. If \texttt{MAP = FALSE}, this is a named numeric \texttt{vector} of the posterior probabilities used for weighting.
    \end{itemize}

    \item \texttt{project\_matrix\_multiperm(emp\_cov, probs)}: This is a lower-level utility that implements the weighted-average projection for a single matrix. It takes in two arguments:
    \begin{description}
        \item[\texttt{emp\_cov}] A single numeric \texttt{matrix}.
        \item[\texttt{probs}] A named numeric \texttt{vector} where the names are permutations and the values are their posterior probabilities.
    \end{description}
    The function returns a single numeric \texttt{matrix} of the same dimensions as the input \texttt{emp\_cov}, representing the final weighted-average covariance matrix.
    \item \texttt{serialize\_for\_json(x)}: A function converting gipsDA classifier object to format eligible for saving to a json file, it takes in a single argument:
    \begin{description}
        \item[\texttt{x}] A gipsDA classifier (an object of class gipslda, gipsqda or gipsmultqda).
    \end{description}
    The function returns a list with possibly more lists nested inside.
    \item \texttt{deserialize\_from\_json(x)}: A function deserializing data loaded from json to create a gipsDA model object, it takes in a single argument:
    \begin{description}
        \item[\texttt{x}] A list constituting serialized model data.
    \end{description}
    The function returns a list which elements are gipsDA components with proper R metadata.
    \item \texttt{gipsDA\_to\_json(obj, file)}: A function saving gipsDA object to a json file, it takes in 2 arguments:
    \begin{description}
        \item[\texttt{obj}] A gipsDA object.
        \item[\texttt{filename}] Path where object is to be saved.
    \end{description}
    The function does not return anything.
    \item \texttt{gipsDA\_from\_json(file, classname)}: A function loading a gipsDA object from a json file, it takes in 2 arguments:
    \begin{description}
        \item[\texttt{obj}] Path where object is to be saved.
        \item[\texttt{classname}] Class of the object to be loaded.
    \end{description}
    The function returns an object of gipslda, gipqda or gipsmultqda class.
    \item \texttt{recursive\_length(x)}: A helper function to serialize\_for\_json(), it takis in a single argument:
    \begin{description}
        \item[\texttt{x}] Any R object.
    \end{description}
    The function returns the number of atomic elements in an object
    \item \texttt{desingularize(A, target)}: A function regularizing a square matrix so that the module of its smallest eigenvalue is the target, it takes in 2 arguments:
    \begin{description}
        \item[\texttt{A}] A square nonsigular matrix.
        \item[\texttt{target}] Desired module of the smaller eigenvalue.
    \end{description}
    The function returns a square regularized matrix.
\end{itemize}
\section{Unit tests and documentation}
The project utilizes the \textbf{\texttt{testthat}} framework ($\ge$ 3.3.1) along with \texttt{mockery} ($\ge$ 0.4.5) for mocking objects to ensure code reliability. The package achieves $85\%$ test coverage.
Documentation was written in the form of \texttt{roxygen2} comments and rendered using \texttt{pkgdown}, it is available at \href{https://antonikingston.github.io/gipsDA/}{link}. Github actions were configured to run tests and generate documentation automatically upon any changes to the main branch


% ======================================================================
%                          Testing
% ======================================================================

\chapter{Testing Methodology}
\label{ch:testing_methodology}

This chapter presents evaluation methodology employed to assess the performance and robustness of the proposed \texttt{gipsDA} framework. To ensure rigorous validation, the assessment strategy is twofold, encompassing controlled experiments on synthetic data and empirical benchmarking on real-world datasets.

First, we detail the procedures for the \textbf{synthetic data analysis}. This includes a description of the generative algorithms used to simulate high-dimensional data with specific covariance structures, methodology for constructing performance visualization curves, and statistical framework applied to quantify significance of results.

Subsequently, we outline approach for \textbf{real-world data benchmarking}. This section describes necessary data preparation and preprocessing pipelines, explains visualization techniques used to monitor model convergence, and provides a brief characterization of specific datasets selected for this study.

In both experimental scenarios, the comparative analysis focuses on the performance of six distinct classification models. These include four variants of the proposed framework:
\begin{itemize}
    \item \texttt{gipsLDA} with weighted averaging (denoted in plots as \texttt{gipsldawa}),
    \item \texttt{gipsLDA} utilizing the classic estimator (\texttt{gipsldacl}),
    \item \texttt{gipsQDA} (\texttt{gipsqda}),
    \item \texttt{gipsMultQDA} (\texttt{gipsmultqda}).
\end{itemize}
These are benchmarked against two baseline models, referred to as \texttt{LDAmod} and \texttt{QDAmod}.
These baselines represent standard Linear and Quadratic Discriminant Analysis, respectively, augmented with a specific regularization technique.
This modification ensures numerical stability by constraining the smallest eigenvalue of the covariance matrix to a minimum threshold of $0.05$, a procedure that will be detailed in subsequent sections.

To quantitatively assess and compare the predictive power of these models, we employ \textbf{Classification Accuracy} as the primary evaluation metric. For a given test dataset consisting of $N$ observations, let $y_i$ denote the true class label of the $i$-th sample, and $\hat{y}_i$ denote the corresponding label predicted by the model. The accuracy is formally defined as:

\begin{equation}
    \text{Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(y_i = \hat{y}_i),
\end{equation}

where $\mathbb{I}(\cdot)$ is the indicator function, taking the value $1$ if the condition $y_i = \hat{y}_i$ is met (i.e., the prediction is correct) and $0$ otherwise. This metric provides a global measure of the model's effectiveness in correctly identifying class membership across the entire test set.

In addition to accuracy, we utilize the \textbf{Area Under the Receiver Operating Characteristic Curve (AUC-ROC)} to evaluate the models' ability to discriminate between classes, independent of specific decision thresholds. For a binary classification problem, the AUC represents the probability that the classifier ranks a randomly chosen positive instance higher than a randomly chosen negative instance \cite{fawcett2006introduction}.

To extend this metric to multi-class problems (where the number of classes $K > 2$), we employ the generalization proposed by \cite{hand2001simple}, often referred to as the \textbf{Hand \& Till M value}. This method adopts a "One-vs-One" strategy.

Instead of comparing one class against all others, we calculate the AUC for every possible pair of distinct classes $(c_i, c_j)$. The final multi-class metric is defined as the arithmetic mean of these pairwise scores:

\begin{equation}
    \text{AUC}_{\text{multi}} = \frac{2}{K(K-1)} \sum_{i < j} \text{AUC}(c_i, c_j),
\end{equation}

where $\text{AUC}(c_i, c_j)$ represents the probability that a randomly chosen member of class $c_i$ will have a higher estimated probability of belonging to class $c_i$ than a randomly chosen member of class $c_j$. This metric is robust to class imbalance and provides a coherent measure of separability across all class boundaries.
\section{Baseline Regularization Strategy}
\label{sec:baseline_regularization}

Standard estimators for Linear and Quadratic Discriminant Analysis (LDA and QDA) rely on the inversion of the sample covariance matrix, $\hat{\boldsymbol{\Sigma}}$. In high-dimensional settings, particularly when $p > n$, $\hat{\boldsymbol{\Sigma}}$ becomes singular or near-singular, possessing eigenvalues close to or equal to zero. This renders the standard precision matrix calculation unstable or impossible.

To ensure a fair comparison between the proposed \texttt{gips}-based models and the classical approaches, we implemented modified versions of the standard algorithms, denoted as \texttt{LDAmod} and \texttt{QDAmod}. These models incorporate a deterministic regularization step—referred to as \textit{desingularization}—which guarantees that the covariance matrix is strictly positive-definite with a lower bound on the smallest module of eigenvalue.

The exact regularization procedure is exhaustively discussed in \autoref{ch:baseline_regularization}.
%
%\subsection{Mathematical Formulation}
%
%The objective of this procedure is to transform the empirical covariance matrix $\mathbf{A}$ into a regularized matrix $\tilde{\mathbf{A}}$ such that its smallest eigenvalue, $\lambda_{\min}(\tilde{\mathbf{A}})$, is no less than a specific target threshold $\tau$. For the purposes of this study, we set $\tau = 0.05$.
%
%If the smallest eigenvalue of the original matrix, denoted as $\lambda = \lambda_{\min}(\mathbf{A})$, already satisfies $\lambda \ge \tau$, no modification is performed. However, if $\lambda < \tau$, we apply a linear shrinkage towards the identity matrix $\mathbf{I}$. The regularized matrix is defined as:
%
%\begin{equation}
%    \tilde{\mathbf{A}} = \frac{\mathbf{A} + s\mathbf{I}}{1 + s},
%\end{equation}
%where $s$ is a non-negative scalar scaling factor.
%
%\subsection{Derivation of the Scaling Factor}
%
%The parameter $s$ is derived analytically to ensure the new smallest eigenvalue exactly matches the target $\tau$. The derivation proceeds as follows:
%
%\begin{enumerate}
%    \item Let the eigenvalues of $\mathbf{A}$ be denoted by $\lambda_i$. The smallest eigenvalue is $\lambda$.
%    \item Adding a multiple of the identity matrix shifts the spectrum. The eigenvalues of $\mathbf{A} + s\mathbf{I}$ are $\lambda_i + s$. Consequently, the smallest eigenvalue becomes $\lambda + s$.
%    \item Dividing by the scalar $(1+s)$ scales the eigenvalues. Thus, the smallest eigenvalue of the normalized matrix $\tilde{\mathbf{A}}$ is given by:
%    \begin{equation}
%        \lambda_{\min}(\tilde{\mathbf{A}}) = \frac{\lambda + s}{1 + s}.
%    \end{equation}
%\end{enumerate}
%
%To enforce the condition $\lambda_{\min}(\tilde{\mathbf{A}}) = \tau$, we solve the following equation for $s$:
%
%\begin{equation}
%    \frac{\lambda + s}{1 + s} = \tau.
%\end{equation}
%
%Rearranging the terms:
%\begin{align*}
%    \lambda + s &= \tau(1 + s) \\
%    \lambda + s &= \tau + \tau s \\
%    s - \tau s &= \tau - \lambda \\
%    s(1 - \tau) &= \tau - \lambda.
%\end{align*}
%
%This yields the closed-form solution for the scaling factor:
%\begin{equation}
%    s = \frac{\tau - \lambda}{1 - \tau}.
%\end{equation}
%
%This transformation ensures that \texttt{LDAmod} and \texttt{QDAmod} remain numerically solvable even in high-dimensional scenarios where standard implementations would fail due to singularity, providing a robust baseline for benchmarking the performance of \texttt{gipsDA}.

\section{Synthetic data}
\label{sec:synthetic_data}

To evaluate the performance of the proposed classification models, a simulation study was designed. This involved generating synthetic datasets with controlled and well-defined properties, allowing us to systematically explore different assumptions regarding the underlying covariance structures of the classes.

\subsection{Experimental Workflow and Reproducibility}

A critical aspect of our methodology is the separation of the \textit{distribution definition} from the \textit{data sampling} phase. To ensure that performance comparisons across different sample sizes ($n$) reflect true model characteristics rather than random variations in the ground-truth parameters, the simulation proceeds in two distinct stages:

\begin{enumerate}
    \item \textbf{Parameter Fixing:} First, the "ground truth" parameters—specifically the class mean vectors ($\boldsymbol{\mu}_k$) and the true covariance matrices ($\boldsymbol{\Sigma}_k$)—are generated according to specific scenarios (detailed in Sections \ref{sec:data_generation_algorithm} and \ref{sec:scenarios}). These parameters are serialized and saved to disk.
    \item \textbf{Monte Carlo Simulation:} Subsequently, for each defined sample size $n$, we conduct a series of independent experiments. In each experiment, training and test datasets are sampled from the distributions defined by the saved parameters.
\end{enumerate}

Consequently, each data point on the resulting performance curves represents the mean accuracy calculated over multiple independent repetitions (e.g., 30 runs). This approach minimizes the variance of the estimator and ensures that the observed trends are statistically robust.

\subsection{Data Generation Algorithm}
\label{sec:data_generation_algorithm}

The core algorithm for defining the distributions consists of the following sequential steps:

\begin{enumerate}
    \item \textbf{Mean Vector Sampling:}
    For each of the $k$ classes, a mean vector $\boldsymbol{\mu}_k$ is sampled as a random point from a $p$-dimensional hypercube, $[0, 1]^p$. To prevent fortuitous overlap between classes, a minimum separation constraint is enforced. After sampling the set of $k$ vectors, the Euclidean distance between every pair of distinct means $(\boldsymbol{\mu}_i, \boldsymbol{\mu}_j)$ is calculated. If any pair satisfies $\|\boldsymbol{\mu}_i - \boldsymbol{\mu}_j\| \leq 0.05$, the entire set is discarded, and the sampling process is repeated. This safeguard prevents the generation of trivially difficult scenarios caused by nearly identical class centroids.

    \item \textbf{Covariance Matrix Construction (Spectral Decomposition):}
    To stress-test the models under varied spectral conditions, we employ a method based on spectral decomposition.  The procedure is as follows:
    \begin{itemize}
        \item \textbf{Eigenvalue Generation:} A vector of eigenvalues $\boldsymbol{\lambda} = (\lambda_1, \dots, \lambda_p)$ is drawn from a specified distribution $\mathcal{D}$ (e.g., Exponential) and sorted such that $\lambda_1 \ge \dots \ge \lambda_p > 0$. Let $\Lambda = \text{diag}(\boldsymbol{\lambda})$.
        \item \textbf{Random Orthogonal Matrix:} A random matrix $\mathbf{Z} \in \mathbb{R}^{p \times p}$ is generated with entries $Z_{ij} \sim \mathcal{N}(0, 1)$. To obtain a uniformly distributed orthogonal matrix (Haar measure), we perform a QR decomposition: $\mathbf{Z} = \mathbf{Q}'\mathbf{R}$. The matrix $\mathbf{Q}'$ is adjusted by the signs of the diagonal elements of $\mathbf{R}$ to ensure uniqueness:
        \begin{equation}
            \mathbf{Q} = \mathbf{Q}' \cdot \text{diag}\left(\text{sgn}(R_{11}), \dots, \text{sgn}(R_{pp})\right).
        \end{equation}
        \item \textbf{Reconstruction:} The base covariance matrix is constructed as $\boldsymbol{\Sigma}_{\text{raw}} = \mathbf{Q} \Lambda \mathbf{Q}^\top$.
    \end{itemize}

    \item \textbf{Covariance Matrix Projection:}
    For scenarios involving \texttt{gips} methodology, raw a covariance matrix $\boldsymbol{\Sigma}_{\text{raw}}$ is projected onto specific permutation group structure. This step imposes predefined symmetries required to test specific hypotheses of our thesis (e.g., cyclic symmetry or block symmetry).

    \item \textbf{Separability Control:}
    To ensure a consistent level of classification difficulty across diverse scenarios, a scaling parameter $\psi$ is introduced. This parameter scales the covariance matrix ($\boldsymbol{\Sigma} = \boldsymbol{\Sigma}_{\text{base}} \cdot \psi$), where smaller values of $\psi$ reduce variance, making classes more compact and separable. The optimal value of $\psi$ is determined via an automated iterative search. Starting with an initial $\psi_{\text{init}}$, we iteratively test $\psi_{\text{new}} =\frac{\psi_{\text{init}}}{2^i}$ until a baseline LDA or QDA model achieves a predefined target accuracy on the generated data.

    This target threshold is calculated dynamically to adapt to the number of classes, $K$. We define a fixed signal strength parameter $S = 0.6$ and the baseline random accuracy as $Acc_{\text{base}} =\frac{1}{K}$. The theoretical target test accuracy is defined as:
    \begin{equation}
        Acc_{\text{test}} = Acc_{\text{base}} + (1 - Acc_{\text{base}}) \cdot S = S + (1 - S) \cdot Acc_{\text{base}}.
    \end{equation}
    To account for estimation variance during the calibration phase, the actual stopping criterion for the search (target training accuracy) is set slightly higher, capped at a maximum of 0.90:
    \begin{equation}
        Acc_{\text{target}} = \min(0.90, Acc_{\text{test}} + 0.10).
    \end{equation}
    This calibration aims to guarantee that all scenarios are normalized to a comparable difficulty level before the main evaluation begins.

    \item \textbf{Final Data Generation:}
    Finally, for a given experiment with sample size $n_k$ for class $k$, the observations are sampled from the multivariate normal distribution $\mathcal{N}_p(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$.
\end{enumerate}

\subsection{Data Generation Scenarios}
\label{sec:scenarios}

To systematically evaluate the proposed models, we define five distinct data generation scenarios. Each scenario corresponds to a specific set of assumptions regarding the covariance structure and symmetry of the underlying classes. These scenarios range from the most constrained (homoscedastic and symmetric) to the most flexible (heteroscedastic and unstructured), effectively creating "ideal" conditions for each of the tested algorithms.

Let $K$ denote the number of classes, $\boldsymbol{\Sigma}_g$ the covariance matrix for class $g$, and $\Gamma$ the permutation group defining symmetry.

\begin{description}
    \item[Scenario 1: gipsLDA (Homoscedastic / Symmetric)] \hfill \\
    This scenario represents the strictest set of assumptions, targeting \texttt{gipsLDA} model.
    \begin{itemize}
        \item \textbf{Covariance Structure:} We assume homoscedasticity, meaning all classes share a single, common covariance matrix:
        \[ \boldsymbol{\Sigma}_1 = \boldsymbol{\Sigma}_2 = \dots = \boldsymbol{\Sigma}_K = \boldsymbol{\Sigma}_{\text{common}}. \]
        \item \textbf{Symmetry:} This common matrix is projected onto a specific, non-trivial permutation group $\Gamma$. Consequently, the data exhibits a simplified dependency structure shared globally across the entire dataset.
    \end{itemize}

    \item[Scenario 2: gipsMultQDA (Heteroscedastic / Shared Symmetry)] \hfill \\
    This scenario is designed to test the intermediate flexibility of the \texttt{gipsMultQDA} model.
    \begin{itemize}
        \item \textbf{Covariance Structure:} We assume heteroscedasticity. Each class $g$ possesses a unique covariance matrix $\boldsymbol{\Sigma}_g$ with distinct eigenvalues and variance scales ($\boldsymbol{\Sigma}_i \neq \boldsymbol{\Sigma}_j$ for $i \neq j$).
        \item \textbf{Symmetry:} Crucially, while the matrices differ numerically, they are all constrained by the \textbf{same} underlying permutation group $\Gamma$. This implies that while the magnitude of correlations may vary between classes, the pattern of invariant relationships between features remains constant across the population.
    \end{itemize}

    \item[Scenario 3: gipsQDA (Heteroscedastic / Unique Symmetry)] \hfill \\
    This is the most general and flexible scenario involving symmetry, targeting the \texttt{gipsQDA} model.
    \begin{itemize}
        \item \textbf{Covariance Structure:} The data is fully heteroscedastic; each class has a distinct covariance matrix $\boldsymbol{\Sigma}_g$.
        \item \textbf{Symmetry:} Unlike the previous scenario, there is no shared structural constraint. Each class covariance matrix $\boldsymbol{\Sigma}_g$ is projected onto its own \textbf{unique} permutation group $\Gamma_g$. This models complex environments where the dependency structure of features changes fundamentally depending on the class label.
    \end{itemize}

    \item[Scenario 4: Classic LDA (Homoscedastic / Unstructured)] \hfill \\
    This serves as the baseline for linear classification.
    \begin{itemize}
        \item \textbf{Covariance Structure:} All classes share a single covariance matrix $\boldsymbol{\Sigma}_{\text{common}}$.
        \item \textbf{Symmetry:} No permutation symmetry is imposed. The covariance matrix is generated from a random spectral distribution without any projection, representing a standard, unstructured multivariate normal distribution.
    \end{itemize}

    \item[Scenario 5: Classic QDA (Heteroscedastic / Unstructured)] \hfill \\
    This serves as the baseline for quadratic classification.
    \begin{itemize}
        \item \textbf{Covariance Structure:} Each class has a unique, distinct covariance matrix $\boldsymbol{\Sigma}_g$.
        \item \textbf{Symmetry:} Similar to Scenario 4, no permutation constraints are applied. Each matrix is independently generated and unstructured. This represents the most difficult setting for estimation when $n$ is small, as the number of parameters to estimate is maximal ($\frac{Kp(p+1)}{2}$).
    \end{itemize}
\end{description}
In every scenario scenario $n_k$ (number of observations generated from class k) varies between $\lceil\frac{16}{n_classes}\rceil$ and 50.


\section{Real data}
\label{sec:real_world_data}

In addition to synthetic simulations, the proposed models were evaluated on a diverse collection of real-world datasets sourced from public repositories. These datasets span various domains, including medical diagnostics, finance, and industrial quality control, providing a robust test bed for assessing model performance under realistic conditions.

The selected datasets present a wide range of challenges, including high dimensionality, class imbalance, and complex feature dependencies. Furthermore, several datasets contain binary and categorical features. By including these, we consciously accept the violation of the multivariate normality assumption inherent to Discriminant Analysis, allowing us to assess the practical robustness of the \texttt{gipsDA} framework in non-ideal settings.

Below, we provide a detailed description of each dataset and the specific preprocessing pipelines applied to prepare the data for experimentation.

\subsection{Heart Failure Prediction Dataset}

\begin{itemize}
    \item \textbf{Domain:} Medical Diagnostics
    \item \textbf{Description:} Contains clinical features used to predict mortality caused by heart failure. It represents a classic binary classification problem.
    \item \textbf{Target Variable:} \texttt{HeartDisease} (1: Heart Disease, 0: Normal).
    \item \textbf{Dimensions:} 11 features, 918 observations.
    \item \textbf{Class Balance:} Normal (410), Heart Disease (508).
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction} \cite{heart}
\end{itemize}

\textbf{Preprocessing Pipeline:}
Since this dataset contains categorical variables (e.g., \texttt{Sex}, \texttt{ChestPainType}), we applied \textbf{One-Hot Encoding} (dummy encoding), dropping one level per factor to avoid perfect multicollinearity. This transformation introduces binary features, explicitly violating the Gaussian assumption, but allows the models to utilize all available information. Following encoding, we removed 1 feature that exhibited near-zero variance (dominance ratio $> 0.9$).

\subsection{Breast Cancer Wisconsin (Diagnostic)}

\begin{itemize}
    \item \textbf{Domain:} Medical Diagnostics
    \item \textbf{Description:} Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, describing characteristics of the cell nuclei.
    \item \textbf{Target Variable:} \texttt{diagnosis} (M: Malignant, B: Benign).
    \item \textbf{Dimensions:} 30 features, 569 observations.
    \item \textbf{Class Balance:} Benign (357), Malignant (212).
    \item \textbf{Source:} \url{https://archive.ics.uci.edu/dataset/17/breast-cancer-wisconsin-diagnostic} \cite{breast}
\end{itemize}

\textbf{Preprocessing Pipeline:}
The data was already in a clean, numerical format suitable for analysis. No additional encoding, imputation, or feature removal was required.

\subsection{Sensorless Drive Diagnosis Data Set}
\label{sec:sensorless_drive_diagnosis}

\begin{itemize}
    \item \textbf{Domain:} Industrial Quality Control
    \item \textbf{Description:} Signals from a sensorless drive used to diagnose 11 different operating conditions (faults).
    \item \textbf{Target Variable:} Condition (11 classes).
    \item \textbf{Dimensions:} 48 features, 58,509 observations.
    \item \textbf{Class Balance:} Perfectly Balanced (5,319 per class).
    \item \textbf{Source:} \url{https://archive.ics.uci.edu/dataset/325/sensorless+drive+diagnosis} \cite{dataset_for_sensorless_drive_diagnosis_325}
\end{itemize}

\textbf{Preprocessing Pipeline:}
\begin{enumerate}
    \item \textbf{Scaling Small Values:} Some features contained values with very small magnitudes (e.g., $10^{-5}$), which can cause numerical instability in covariance matrix calculations. We applied a custom \texttt{fix\_tiny\_values} algorithm: for any column with a mean absolute value $< 0.01$, we scaled the column by a factor of $10^k$ to bring the magnitude into the standard range ($[1, 10]$).
    \item \textbf{Collinearity Removal:} We identified and removed 17 highly collinear features using a pairwise correlation threshold of 0.99.
\end{enumerate}

\subsection{Credit Card Fraud Detection}

\begin{itemize}
    \item \textbf{Domain:} Rare-Event Detection / Finance
    \item \textbf{Description:} A dataset of credit card transactions aimed at identifying fraudulent activity.
    \item \textbf{Target Variable:} \texttt{Class} (1: Fraud, 0: Legitimate).
    \item \textbf{Dimensions:} 30 features, $\approx 285,000$ observations.
    \item \textbf{Class Balance:} Highly Imbalanced (only 492 fraud cases, $\approx 0.17\%$).
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud} \cite{ulb_creditcard_fraud}
\end{itemize}

\textbf{Preprocessing Pipeline:}
The extreme class imbalance poses a problem for standard classifiers. We applied a strict \textbf{undersampling} strategy to create a balanced subset for training:
\begin{enumerate}
    \item We retained all minority class samples (Fraud).
    \item We sampled the majority class (Legitimate) such that the final dataset composition was 40\% Fraud and 60\% Legitimate.
    \item This resulted in a dataset of 1230 observations.
    \item Finally, we removed 2 collinear features using a correlation cutoff of 0.9.
\end{enumerate}

\subsection{EEG Brainwave Dataset: Feeling Emotions}

\begin{itemize}
    \item \textbf{Domain:} Medical / Neuroscience
    \item \textbf{Description:} Contains statistical features extracted from EEG brainwave signals collected from individuals in different emotional states.
    \item \textbf{Target Variable:} \texttt{emotion} (Positive, Neutral, Negative).
    \item \textbf{Dimensions:} 2548 features, 2132 observations.
    \item \textbf{Class Balance:} Balanced ($\approx 700$ per class).
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/birdy654/eeg-brainwave-dataset-feeling-emotions} \cite{birdy654_eeg_emotions}
\end{itemize}

\textbf{Preprocessing Pipeline:}
Given the extremely high dimensionality ($p=2548$), feature selection was critical.
\begin{enumerate}
    \item \textbf{Low Variance Filtering:} We removed 2 columns where a single value dominated more than 90\% of the observations.
    \item \textbf{Feature Selection:} We trained a Random Forest classifier on the full dataset and selected the \textbf{top 30 features} based on the Gini importance measure. This reduced the dimensionality to a manageable level while preserving the most predictive signals.
\end{enumerate}

\subsection{Steel Plates Faults Dataset}

\begin{itemize}
    \item \textbf{Domain:} Industrial Quality Control
    \item \textbf{Description:} Attributes of steel plates used to classify surface defects into 7 distinct fault types.
    \item \textbf{Target Variable:} Fault Type (e.g., \texttt{Pastry}, \texttt{Z\_Scratch}, \texttt{K\_Scatch}).
    \item \textbf{Dimensions:} 27 features, 1941 observations.
    \item \textbf{Class Balance:} Multi-class, ranging from 55 to 673 samples per class.
    \item \textbf{Source:} \url{https://archive.ics.uci.edu/dataset/198/steel-plates-faults} \cite{steel_plates_faults_198}
\end{itemize}

\textbf{Preprocessing Pipeline:}
\begin{enumerate}
    \item \textbf{Infinite Value Handling:} We scanned the dataset for infinite values, replaced them with \texttt{NA}, and subsequently removed rows containing missing values.
    \item \textbf{Scaling Small Values:} We applied the \texttt{fix\_tiny\_values} algorithm (described in \ref{sec:sensorless_drive_diagnosis}) to prevent numerical underflow.
    \item \textbf{Collinearity Removal:} We removed 6 features that exhibited a pairwise correlation greater than 0.9.
\end{enumerate}


\section{Performance Evaluation and Visualization Strategy}
\label{sec:visualization_methodology}

To construct the performance curves, we employ a systematic evaluation protocol that assesses model accuracy across a range of sample sizes. This process is controlled by a specific set of hyperparameters designed to ensure both statistical reliability and computational feasibility.

\subsection{Sample Size Sampling (The X-axis)}

The performance of the classifiers is evaluated at discrete intervals of the sample size, denoted as $n$. To effectively capture the model behavior in the critical "small $n$" regime (where $p \approx n$ or $p > n$) while covering the convergence behavior at larger sample sizes, we do not sample $n$ linearly. Instead, we employ a \textbf{logarithmic spacing} strategy.

Let $N_{\min}$ denote the lower bound (controlled by the parameter \texttt{lb}, typically set to 16) and $N_{\max}$ denote the upper bound (controlled by the parameter \texttt{ub}). The sequence of sample sizes is generated as follows:

\begin{equation}
    n_i = \left\lfloor \exp\left( \ln(N_{\min}) + \frac{i-1}{G-1} (\ln(N_{\max}) - \ln(N_{\min})) \right) \right\rceil, \quad \text{for } i = 1, \dots, G,
\end{equation}
where $G$ is the \texttt{granularity} parameter (typically set to 10 or more). This results in $G$ distinct evaluation points on the x-axis, densely clustered at the lower end where the regularization impact is most significant.

\subsection{Monte Carlo Repetitions (The Y-axis)}

Each data point plotted on the performance curve represents the \textbf{mean accuracy} calculated over a set of independent experiments, controlled by the parameter \texttt{n\_experiments} (typically 30).

For a specific sample size $n_i$:
\begin{enumerate}
    \item We perform $M$ independent repetitions (where $M = \texttt{n\_experiments}$).
    \item In each repetition $j$:
    \begin{itemize}
        \item \textbf{Synthetic Data:} Fresh training and testing sets are sampled directly from the underlying multivariate normal distributions $\mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$ defined by the saved scenario parameters. This ensures true independence between experiments.
        \item \textbf{Real-World Data:} A subset of size $n_i$ is randomly sampled (without replacement) from the full dataset to form the training set, with the remaining data used for testing.
    \end{itemize}
    \item The data is split according to the \texttt{tr\_ts\_split} ratio (e.g., 0.7 for training).
    \item The accuracy $A_{ij}$ is computed.
\end{enumerate}

\textbf{Handling Model Failures:} A crucial aspect of our benchmarking is robustness. If a model fails to fit (e.g., due to numerical instability or singularity that the regularization failed to handle), the experiment is not discarded. Instead, it is penalized by assigning an accuracy of $0$. The reported value for sample size $n_i$ is the arithmetic mean:
\[ \bar{A}_i = \frac{1}{M} \sum_{j=1}^{M} A_{ij}. \]

\subsection{Optimization Strategy}

The \texttt{gips}-based models require finding the optimal permutation group. The search strategy is determined automatically based on the dimensionality $p$ of the data to balance precision and computation time:

\begin{itemize}
    \item \textbf{Small Dimension ($p \le 9$):} We utilize a \textbf{Brute Force} optimizer. This algorithm exhaustively searches the entire space of permutation groups, guaranteeing the discovery of the global maximum for the posterior probability.
    \item \textbf{High Dimension ($p > 9$):} The permutation space becomes too large for exhaustive search. Here, we employ the \textbf{Metropolis-Hastings} (MH) algorithm, a stochastic Markov Chain Monte Carlo (MCMC) method. The depth of the search is controlled by the \texttt{max\_iter} parameter (typically set to 1000 iterations).
\end{itemize}

Additionally, the \texttt{MAP} parameter controls the estimation logic: if \texttt{TRUE}, the model projects covariance matrices onto the single best permutation found (Maximum A Posteriori); if \texttt{FALSE}, it utilizes Bayesian Model Averaging over the visited permutations.

\subsection{Statistical Significance Testing}
\label{sec:statistical_testing}

To determine whether the observed performance differences between the proposed \texttt{gips}-based models and the baselines are statistically significant rather than artifacts of random sampling, we employ a non-parametric testing framework. Specifically, we utilize a Wilcoxon signed-rank test \cite{hollander2013nonparametric}.
\subsubsection{Hypotheses}
For a given pair of models i.e., Model A (baseline, e.g., \texttt{QDAmod}) and Model B (proposed, e.g., \texttt{gipsQDA}), accompanied by a pair of equal size samples $(\mathbbm{Y}, \mathbbm{Z})$ sampled from prediction of models trained on the same number of observations, null and alternative hypotheses are defined as follows:
\begin{itemize}
    \item $H_0$ : The median of differences $(\mathbbm{Y} - \mathbbm{Z})$ is 0.
    \item $H_1$ : The median of differences is less than 0.
\end{itemize}
\subsubsection{Test procedure}
The Wilcoxon test is fundamentaly a one-sample test, as we are dealing with 2 samples we first create a single sample $\mathbbm{X}$ by subtracting the second one from the first:
\begin{equation}
    \mathbbm{X} = \mathbbm{Y} - \mathbbm{Z}
\end{equation},
then the following procedure is applied:
\begin{enumerate}
    \item Compute $|X_1|, \dots, |X_n|$.
    \item Sort $|X_1|, \dots, |X_n|$, and assign ranks $R_1, \dots, R_n$. The smallest observation is ranked 1.
    \item Compute test statistic as the signed-rank sum $T = \sum_{i=1}^{n} sgn(X_i)R_i$
    \item By comparing T to its distribution under null hypothesis compute p-value.
\end{enumerate}
The distribution of T-statistic can be easly calculated under the assmuption of uniform distribution of rank assignment.


%\subsubsection{Hypothesis Definition}
%
%For a given pair of models, Model A (baseline, e.g., \texttt{QDAmod}) and Model B (proposed, e.g., \texttt{gipsQDA}), we define the following hypotheses:
%\begin{itemize}
%    \item $H_0$ (Null Hypothesis): There is no difference in performance between Model A and Model B. The assignment of accuracy scores to the models is arbitrary within each experimental block.
%    \item $H_1$ (Alternative Hypothesis): Model B performs significantly better than Model A (one-sided test).
%\end{itemize}
%
%\subsubsection{Test Statistic Construction}
%
%Let $G$ denote the number of sample size levels (granularity). For a specific sample size $n_k$ (where $k=1, \dots, G$), we perform $M$ independent experiments. Let $Acc_{k,j}^{(A)}$ and $Acc_{k,j}^{(B)}$ denote the accuracy of Model A and Model B, respectively, in the $j$-th experiment at sample size $n_k$.
%
%First, we calculate the mean difference in accuracy for each sample size block $k$:
%\begin{equation}
%    \bar{\delta}_k = \frac{1}{M} \sum_{j=1}^{M} \left( Acc_{k,j}^{(B)} - Acc_{k,j}^{(A)} \right).
%\end{equation}
%
%The observed test statistic, $T_{\text{obs}}$, is defined as the grand mean of these block-wise differences:
%\begin{equation}
%    T_{\text{obs}} = \frac{1}{G} \sum_{k=1}^{G} \bar{\delta}_k.
%\end{equation}
%This statistic effectively aggregates the performance gain across the entire learning curve, treating each sample size level as equally important.
%
%\subsubsection{Permutation Procedure}
%
%To approximate the null distribution of the test statistic, we perform $B = 5000$ Monte Carlo permutations. The procedure preserves the block structure of the data:
%
%\begin{enumerate}
%    \item \textbf{Within-Block Swapping:} For every specific sample size $n_k$ and every experiment $j$, we randomly swap the accuracy values of Model A and Model B with a probability of $0.5$. Formally, we define a Bernoulli variable $S_{k,j} \sim \text{Bern}(0.5)$.
%    \[
%    (X, Y) =
%    \begin{cases}
%    (Acc_{k,j}^{(A)}, Acc_{k,j}^{(B)}) & \text{if } S_{k,j} = 0 \\
%    (Acc_{k,j}^{(B)}, Acc_{k,j}^{(A)}) & \text{if } S_{k,j} = 1
%    \end{cases}
%    \]
%    \item \textbf{Statistic Recomputation:} Using these permuted labels, we calculate a new permuted statistic $T^*_b$.
%    \item \textbf{P-value Calculation:} After $B$ repetitions, the empirical p-value is calculated as the proportion of permuted statistics that are greater than or equal to the observed statistic:
%    \begin{equation}
%        p\text{-value} = \frac{1}{B} \sum_{b=1}^{B} \mathbb{I}(T^*_b \ge T_{\text{obs}}).
%    \end{equation}
%\end{enumerate}
%
%A low p-value (typically $< 0.05$) indicates that the observed performance improvement of Model B over Model A is statistically significant and unlikely to have occurred by chance.


% ======================================================================
%                          Experiments
% ======================================================================

\chapter{Experimental Results}
\label{ch:results}

This chapter presents the empirical evaluation of the \texttt{gipsDA} framework. The results are divided into two main parts: controlled simulations on synthetic data and benchmarking on real-world datasets.

\section{Synthetic Data Experiments}

In this section, we analyze the performance of the proposed classification models against the regularized baselines. To ensure clarity when interpreting the results, we first establish the mapping between the theoretical model names and the labels used in the visualization plots.

\subsection{Model Nomenclature}

The comparative analysis involves six distinct models. In the generated plots and subsequent discussions, the following abbreviations are used:

\begin{itemize}
    \item \textbf{\texttt{gipsldawa}}: The \texttt{gipsLDA} model utilizing the \textbf{weighted average} estimator (Bayesian Model Averaging).
    \item \textbf{\texttt{gipsldacl}}: The \texttt{gipsLDA} model utilizing the \textbf{classic} estimator (unbiased pooled covariance).
    \item \textbf{\texttt{gipsqda}}: The \texttt{gipsQDA} model, where each class has a unique covariance structure.
    \item \textbf{\texttt{gipsmultqda}}: The \texttt{gipsMultQDA} model, where classes share a common permutation symmetry.
    \item \textbf{\texttt{lda}}: The baseline Linear Discriminant Analysis model, modified with the eigenvalue regularization described in appendix \ref{sec:baseline_regularization} (\texttt{LDAmod}).
    \item \textbf{\texttt{qda}}: The baseline Quadratic Discriminant Analysis model, modified with eigenvalue regularization (\texttt{QDAmod}).
\end{itemize}

\subsection{Experimental Scope and Hyperparameters}

We conducted an extensive simulation study comprising over 100 independent large-scale experiments. These experiments explored a vast grid of hyperparameters, including varying dimensions ($p$), numbers of classes ($k$), eigenvalue distributions (Exponential, Log-Normal, Chi-squared), and optimization strategies (MAP = TRUE vs. MAP = FALSE). From this extensive collection, we selected \textbf{representative configurations} (parameter sets) for detailed discussion in this chapter. These selections best illustrate the specific strengths and limitations of the proposed methods.

The optimization strategy for the \texttt{gips}-based models was strictly dependent on the problem dimensionality $p$:
\begin{itemize}
    \item \textbf{For $p=5$:} The permutation space is sufficiently small ($5! = 120$) to allow for an exhaustive search. Therefore, the \textbf{Brute Force} optimizer was employed to guarantee finding the global maximum of the posterior probability.
    \item \textbf{For $p=10$:} The permutation space becomes computationally intractable ($10! \approx 3.6 \times 10^6$). In these cases, the stochastic \textbf{Metropolis-Hastings} algorithm was used, constrained by a fixed \texttt{max\_iter} parameter to balance accuracy and runtime.
\end{itemize}

\subsection{Permutation Selection Protocol}

A critical component of the data generation process is the selection of the "ground truth" permutation symmetries imposed on the covariance matrices. We defined specific pools of permutations for $p=5$ and $p=10$, ranging from simple structures (e.g., sparse transpositions) to complex ones (e.g., full cycles).

The selection logic depends on the target scenario:

\begin{enumerate}
    \item \textbf{Shared Symmetry Scenarios (gipsLDA, gipsMultQDA):}
    In these scenarios, the data generation assumes a single permutation structure is shared across all classes. We select \textbf{one} specific permutation type from the pool—either \texttt{sparse\_transposition}, \texttt{dense\_transposition}, or \texttt{full\_cycle}—and apply it to generate the covariance matrices for all groups.

    \item \textbf{Unique Symmetry Scenario (gipsQDA):}
    This scenario assumes that each class possesses a distinct symmetry structure. If the number of classes is $k$, we select the first $k$ permutations from the pre-defined list.
    \begin{itemize}
        \item For example, if $k=5$, we take the first 5 permutations from the $p$-specific list.
        \item If $k=10$, we utilize all 10 available permutations in the list.
    \end{itemize}
\end{enumerate}

The specific permutation lists used for generation are defined as follows:

\begin{itemize}
    \item \textbf{For $p=5$:}
    \begin{enumerate}
        \item $(1,2)$ [sparse transposition]
        \item $(1,2)(3,4)$ [dense transposition]
        \item $(1,2,3,4,5)$ [full cycle]
        \item $(1,3)(2,4)$
        \item $(1,2,3)(4,5)$
        \item ... (and so on up to 10 distinct permutations).
    \end{enumerate}
    \item \textbf{For $p=10$:}
    \begin{enumerate}
        \item $(1,2)(5,6)(8,9)$ [sparse transposition]
        \item $(1,2)(3,4)(5,6)(7,8)(9,10)$ [dense transposition]
        \item $(1,2,\dots,10)$ [full cycle]
        \item $(1,3)(2,4)(7,9)$
        \item ... (and so on up to 10 distinct permutations).
    \end{enumerate}
\end{itemize}

Below, we present the results for the selected parameter configurations.
For each configuration, a composite figure displays five subplots corresponding to the five fundamental data generation scenarios defined in Chapter \ref{ch:testing_methodology}.

% ------------------------------------------------------------------------
% SIMULATION SETS
% ------------------------------------------------------------------------
\subsection{Simulation Set 1: High Dimension and 10 Classes}
\label{sec:sim_set_1}

\textbf{Configuration:} $p=10$, $k=10$, Distribution: Chi-squared, MAP: \texttt{TRUE}, Main Permutation: Dense Transposition.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_10_10_chisq_dense_trans_TRUE}
    \caption{Learning curves for Simulation Set 1. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_10_10_chisq_dense_trans_TRUE_auc}
    \caption{Learning curves showing the \textbf{Area Under the ROC Curve (AUC)} for Simulation Set 1. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_1_auc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_10_10_chisq_dense_trans_TRUE_spe}
    \caption{\textbf{Comparative boxplots} illustrating the distribution of performance for Simulation Set 1. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title), visualizing the variability of the models.}
    \label{fig:sim_set_1_box}
\end{figure}

\begin{table}[H]
    \centering
    \setlength{\tabcolsep}{4pt}
    \small
    \begin{tabular}{lllc c}
    \toprule
    \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{No. Obs.} & \textbf{p-value} \\
    \midrule
    % --- SCENARIO: LDA ---
    \multirow{6}{*}{lda}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA classic} & 34 & 0.0000 \\
      & & & 74 & 0.0656 \\
      \cmidrule{2-5}
      & LDAmod & gipsLDA weighted & 34 & 0.0000 \\
      \cmidrule{2-5}
      & QDAmod & gipsQDA & 34 & 0.0462 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsMultQDA} & 34 & 0.0465 \\
      & & & 74 & 0.0000 \\
    \midrule
    % --- SCENARIO: GIPSLDA ---
    \multirow{6}{*}{gipslda}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA classic} & 34 & 0.0000 \\
      & & & 74 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA weighted} & 34 & 0.0000 \\
      & & & 74 & 0.0017 \\
      \cmidrule{2-5}
      & gipsLDA classic & gipsLDA weighted & 34 & 0.0079 \\
      \cmidrule{2-5}
      & QDAmod & gipsMultQDA & 74 & 0.0000 \\
    \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results for Simulation Set 1 - Part 1. The table presents p-values for specific model pairs in LDA-based scenarios.}
    \label{tab:stat_sim_1_part2}
\end{table}

\begin{table}[H]
    \centering
    \setlength{\tabcolsep}{4pt}
    \small
    \begin{tabular}{lllc c}
    \toprule
    \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{No. Obs.} & \textbf{p-value} \\
    \midrule
    % --- SCENARIO: QDA ---
    \multirow{7}{*}{qda}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA classic} & 34 & 0.0000 \\
      & & & 74 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA weighted} & 34 & 0.0000 \\
      & & & 74 & 0.0000 \\
      \cmidrule{2-5}
      & gipsLDA classic & gipsLDA weighted & 34 & 0.0439 \\
      \cmidrule{2-5}
      & QDAmod & gipsMultQDA & 74 & 0.0000 \\
    \midrule
    % --- SCENARIO: GIPSQDA ---
    \multirow{5}{*}{gipsqda}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA classic} & 34 & 0.0000 \\
      & & & 74 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA weighted} & 34 & 0.0000 \\
      & & & 74 & 0.0000 \\
      \cmidrule{2-5}
      & QDAmod & gipsMultQDA & 74 & 0.0000 \\
    \midrule
    % --- SCENARIO: GIPSMULTQDA ---
    \multirow{5}{*}{gipsmultqda}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA classic} & 34 & 0.0000 \\
      & & & 74 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA weighted} & 34 & 0.0000 \\
      & & & 74 & 0.0000 \\
      \cmidrule{2-5}
      & QDAmod & gipsMultQDA & 74 & 0.0000 \\
    \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results for Simulation Set 1 - Part 2. The table presents p-values for specific model pairs in QDA-based scenarios.}
    \label{tab:stat_sim_1_part1}
\end{table}

\textbf{Observations:}
%\textcolor{red}{Do napisania od nowa}
In numerous scenarios we observe null hypothesis rejection signifiyng supremacy of \texttt{gips} models. While this is a result backed with statistical machinery, overall look of accuracy line and boxplots does not reveal a substantial advantage.
%In this high-dimensional multi-class setting ($p=10, k=10$) characterized by a "Dense Transposition" symmetry structure, the \texttt{gips}-based models demonstrate a decisive advantage.
%The statistical analysis reveals that \texttt{gipsMultQDA} significantly outperforms the \texttt{QDAmod} baseline across all heteroscedastic scenarios, achieving p-values near zero in both the \texttt{gipsqda} and \texttt{gipsmultqda} scenarios.
%Furthermore, within the homoscedastic \texttt{gipslda} scenario, the \texttt{gipsLDA classic} estimator provides a statistically significant improvement over \texttt{LDAmod} ($pvalue < 0.001$).
%These results confirm that even with the Bayesian Model Averaging strategy (\texttt{MAP = FALSE}), enforcing the shared permutation constraint successfully regularizes the covariance estimation in data-scarce regimes.


\subsection{Simulation Set 2: High Dimension and Binary Classification}
\label{sec:sim_set_2}

\textbf{Configuration:} $p=10$, $k=2$, Distribution: Chi-squared, MAP: \texttt{FALSE}, Main Permutation: Dense Transposition.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_10_2_chisq_dense_trans_FALSE}
    \caption{Learning curves for Simulation Set 2. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_10_2_chisq_dense_trans_FALSE_auc}
    \caption{Learning curves showing the \textbf{Area Under the ROC Curve (AUC)} for Simulation Set 2. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_2_auc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_10_2_chisq_dense_trans_FALSE_spe}
    \caption{\textbf{Comparative boxplots} illustrating the distribution of performance for Simulation Set 2. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title), visualizing the variability of the models.}
    \label{fig:sim_set_2_box}
\end{figure}


\begin{table}[H]
    \centering
    \setlength{\tabcolsep}{4pt}
    \small
    \begin{tabular}{lllc c}
    \toprule
    \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{No. Obs.} & \textbf{p-value} \\
    \midrule
    % --- SCENARIO: QDA ---
    \multirow{9}{*}{qda}
      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA classic} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA weighted} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{3}{*}{QDAmod} & \multirow{3}{*}{gipsMultQDA} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
    \midrule
    % --- SCENARIO: GIPSQDA ---
    \multirow{9}{*}{gipsqda}
      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA classic} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA weighted} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{3}{*}{QDAmod} & \multirow{3}{*}{gipsMultQDA} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
    \midrule
    % --- SCENARIO: GIPSMULTQDA ---
    \multirow{9}{*}{gipsmultqda}
      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA classic} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA weighted} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{3}{*}{QDAmod} & \multirow{3}{*}{gipsMultQDA} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
    \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results for Simulation Set 2 - Part 1. The table presents p-values for specific model pairs in QDA-based scenarios.}
    \label{tab:stat_sim_2_part1}
\end{table}

\begin{table}[H]
    \centering
    \setlength{\tabcolsep}{4pt}
    \small
    \begin{tabular}{lllc c}
    \toprule
    \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{No. Obs.} & \textbf{p-value} \\
    \midrule
    % --- SCENARIO: LDA ---
    \multirow{9}{*}{lda}
      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA classic} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA weighted} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{3}{*}{QDAmod} & \multirow{3}{*}{gipsMultQDA} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
    \midrule
    % --- SCENARIO: GIPSLDA ---
    \multirow{9}{*}{gipslda}
      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA classic} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA weighted} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
      \cmidrule{2-5}
      & \multirow{3}{*}{QDAmod} & \multirow{3}{*}{gipsMultQDA} & 16 & 0.0000 \\
      & & & 24 & 0.0000 \\
      & & & 36 & 0.0000 \\
    \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results for Simulation Set 2 - Part 2. The table presents p-values for specific model pairs in LDA-based scenarios.}
    \label{tab:stat_sim_2_part2}
\end{table}

\textbf{Observations:}
%\textcolor{red}{Do napisania}
As binary classification is a much simpler task that predicting 1 of 10 classes line and boxplots seem even less informative, at the same time statistical testing suggests an even bigger advantage of \texttt{gips} models than in the previous scenario.
%In this binary classification setting ($p=10, k=2$) with covariance matrix with eigenvalues coming form Chi-squared distribution, the benefits of permutation-based regularization are profound.
%The statistical analysis reveals overwhelming evidence ($p \approx 0$) favoring the \texttt{gips}-based models over the regularized baselines across almost all scenarios.
%Notably, \texttt{gipsMultQDA} consistently outperforms \texttt{QDAmod} in heteroscedastic environments (Scenarios \texttt{gipsqda} and \texttt{gipsmultqda}), confirming that the shared "Dense Transposition" constraint effectively stabilizes estimation without sacrificing necessary flexibility.
%Similarly, in homoscedastic scenarios, \texttt{gipsLDA classic} proves significantly superior to \texttt{LDAmod}, demonstrating that even with a small number of classes, the reduction in free parameters yields a substantial gain in predictive accuracy.


\subsection{Simulation Set 3: Low Dimension and 10 Classes}
\label{sec:sim_set_3}

\textbf{Configuration:} $p=5$, $k=10$, Distribution: Log-Normal, MAP: \texttt{TRUE}, Main Permutation: Full Cycle.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_5_10_lnorm_full_cycle_TRUE}
    \caption{Learning curves for Simulation Set 3. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_5_10_lnorm_full_cycle_TRUE_auc}
    \caption{Learning curves showing the \textbf{Area Under the ROC Curve (AUC)} for Simulation Set 3. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
    \label{fig:sim_set_3_auc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/synth_5_10_lnorm_full_cycle_TRUE_spe}
    \caption{\textbf{Comparative boxplots} illustrating the distribution of performance for Simulation Set 3. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title), visualizing the variability of the models.}
    \label{fig:sim_set_3_box}
\end{figure}

\begin{table}[H]
    \centering
    \setlength{\tabcolsep}{4pt}
    \small
    \begin{tabular}{lllc c}
    \toprule
    \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{No. Obs.} & \textbf{p-value} \\
    \midrule
    % --- SCENARIO: LDA ---
    \multirow{2}{*}{lda}
      & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsMultQDA} & 34 & 0.0000 \\
      & & & 74 & 0.0071 \\
    \midrule
    % --- SCENARIO: GIPSLDA ---
    \multirow{7}{*}{gipslda}
      & LDAmod & gipsLDA classic & 34 & 0.0040 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA weighted} & 34 & 0.0000 \\
      & & & 74 & 0.0013 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{gipsLDA classic} & \multirow{2}{*}{gipsLDA weighted} & 34 & 0.0013 \\
      & & & 74 & 0.0094 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsMultQDA} & 34 & 0.0095 \\
      & & & 74 & 0.0001 \\
    \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results for Simulation Set 3 - Part 1. The table presents p-values for specific model pairs in LDA-based scenarios.}
    \label{tab:stat_sim_3_part2}
\end{table}

\begin{table}[H]
    \centering
    \setlength{\tabcolsep}{4pt}
    \small
    \begin{tabular}{lllc c}
    \toprule
    \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{No. Obs.} & \textbf{p-value} \\
    \midrule
    % --- SCENARIO: QDA ---
    \multirow{1}{*}{qda}
      & LDAmod & gipsLDA classic & 34 & 0.0031 \\
    \midrule
    % --- SCENARIO: GIPSQDA ---
    \multirow{7}{*}{gipsqda}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA classic} & 34 & 0.0092 \\
      & & & 74 & 0.0618 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA weighted} & 34 & 0.0001 \\
      & & & 74 & 0.0183 \\
      \cmidrule{2-5}
      & gipsLDA classic & gipsLDA weighted & 34 & 0.0344 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsMultQDA} & 34 & 0.0000 \\
      & & & 74 & 0.0004 \\
    \midrule
    % --- SCENARIO: GIPSMULTQDA ---
    \multirow{6}{*}{gipsmultqda}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA classic} & 34 & 0.0012 \\
      & & & 74 & 0.0009 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA weighted} & 34 & 0.0000 \\
      & & & 74 & 0.0696 \\
      \cmidrule{2-5}
      & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsMultQDA} & 34 & 0.0000 \\
      & & & 74 & 0.0000 \\
    \bottomrule
    \end{tabular}
    \caption{Selected statistical comparison results for Simulation Set 3 - Part 2. The table presents p-values for specific model pairs in QDA-based scenarios.}
    \label{tab:stat_sim_3_part1}
\end{table}

\textbf{Observations:}
%\textcolor{red}{Do napisania od nowa}
Due to disparity in dimensionality and the number of classes in favor of the former this problem is much more difficult than the 2 previous ones. This manifests in the form of lower plateaus of accuracy and a visible difference in model performance.
Tests results similarly to the previously discussed scenarios do stem from visualized data at the first glance though.
%In this low-dimensional, multi-class setting ($p=5, k=10$) with covariance matrix with eigenvalues coming form Log-Normal distribution, the distinction between models is more nuanced than in higher-dimensional scenarios.
%Due to the low feature count, the "curse of dimensionality" is less pronounced, allowing standard baselines to remain competitive; consequently, many direct comparisons between \texttt{gips}-based models and their baselines did not yield statistically significant differences ($p > 0.05$).
%However, the results in the \texttt{gipslda} scenario highlight the robustness of the \texttt{gipsMultQDA} approach.
%Even when the ground truth implies homoscedasticity, the \texttt{gipsMultQDA} model—which enforces the "Full Cycle" symmetry while allowing for potential scale variations—significantly outperforms the regularized \texttt{LDAmod} ($pvalue \approx 0$).
%This suggests that the shared permutation constraint captures the complex dependency structure of the Log-Normal data more effectively than the standard linear assumption.

%\subsection{Simulation Set 4: Low Dimension and Binary Classification}
%\label{sec:sim_set_4}
%
%\textbf{Configuration:} $p=5$, $k=2$, Distribution: Chi-squared, MAP: \texttt{False}, Main Permutation: Dense Transposition.
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=\textwidth]{img/synth_5_2_chisq_dense_trans_FALSE}
%    \caption{Learning curves for Simulation Set 4. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
%    \label{fig:sim_set_4}
%\end{figure}
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=\textwidth]{img/synth_5_2_chisq_dense_trans_FALSE_auc}
%    \caption{Learning curves showing the \textbf{Area Under the ROC Curve (AUC)} for Simulation Set 4. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
%    \label{fig:sim_set_4_auc}
%\end{figure}
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=\textwidth]{img/synth_5_2_chisq_dense_trans_FALSE_spe}
%    \caption{\textbf{Comparative boxplots} illustrating the distribution of performance for Simulation Set 4. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title), visualizing the variability of the models.}
%    \label{fig:sim_set_4_box}
%\end{figure}
%
%
%\begin{table}[H]
%    \centering
%    \setlength{\tabcolsep}{4pt}
%    \small
%    \begin{tabular}{lllc c}
%    \toprule
%    \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{No. Obs.} & \textbf{p-value} \\
%    \midrule
%    % --- SCENARIO: QDA ---
%    \multirow{12}{*}{qda}
%      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA classic} & 16 & 0.0000 \\
%      & & & 24 & 0.0006 \\
%      & & & 36 & 0.0024 \\
%      \cmidrule{2-5}
%      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA weighted} & 16 & 0.0003 \\
%      & & & 24 & 0.0004 \\
%      & & & 36 & 0.0167 \\
%      \cmidrule{2-5}
%      & \multirow{3}{*}{QDAmod} & \multirow{3}{*}{gipsQDA} & 16 & 0.0819 \\
%      & & & 24 & 0.0050 \\
%      & & & 36 & 0.0115 \\
%      \cmidrule{2-5}
%      & \multirow{3}{*}{QDAmod} & \multirow{3}{*}{gipsMultQDA} & 16 & 0.0006 \\
%      & & & 24 & 0.0000 \\
%      & & & 36 & 0.0001 \\
%    \midrule
%    % --- SCENARIO: GIPSQDA ---
%    \multirow{9}{*}{gipsqda}
%      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA classic} & 16 & 0.0000 \\
%      & & & 24 & 0.0001 \\
%      & & & 36 & 0.0042 \\
%      \cmidrule{2-5}
%      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA weighted} & 16 & 0.0003 \\
%      & & & 24 & 0.0337 \\
%      \cmidrule{2-5}
%      & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsQDA} & 24 & 0.0738 \\
%      & & & 36 & 0.0072 \\
%      \cmidrule{2-5}
%      & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsMultQDA} & 16 & 0.0165 \\
%      & & & 24 & 0.0000 \\
%    \midrule
%    % --- SCENARIO: GIPSMULTQDA ---
%    \multirow{11}{*}{gipsmultqda}
%      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA classic} & 24 & 0.0001 \\
%      & & & 36 & 0.0068 \\
%      \cmidrule{2-5}
%      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA weighted} & 16 & 0.0009 \\
%      & & & 24 & 0.0002 \\
%      & & & 36 & 0.0091 \\
%      \cmidrule{2-5}
%      & gipsLDA classic & gipsLDA weighted & 16 & 0.0163 \\
%      \cmidrule{2-5}
%      & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsQDA} & 24 & 0.0117 \\
%      & & & 36 & 0.0000 \\
%      \cmidrule{2-5}
%      & \multirow{3}{*}{QDAmod} & \multirow{3}{*}{gipsMultQDA} & 16 & 0.0000 \\
%      & & & 24 & 0.0000 \\
%      & & & 36 & 0.0000 \\
%    \bottomrule
%    \end{tabular}
%    \caption{Selected statistical comparison results for Simulation Set 4 - Part 1. The table presents p-values for specific model pairs in QDA-based scenarios.}
%    \label{tab:stat_sim_4_part1}
%\end{table}
%
%\begin{table}[H]
%    \centering
%    \setlength{\tabcolsep}{4pt}
%    \small
%    \begin{tabular}{lllc c}
%    \toprule
%    \textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{No. Obs.} & \textbf{p-value} \\
%    \midrule
%    % --- SCENARIO: LDA ---
%    \multirow{3}{*}{lda}
%      & \multirow{2}{*}{LDAmod} & \multirow{2}{*}{gipsLDA classic} & 16 & 0.0041 \\
%      & & & 24 & 0.0209 \\
%      \cmidrule{2-5}
%      & LDAmod & gipsLDA weighted & 16 & 0.0200 \\
%    \midrule
%    % --- SCENARIO: GIPSLDA ---
%    \multirow{10}{*}{gipslda}
%      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA classic} & 16 & 0.0000 \\
%      & & & 24 & 0.0039 \\
%      & & & 36 & 0.0011 \\
%      \cmidrule{2-5}
%      & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA weighted} & 16 & 0.0000 \\
%      & & & 24 & 0.0022 \\
%      & & & 36 & 0.0298 \\
%      \cmidrule{2-5}
%      & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsQDA} & 16 & 0.0077 \\
%      & & & 36 & 0.0014 \\
%      \cmidrule{2-5}
%      & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsMultQDA} & 16 & 0.0000 \\
%      & & & 36 & 0.0071 \\
%    \bottomrule
%    \end{tabular}
%    \caption{Selected statistical comparison results for Simulation Set 4 - Part 2. The table presents p-values for specific model pairs in LDA-based scenarios.}
%    \label{tab:stat_sim_4_part2}
%\end{table}


%\textbf{Observations:}
%\textcolor{red}{Do Napisania od nowa}
%In this low-dimensional binary setting ($p=5, k=2$), the classification task proves to be relatively straightforward, with all models rapidly converging to near-perfect accuracy (saturation effect). Despite this ceiling effect, the \texttt{gips}-based regularization still yields statistically significant improvements. In the heteroscedastic \texttt{gipsqda} scenario, both \texttt{gipsQDA} and \texttt{gipsMultQDA} significantly outperform the standard \texttt{QDAmod} ($pvalue < 0.05$). A notable deviation from previous trends is observed here: the \texttt{gipsLDA weighted average} estimator demonstrates a statistically significant advantage over both \texttt{LDAmod} and the \texttt{gipsLDA classic} estimator. This suggests that in specific low-dimensional, non-Gaussian (Chi-squared) regimes, averaging over the permutation posterior captures the data structure more effectively than a single point estimate.
%
%\subsection{Simulation Set 5: Low Dimension and 5 Classes}
%\label{sec:sim_set_5}
%
%\textbf{Configuration:} $p=5$, $k=5$, Distribution: Exponential, MAP: \texttt{False}, Main Permutation: Sparse Transposition.
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=\textwidth]{img/synth_5_5_exp_sparse_trans_FALSE}
%    \caption{Learning curves showing \textbf{Classification Accuracy} for Simulation Set 5. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
%    \label{fig:sim_set_5}
%\end{figure}
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=\textwidth]{img/synth_5_5_exp_sparse_trans_FALSE_auc}
%    \caption{Learning curves showing the \textbf{Area Under the ROC Curve (AUC)} for Simulation Set 5. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title). All six models are compared within each scenario.}
%    \label{fig:sim_set_5_auc}
%\end{figure}
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=\textwidth]{img/synth_5_5_exp_sparse_trans_FALSE_spe}
%    \caption{\textbf{Comparative boxplots} illustrating the distribution of performance for Simulation Set 5. The figure contains five subplots, each corresponding to a specific ground-truth scenario (indicated by the facet title), visualizing the variability of the models.}
%    \label{fig:sim_set_5_box}
%\end{figure}
%
%
%\begin{table}[H]
%\centering
%\label{tab:stat_sim_5}
%\begin{tabular}{lllcS[table-format=1.4]}
%\toprule
%\textbf{Scenario} & \textbf{Model A} & \textbf{Model B} & \textbf{No. Obs.} & \textbf{p-value} \\
%\midrule
%\multirow{2}{*}{qda}
%  & LDAmod & gipsLDA classic & 16 & 0.0049 \\
%  & LDAmod & gipsLDA weighted & 16 & 0.0099 \\
%\midrule
%\multirow{3}{*}{gipsqda}
%  & LDAmod & gipsLDA classic & 16 & 0.0331 \\
%  & LDAmod & gipsLDA weighted & 16 & 0.0287 \\
%  & QDAmod & gipsMultQDA & 16 & 0.0029 \\
%\midrule
%\multirow{6}{*}{gipsmultqda}
%  & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA classic} & 16 & 0.0001 \\
%  & & & 29 & 0.0000 \\
%  & & & 54 & 0.0028 \\
%  \cmidrule{2-5}
%  & \multirow{3}{*}{LDAmod} & \multirow{3}{*}{gipsLDA weighted} & 16 & 0.0008 \\
%  & & & 29 & 0.0025 \\
%  & & & 54 & 0.0017 \\
%\midrule
%\multirow{3}{*}{lda}
%  & LDAmod & gipsLDA classic & 16 & 0.0654 \\
%  \cmidrule{2-5}
%  & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsMultQDA} & 16 & 0.0032 \\
%  & & & 29 & 0.0000 \\
%\midrule
%\multirow{2}{*}{gipslda}
%  & \multirow{2}{*}{QDAmod} & \multirow{2}{*}{gipsMultQDA} & 16 & 0.0809 \\
%  & & & 29 & 0.0589 \\
%\bottomrule
%\end{tabular}
%\caption{Selected statistical comparison results including Number of Observations. The table presents p-values for specific model pairs derived from the raw simulation data.}
%\end{table}

%\textbf{Observations:}
%\textcolor{red}{Do napisania od nowa}
%This configuration ($p=5, k=5$) with covariance matrix with eigenvalues coming form Exponential distribution and a "Sparse Transposition" symmetry presents a scenario where the performance gap between the proposed models and the baselines narrows significantly. The statistical analysis indicates that for the majority of comparisons, including the primary \texttt{gipsQDA} vs. \texttt{QDAmod} case, the difference in accuracy is not statistically significant ($pvalue > 0.05$). This suggests that with the "Sparse Transposition" structure—which imposes relatively weak constraints—and non-Gaussian noise, the regularization benefit of the \texttt{gips} models is less pronounced than in denser symmetry settings. Additionally, the plots reaffirm the trend that for homoscedastic scenarios (\texttt{lda} and \texttt{gipslda}), the \texttt{gipsLDA classic} estimator is far superior to the \texttt{weighted average} variant, which performs poorly in this specific setup.

\section{Real-World Data Results}

In this section, we evaluate the models on the six real-world datasets described in Chapter \ref{ch:testing_methodology}.
Unlike the synthetic experiments, the "true" covariance structure is unknown, and the data may violate Gaussian assumptions.

Each subsection presents the learning curve for a specific dataset, followed by a statistical comparison of the best-performing \texttt{gips} model against its relevant baseline.

% ========================================================================
% SZABLON DLA DANYCH REALNYCH
% ========================================================================

\subsection{Heart Failure Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/heart_plot}
    \caption{Classification accuracy on the Heart Failure dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:real_heart}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/heart_plot_auc}
    \caption{Area Under the ROC Curve (AUC) on the Heart Failure dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:heart_auc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/heart_boxplot}
    \caption{Classification accuracy on the Heart Failure dataset. The boxplots compare the performance distribution of all six models.}
    \label{fig:heart_box}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{Number of observations} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & 20 & $0.0000$ \\
                                  & 35 & $0.0000$ \\
                                  & 76 & $0.0000$ \\
        \midrule
        LDAmod vs gipsLDA weighted average & 20 & $0.0000$ \\
                                           & 35 & $0.0000$ \\
                                           & 76 & $0.4091$ \\
        \midrule
        gipsLDA classic vs gipsLDA weighted average & 20 & $0.9768$ \\
                                                    & 35 & $1.0000$ \\
                                                    & 76 & $1.0000$ \\
        \midrule
        QDAmod vs gipsQDA & 20 & $0.0157$ \\
                          & 35 & $0.0960$ \\
                          & 76 & $0.4234$ \\
        \midrule
        QDAmod vs gipsMultQDA & 20 & $0.0000$ \\
                              & 35 & $0.0000$ \\
                              & 76 & $0.0066$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison for Heart Failure dataset.}
\end{table}

\textbf{Analysis:}
Despite the application of One-Hot Encoding, which explicitly violates the multivariate normality assumption, the \texttt{gips}-based models demonstrate remarkable robustness. Visually, the plots indicate a clear trend: in the data-scarce regime (small number of observations), models utilizing \texttt{gips} regularization achieve higher accuracy, whereas for larger training sets, all models converge toward a similar performance plateau.

Statistically, this advantage is most durable for the constrained models. \texttt{gipsLDA classic} consistently outperforms the regularized \texttt{LDAmod} baseline (even at $n=76$). Similarly, within the quadratic family, \texttt{gipsMultQDA} proves superior to \texttt{QDAmod}, retaining statistical significance throughout the experiment (p-value $=0.0066$ at $n=76$). In contrast, the more flexible \texttt{gipsQDA} and the \texttt{gipsLDA weighted average} estimator provide a significant benefit only in the early learning phase ($n=20$), becoming statistically indistinguishable from their baselines as data availability increases (p-value $> 0.4$ at $n=76$).

\subsection{Breast Cancer Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/breast_plot}
    \caption{Classification accuracy on the Breast Cancer dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:breast}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/breast_plot_auc}
    \caption{Area Under the ROC Curve (AUC) on the Breast Cancer dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:breast_auc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/breast_boxplot}
    \caption{Classification accuracy on the Breast Cancer dataset. The boxplots compare the performance distribution of all six models.}
    \label{fig:breast_box}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{Number of observations} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & 20 & $0.0010$ \\
                                  & 30 & $0.0096$ \\
                                  & 51 & $0.0833$ \\
        \midrule
        LDAmod vs gipsLDA weighted average & 20 & $0.0000$ \\
                                           & 30 & $0.8072$ \\
                                           & 51 & $1.0000$ \\
        \midrule
        gipsLDA classic vs gipsLDA weighted average & 20 & $0.3287$ \\
                                                    & 30 & $0.9999$ \\
                                                    & 51 & $1.0000$ \\
        \midrule
        QDAmod vs gipsQDA & 20 & $0.1302$ \\
                          & 30 & $0.0046$ \\
                          & 51 & $0.1852$ \\
        \midrule
        QDAmod vs gipsMultQDA & 20 & $0.0311$ \\
                              & 30 & $0.0004$ \\
                              & 51 & $0.0478$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison results for varying numbers of observations for Breast Cancer dataset.}
    \label{tab:stat_comparison_obs_breast}
\end{table}

\textbf{Analysis:}
The dual evaluation using Classification Accuracy and AUC highlights the efficiency of the \texttt{gipsLDA} models in data-scarce regimes ($n < 30$). Statistical tests confirm that \texttt{gipsLDA classic} significantly outperforms the regularized \texttt{LDAmod} baseline at small sample sizes ($n=20$ and $n=30$, with p-values $< 0.01$), achieving high discriminative power (AUC $> 0.90$) faster than the baselines. However, this linear advantage diminishes as the sample size increases to $n=51$ (p-value $> 0.05$).

The quadratic family benefits significantly from the symmetry constraints here. \texttt{gipsMultQDA} consistently outperforms \texttt{QDAmod} across all tested sample sizes (p-value $< 0.05$ for $n=20, 30, 51$), proving that the shared permutation constraint effectively regularizes the complex quadratic decision boundary even when linear models perform well. Finally, regarding the weighted approach, while \texttt{gipsLDA weighted average} shows extreme statistical significance at the very start ($n=20$, p-value $\approx 0$), it loses this advantage completely by $n=30$ (p-value $> 0.05$), reinforcing the visual observation that it plateaus \textbf{at a much lower accuracy} compared to the \texttt{classic} estimator as the dataset grows.

\subsection{Sensorless Drive Diagnosis Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/sensorless_plot}
    \caption{Classification accuracy on the Sensorless Drive Diagnosis dataset. The plot compares the performance of all six models as a function of training sample size.
    Note that the curves for \texttt{qda} and \texttt{gipsqda} overlap significantly.}
    \label{fig:sensorless}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/sensorless_plot_auc}
    \caption{Area Under the ROC Curve (AUC) on the Sensorless Drive Diagnosis dataset. The plot compares the performance of all six models as a function of training sample size.
    Note that the curves for \texttt{qda} and \texttt{gipsqda} overlap significantly.}
    \label{fig:sensor_auc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/sensorless_boxplot}
    \caption{Classification accuracy on the Sensorless Drive Diagnosis dataset. The boxplots compare the performance distribution of all six models.}
    \label{fig:sensor_box}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{Number of observations} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & 20 & $0.6566$ \\
                                  & 58 & $0.0349$ \\
                                  & 237 & $1.0000$ \\
        \midrule
        LDAmod vs gipsLDA weighted average & 20 & $1.0000$ \\
                                           & 58 & $0.0000$ \\
                                           & 237 & $0.0000$ \\
        \midrule
        gipsLDA classic vs gipsLDA weighted average & 20 & $1.0000$ \\
                                                    & 58 & $0.0000$ \\
                                                    & 237 & $0.0000$ \\
        \midrule
        QDAmod vs gipsQDA & 20 & $1.0000$ \\
                          & 58 & $0.5396$ \\
                          & 237 & $0.6516$ \\
        \midrule
        QDAmod vs gipsMultQDA & 20 & $1.0000$ \\
                              & 58 & $1.0000$ \\
                              & 237 & $0.0072$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison for Sensorless Drive Diagnosis dataset.}
\end{table}

\textbf{Analysis:}
The results for this dataset reveal a striking dichotomy between linear and quadratic classifiers, driven by the high dimensionality and the multi-class nature ($k=11$) of the problem. The data appears to possess an inherently linear decision boundary, as evidenced by the \texttt{LDA} family achieving accuracies above $0.90$, while all \texttt{QDA} variants fail to surpass $0.80$. This failure is attributed to the "parameter explosion"; estimating 11 distinct covariance matrices requires far more data than available, leading to fitting failures or extreme overfitting in the low-sample regime.

In the extreme data-scarce region ($n=20$), many models fail to fit entirely. However, \texttt{gipsLDA classic} and \texttt{LDAmod} demonstrate superior robustness, successfully learning a viable decision boundary where others fail. Notably, at the intermediate stage ($n=58$), \texttt{gipsLDA classic} achieves a statistically significant improvement over the baseline (p-value $= 0.0349$), confirming that the symmetry constraint aids in stabilizing the pooled covariance estimate before asymptotic convergence is reached.

While the linear models generally perform well, the \texttt{gipsLDA weighted average} estimator emerges as a standout performer when the Number of Observations is 237. Far from underperforming, it achieves exceptional accuracy, significantly outpacing both the baseline and the \texttt{classic} estimator (p-value $\approx 0$). This suggests that in this specific regime, the \texttt{gipsLDA weighted average} estimator effectively captures the underlying structure, acting as a "dark horse" that delivers superior predictive power compared to the standard pooled estimates.

\subsection{Credit Card Fraud Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/creditcard_plot}
    \caption{Classification accuracy on the Credit Card Fraud dataset. The plot compares the performance of all six models as a function of training sample size. Note that the curves for \texttt{qda}, \texttt{gipsqda}, and \texttt{gipsmultqda} overlap significantly.}
    \label{fig:creditcard}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/creditcard_plot_auc}
    \caption{Area Under the ROC Curve (AUC) on the Credit Card Fraud dataset. The plot compares the performance of all six models as a function of training sample size.
    Note that the curves for \texttt{qda}, \texttt{gipsqda}, and \texttt{gipsmultqda} overlap significantly.}
    \label{fig:creditcard_auc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/creditcard_boxplot}
    \caption{Classification accuracy on the Credit Card Fraud dataset. The boxplots compare the performance distribution of all six models.}
    \label{fig:creditcard_box}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{Number of observations} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & 20 & $< 0.0001$ \\
                                  & 41 & $< 0.0001$ \\
                                  & 109 & $0.1400$ \\
        \midrule
        LDAmod vs gipsLDA weighted average & 20 & $1.0000$ \\
                                           & 41 & $1.0000$ \\
                                           & 109 & $1.0000$ \\
        \midrule
        gipsLDA classic vs gipsLDA weighted average & 20 & $1.0000$ \\
                                                    & 41 & $1.0000$ \\
                                                    & 109 & $1.0000$ \\
        \midrule
        QDAmod vs gipsQDA & 20 & $0.5005$ \\
                          & 41 & $0.5005$ \\
                          & 109 & $0.5005$ \\
        \midrule
        QDAmod vs gipsMultQDA & 20 & $0.5005$ \\
                              & 41 & $0.5005$ \\
                              & 109 & $0.5005$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison results for varying numbers of observations for Credit Card Fraud dataset.}
\end{table}

\textbf{Analysis:}
The quadratic models (\texttt{qda}, \texttt{gipsqda}, and \texttt{gipsmultqda}) exhibit identical performance trajectories, with statistical tests (p-value $\approx 0.5$) confirming that \texttt{gips} regularization converged to the standard QDA solution, likely identifying the identity permutation as optimal. Within the linear family, \texttt{gipsLDA classic} significantly outperforms the baseline \texttt{LDAmod} in the early training phase ($n=41$, p-value $\approx 0$), indicating effective stabilization via pooled symmetry when data is scarce. This advantage diminishes at $n=109$ (p-value $= 0.1400$) as the baseline converges to the high performance level of the QDA variants. Conversely, the \texttt{gipsLDA weighted average} estimator stands out as a significant outlier, consistently underperforming compared to all other models and failing to converge to the optimal accuracy plateau.

\subsection{EEG Brainwave Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/emotions_plot}
    \caption{Classification accuracy on the EEG Brainwave dataset. The plot compares the performance of all six models as a function of training sample size.}
    \label{fig:emotions}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/emotions_plot_auc}
    \caption{Area Under the ROC Curve (AUC) on the EEG Brainwave dataset. The plot compares the performance of all six models as a function of training sample size. Note that the curves for \texttt{qda}, \texttt{gipsqda}, and \texttt{gipsmultqda} overlap significantly.}
    \label{fig:emotions_auc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/emotions_boxplot}
    \caption{Classification accuracy on the Credit EEG Brainwave dataset. The boxplots compare the performance distribution of all six models.}
    \label{fig:emotions_box}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{Number of observations} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & 20 & $0.0381$ \\
                                  & 38 & $0.1504$ \\
                                  & 90 & $0.5880$ \\
        \midrule
        LDAmod vs gipsLDA weighted average & 20 & $0.5510$ \\
                                           & 38 & $1.0000$ \\
                                           & 90 & $1.0000$ \\
        \midrule
        gipsLDA classic vs gipsLDA weighted average & 20 & $0.9655$ \\
                                                    & 38 & $1.0000$ \\
                                                    & 90 & $1.0000$ \\
        \midrule
        QDAmod vs gipsQDA & 20 & $0.7302$ \\
                          & 38 & $0.5097$ \\
                          & 90 & $0.4824$ \\
        \midrule
        QDAmod vs gipsMultQDA & 20 & $0.9310$ \\
                              & 38 & $0.4625$ \\
                              & 90 & $0.4776$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison for EEG Brainwave dataset.}
\end{table}

\textbf{Analysis:}
The results for the EEG Brainwave dataset reveal a distinct separation between linear and quadratic classifiers, with the linear family consistently achieving higher accuracy, suggesting a homoscedastic underlying structure. Within this favorable linear regime, \texttt{gipsLDA classic} provides a tangible benefit in the data-scarce region, achieving a statistically significant improvement over \texttt{LDAmod} at $n=20$ (p-value $= 0.0381$). However, as the sample size increases ($n=38, 90$), this advantage diminishes (p-value $> 0.05$) as the baseline model catches up and both linear models behave identically.

In contrast, all quadratic variants (\texttt{QDAmod}, \texttt{gipsQDA}, \texttt{gipsMultQDA}) exhibit nearly identical performance trajectories with no statistically significant differences, starting with lower accuracy and converging more slowly due to higher parameter overhead. Notably, the \texttt{gipsLDA weighted average} estimator stands out negatively; while it surpasses the struggling quadratic models at the very smallest sample sizes, it consistently underperforms compared to the rest of the linear family and fails to reach the optimal accuracy plateau.

\subsection{Steel Plates Faults Prediction}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/steel_plot}
    \caption{Classification accuracy on the Steel Plates Faults dataset. The plot compares the performance of all six models as a function of training sample size.
    Note that the curves for \texttt{qda}, \texttt{gipsqda}, and \texttt{gipsmultqda} overlap significantly.}
    \label{fig:steel}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/steel_plot_auc}
    \caption{Area Under the ROC Curve (AUC) on the Steel Plates Faults dataset. The plot compares the performance of all six models as a function of training sample size.
    Note that the curves for \texttt{qda}, \texttt{gipsqda}, and \texttt{gipsmultqda} overlap significantly.}
    \label{fig:steel_auc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/steel_boxplot}
    \caption{Classification accuracy on the Steel Plates Faults dataset. The boxplots compare the performance distribution of all six models.}
    \label{fig:steel_box}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{llc}
        \toprule
        \textbf{Comparison} & \textbf{Number of observations} & \textbf{p-value} \\
        \midrule
        LDAmod vs gipsLDA classic & 20 & $0.0502$ \\
                                  & 42 & $0.0209$ \\
                                  & 112 & $0.7386$ \\
        \midrule
        LDAmod vs gipsLDA weighted average & 20 & $1.0000$ \\
                                           & 42 & $1.0000$ \\
                                           & 112 & $1.0000$ \\
        \midrule
        gipsLDA classic vs gipsLDA weighted average & 20 & $1.0000$ \\
                                                    & 42 & $1.0000$ \\
                                                    & 112 & $1.0000$ \\
        \midrule
        QDAmod vs gipsQDA & 20 & $1.0000$ \\
                          & 42 & $0.5010$ \\
                          & 112 & $0.5005$ \\
        \midrule
        QDAmod vs gipsMultQDA & 20 & $1.0000$ \\
                              & 42 & $0.5010$ \\
                              & 112 & $0.5005$ \\
        \bottomrule
    \end{tabular}
    \caption{Statistical comparison for Steel Plates Faults dataset.}
\end{table}

\textbf{Analysis:}
In this multi-class industrial classification task, the performance dynamics shift notably as the Number of Observations increases. Initially, the linear models dominate, with \texttt{gipsLDA classic} demonstrating a tangible advantage over the standard baseline. The statistical analysis confirms this observation: \texttt{gipsLDA classic} achieves a statistically significant improvement over \texttt{LDAmod} when the Number of Observations is 42 (p-value $= 0.0209$) and sits on the threshold of significance when the Number of Observations is 20 (p-value $= 0.0502$). This indicates that the pooled symmetry constraint effectively stabilizes the decision boundary when data is limited.

Regarding the quadratic family, all variants (\texttt{QDAmod}, \texttt{gipsQDA}, \texttt{gipsMultQDA}) exhibit identical behavior, implying that the regularization identified the trivial symmetry as optimal. While these models struggle when the Number of Observations is small due to the parameter explosion inherent in 7-class classification, visual inspection reveals that they eventually surpass the linear models as data becomes abundant and the high variance of the estimator is overcome. Finally, the \texttt{gipsLDA weighted average} estimator occupies a distinct middle ground in the early phase—performing better than the struggling quadratic models but falling significantly behind the robust \texttt{gipsLDA classic} and \texttt{LDAmod} variants.

\section{Summary of Findings}
Based on the analysis of both the synthetic simulation sets and the diverse real-world benchmarks, we can synthesize the following key findings regarding the proposed \texttt{gipsDA} framework:

\begin{itemize}
    \item \textbf{Efficacy in High-Dimensional, Data-Scarce Regimes:}
    The synthetic experiments  demonstrate that imposing permutation symmetry acts as a form of model-based regularization to be reckoned with. In scenarios characterized by high dimensionality ($p=10$) and low sample sizes ($n \approx p$ or $n < p$), the \texttt{gips}-based models significantly outperformed the standard regularized baselines. Specifically, \texttt{gipsMultQDA} and \texttt{gipsQDA} consistently achieved p-values near zero when compared to \texttt{QDAmod}, suggesting that reducing the parameter space via symmetry constraints  mitigates the "curse of dimensionality" and prevents overfitting.

    \item \textbf{Superiority of the Classic Estimator for gipsLDA:}
    A recurrent pattern across both synthetic and real-world results (e.g., Credit Card, Steel Plates) is the performance of the \texttt{gipsLDA classic} estimator. This approach consistently outperformed \texttt{gipsLDA weighted average} variant. The latter often failed to converge to optimal accuracy in higher-dimensional or multi-class settings.

    \item \textbf{Data Efficiency in Real-World Applications:}
    The benchmarking on real-world datasets highlighted the "data efficiency" of the proposed models. In the Breast Cancer and Heart Failure datasets, \texttt{gipsLDA classic} and \texttt{gipsMultQDA} reached their asymptotic accuracy levels with significantly fewer training samples than their unstructured counterparts. This confirms that discovering and enforcing latent symmetries allows the models to learn the decision boundary faster, an advantage in domains where data acquisition is costly.

    \item \textbf{Robustness to Assumption Violations:}
    The experiments confirmed that the \texttt{gipsDA} framework remains effective even when theoretical assumptions are violated. For instance, in the Heart Failure dataset, where One-Hot Encoding introduced binary features (violating multivariate normality), the \texttt{gips}-based models still provided statistically significant improvements over the baselines. This suggests that the structural constraints imposed by \texttt{gips} are robust enough to capture useful signal even in non-Gaussian distributions.

    \item \textbf{Linear vs. Quadratic Reality Check:}
    Finally, the real-world benchmarks (specifically Steel Plates, Sensorless, and EEG) underscored a practical reality: many industrial datasets exhibit inherent linear separability. In these cases, while quadratic models (both standard and \texttt{gips}-based) struggled due to parameter complexity, the linear \texttt{gipsLDA} models maintained high performance.
\end{itemize}

% ======================================================================
%                          Conclusion
% ======================================================================

\chapter{Conclusion and Discussion}
\label{ch:conclusion}
This thesis set out to address the limitations of classical discriminant analysis in high-dimensional, data-scarce environments. By developing the \texttt{gipsDA} package, we  integrated the theory of permutation-invariant covariance estimation with standard classification workflows. This final chapter summarizes the broader implications of our approach, reflects on the engineering challenges encountered, and outlines potential avenues for future research.

\section{Reflection on the Approach}
\label{sec:reflection}

The core hypothesis of this work was that imposing permutation symmetry on the covariance matrix acts as a physically motivated form of regularization, superior to arbitrary shrinkage in settings where $p \gg n$. The empirical results presented in Chapter \ref{ch:results} do not reject this hypothesis.

\begin{itemize}
    \item \textbf{Symmetry as a Prior:} The success of \texttt{gipsQDA} and \texttt{gipsMultQDA} in synthetic scenarios confirms that discovering latent symmetries allows the model to drastically reduce the number of free parameters without discarding the covariance structure entirely. This suggests that "data-driven parsimony" is a viable alternative to sparsity or diagonal constraints.
    \item \textbf{The "Classic" vs. "Weighted" Dilemma:} One of the most significant findings was the underperformance of the Bayesian Model Averaging (weighted average) strategy in the homoscedastic \texttt{gipsLDA} model. The models are theoreticaly asymptoticaly equivalent the issue may be an external factor like numerical instability.
%    \item \textbf{Practical Utility:} The integration with the \texttt{MASS} library syntax proved to be a vital design decision. By lowering the barrier to entry, we ensured that the complex mathematical backend remains accessible to data scientists, bridging the gap between advanced statistical theory and practical application. % wiem, że w listingu to min 3 wyglądae dobrze ale to już jest na prawdę zeroinformatywna linijka na granicy dezinofrmacji powiedziałbym wręcz.
\end{itemize}

\section{Engineering Experience and Skills Acquired}
\label{sec:project_experience}

The development of \texttt{gipsDA} was not merely a statistical exercise but a rigorous software engineering project. This process provided the team with deep insights into the lifecycle of R package development and collaborative workflows.

\subsection{Technical Competence}
\begin{itemize}
    \item \textbf{Advanced R Development:} We gained profound expertise in the R ecosystem, particularly the S3 object-oriented system and namespace management. Extending the functionality of the established \texttt{MASS} library required a precise understanding of method dispatch and environment scoping to ensure compatibility without shadowing existing functions.
    \item \textbf{CRAN Compliance and Publishing Standards:} We mastered the rigorous process of preparing an R package for the Comprehensive R Archive Network (CRAN) \cite{CRAN}. This involved ensuring cross-platform compatibility, strictly resolving all \texttt{R CMD check} notes and warnings, and sanitizing the code to meet the community's high-quality standards for public distribution.
    \item \textbf{Mathematical Optimization:} Implementing the \texttt{gipsmult} backend bridged the gap between theory and code. We gained practical experience with Bayesian statistics, specifically implementing Metropolis-Hastings algorithms for discrete spaces and handling permutation group theory computationally.
    \item \textbf{Custom Serialization:} A major technical hurdle was data persistence. We learned that standard serialization (e.g., \texttt{saveRDS}) is insufficient for interoperability. Developing a custom JSON engine required mapping complex R objects—such as formulas and attributes—to text-based formats and implementing robust reconstruction logic.
\end{itemize}

\subsection{Software Engineering Practices}
\begin{itemize}
%    \item \textbf{Robust Testing Strategies:} Initially, our focus was on "happy path" testing. However, the usability analysis revealed the critical importance of "Negative Testing." Implementing robust error handling for invalid inputs (e.g., singular matrices, mismatched dimensions) significantly improved the package's stability and user experience.
    \item \textbf{Version Control Proficiency:} Managing a complex codebase with parallel development streams required strict adherence to version control best practices. We refined our skills in using Git and GitHub, mastering branching strategies, code reviews, and conflict resolution to maintain a clean project history.
    \item \textbf{Documentation as a First-Class Citizen:} Writing documentation concurrently with code development ensured that the final API was consistent and intuitive. This discipline prevented "feature creep" by keeping the development aligned with the functional requirements defined at the project's inception.
    \item \textbf{Collaborative Synergy:} The project highlighted that successful engineering extends beyond writing code. We learned to effectively divide responsibilities—balancing the backend mathematical logic with the user-facing API—and synchronized our efforts through constant communication.
\end{itemize}

\section{Future Work}
\label{sec:future_work}

While \texttt{gipsDA} provides a robust foundation for symmetry-based classification, several avenues remain open for further development and research.

\begin{itemize}
    \item \textbf{Integration of Conditional Independence:} Recent theoretical advancements have introduced a new class of models that combine permutation symmetry with graphical modeling, known as Colored Graphical Gaussian Models \cite{chojecki2026newclasscoloredgaussian}. Expanding \texttt{gipsDA} to incorporate these constraints would allow for the simultaneous discovery of symmetries and conditional independence structures (sparsity). This is particularly relevant for high-dimensional biological networks, where variables are often both exchangeable within pathways and sparsely connected across them.

    \item \textbf{Extension to Unsupervised Learning:} The principles of \texttt{gips} are currently applied to supervised classification. A natural extension would be to adapt permutation symmetry constraints to unsupervised models, such as Gaussian Mixture Models (GMM) or Model-Based Clustering. Constraining clusters to share internal symmetries could stabilize clustering in high-dimensional spaces.

    \item \textbf{Complex Symmetry Groups:} Currently, the library primarily optimizes for cyclic permutation subgroups. Expanding the backend to support more complex symmetry structures—such as block symmetries or direct products of groups—would allow the model to capture more intricate dependency patterns, such as those found in hierarchical biological data.

    \item \textbf{Computational Scalability via C++ Integration:} While the current implementation is efficient for moderate dimensions, searching the permutation space in extremely high-dimensional settings (e.g., genomic data where $p > 1000$) remains computationally intensive. A critical next step is to rewrite the bottleneck matrix operations using the \texttt{Rcpp} library to interface directly with C++, which would significantly reduce overhead and improve scalability.
\end{itemize}
% ======================================================================
%                          Rest
% ======================================================================

\chapter{Division of Work and AI Declaration}

\begin{table}[H]
        \centering
        \small
        % Zwiększenie odstępów między wierszami dla lepszej czytelności
        \renewcommand{\arraystretch}{1.1}

        \begin{tabularx}{\textwidth}{l X c c}
            \toprule
            \textbf{Category} & \textbf{Task / Responsibility} & \textbf{Antoni} & \textbf{Norbert} \\
            \midrule
            \textbf{Theory} & Mathematical derivation of the \texttt{gipsmult} posterior & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Package} & \texttt{gipsmult} module: Optimization engines (MH, BF) & \textcolor{teal}{$\checkmark$} & \textcolor{gray}{$\times$} \\
            \addlinespace
            \textbf{Package} & \texttt{models} module: S3 methods, API consistency & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Package} & Model serialization (JSON engine) & \textcolor{teal}{$\checkmark$} & \textcolor{gray}{$\times$} \\
            \addlinespace
            \textbf{Testing} & Unit testing framework and "Negative Testing" suite & \textcolor{gray}{$\times$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Experiments} & Synthetic data generation and benchmarking & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Experiments} & Real-world dataset benchmarking & \textcolor{gray}{$\times$} & \textcolor{teal}{$\checkmark$} \\
            \addlinespace
            \textbf{Documentation} & Technical reports, User's Manual, and Thesis Writing & \textcolor{teal}{$\checkmark$} & \textcolor{teal}{$\checkmark$} \\
            \bottomrule
        \end{tabularx}
        \caption{Summary of individual and shared contributions.}
    \end{table}

\noindent
It is important to note that our collaboration was highly interactive. We worked closely together, constantly cross-checking each other's work and frequently consulting on solutions. Often, one author would prepare the initial draft of a chapter, while the other would thoroughly review and edit it. This reciprocal approach characterized our entire workflow and cooperation.

\begin{table}[H]
    \centering
    \small
    \renewcommand{\arraystretch}{1.5}
    \begin{tabularx}{\textwidth}{ >{\raggedright\arraybackslash}p{6cm} X X }
        \toprule
        \textbf{Role} & \textbf{Norbert} \newline 327273 & \textbf{Antoni} \newline 327284 \\
        \midrule
        % --- WRITING DRAFT ---
        \textbf{Writing – original draft} &
        Chapters: \newline 1, 2, 5, 6, 7, 8 &
        Chapters: \newline 3, 4 \\
        \addlinespace[10pt]
        % --- WRITING REVIEW ---
        \textbf{Writing – review \& editing}  &
        Chapters: \newline 3, 4 &
        Chapters: \newline 1, 2, 5, 6, 7, 8 \\
        \bottomrule
    \end{tabularx}
    \caption{The contribution of the authors of the thesis (Writing section).}
\end{table}

\section*{The use of IT tools in this thesis}
\vspace{3pt}

\normalsize
We hereby declare that:
\begin{enumerate}
    \item We have not used IT tools to generate the content of the manuscript of this thesis.
    \item We have used IT tools to generate the code of the software developed for the thesis.
    \item We take full responsibility for all content in this thesis, including both the manuscript and the software developed for it.
\end{enumerate}

\begin{table}[H]
    \centering
    \label{tab:it_tools_scope}
    \begin{tabularx}{\textwidth}{|X|X|}
    \hline
    \textbf{The scope of the use of IT tools to generate manuscript} & \textbf{The scope of the use of IT tools to generate software code} \\ \hline
    None & Helping with Unit Tests for package \\ \hline
    \end{tabularx}
    \caption{Scope of IT tools usage used in the thesis}
\end{table}


% ------------------------------- BIBLIOGRAPHY ---------------------------
% LEXICOGRAPHICAL ORDER BY AUTHORS' LAST NAMES
% FOR AMBITIOUS ONES - USE BIBTEX


%\begin{thebibliography}{20} % IF YOU HAVE MORE REFERENCES, WRITE THE BIGGER NUMBER
%
%\bibitem[1]{Ktos} A. Author, \emph{Title of a book}, Publisher, year, page--page.
%\bibitem[2]{Innyktos} J. Bobkowski, S. Dobkowski, Title of an article, \emph{Magazine X, No. 7}, year, PAGE--PAGE.
%\bibitem[3]{B} C. Brink, Power structures, \emph{Algebra Universalis 30(2)}, 1993, 177--216.
%\bibitem[4]{H} F. Burris, H. P. Sankappanavar, \emph{A Course of Universal Algebra}, Springer-Verlag, New York, 1981.
%\end{thebibliography}

% Wybór stylu bibliografii (np. plain, abbrv, alpha, unsrt)
\bibliographystyle{plain}

% Wskazanie pliku z bazą danych (bez rozszerzenia .bib)
\bibliography{thesis-en}

\pagenumbering{gobble}
\thispagestyle{empty}

\appendix
\chapter{Derivation of a posteriori distribution in \texttt{gipsmult} model}
We examine m multidimensional gaussian samples $Z^{(1)}_{g},\dots Z_g^{(n_g)}$ under
set $\{\mathcal{K}_g=\kappa_g, \Gamma=c\} $ consisting of i.i.d. random
vectors $\mathcal{N}_p(0, \kappa_g^{-1})$.
Let $\Gamma$ be a discrete random variable uniformly distributed over the set of
$
\mathcal{C} := \{ \langle \sigma \rangle : \sigma \in \mathfrak{S}_p \}
$
cyclic subgroups of $\mathfrak{S}_p$. We assume that each $\mathcal{K}_g$ under set $\Gamma = c$ follows conjugate Diaconis-Ylvisacker a priori distribution \cite{diaconis1979conjugate} defined by its PDF:
\begin{equation}
    f_{\mathcal{K}_g \mid \Gamma = c}(k_g)
  = \frac{1}{I_{c}(\delta, D)}
    \operatorname{Det}(k_g)^{(\delta - 2)/2}
    e^{-\tfrac12 \operatorname{tr}[D^{-1} k_g] }
    \mathbf{1}_{\mathcal{P}_{c}}(k_g),
\end{equation}
where $\delta>1$ and $D\in \mathcal{P}_c$ are hyperparameters and $I_c(\delta, D)$ is a normalizing constant.
We start with standard bayesian reasoning linking the probability of any given permutation conditioned by data with probability of specific data conditioned by given permutation:
\begin{equation}
\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) =
\frac{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right) \mathbbm{P}\left(\Gamma = c, \mathbbm{Y} = \mathbbm{y} \right)}{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right)} \propto \mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right).
\end{equation},
phrasing this in more probabilistic terms and conditioning $\mathbbm{X}$ directly on precision matrices we get:
\begin{equation}
\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) \propto f_{\mathbbm{X} \; \mid \; \Gamma = c, \; \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) = \idotsint\limits_{\mathcal{P}_c^m}
    f_{\mathbbm{X} \mid \mathcal{K} = \kappa, \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x})\,
    f_{\mathcal{K} \mid \Gamma = c}(\kappa)
    \, d \kappa.
\end{equation}
Let us now tackle the first factor under the integral:
\begin{equation}
    \begin{align}
        f_{\mathbbm{X} \mid \mathcal{K} = \kappa, \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) = \prod_{i=i}^{n}(2\pi)^{-\frac{p}{2}}\left|K_{g_i}\right|^{\frac{1}{2}}
        \exp \left(-\frac{1}{2}x_i^T K_{g_i}x_i\right) \propto \\ \propto \prod_{g=1}^m\left|K_g\right|^{\frac{n_g}{2}}\exp\left(-\frac{1}{2}\sum_{g=1}^m\sum_{j=1}^{n_g}
        x_{g,j}^T K_g x_{g,j}\right),
    \end{align}
\end{equation}
as inside exponential we are summing real numbers we can replace them by their trace:
\begin{equation}
    \begin{align}
        \exp\left(-\frac{1}{2}\sum_{g=1}^m\sum_{j=1}^{n_g}
        x_{g,j}^T K_g x_{g,j}\right) = \exp\left(-\frac{1}{2}\sum_{g=1}^m\sum_{j=1}^{n_g}
        tr\left(x_{g,j}^T K_g x_{g,j}\right)\right) = \\ =
        \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_g\sum_{j=1}^{n_g}
        x_{g,j}x_{g,j}^T\right)\right) = \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_{g}U_g\right)\right),
    \end{align}
\end{equation}
where:
\begin{equation}
    U_g = \sum_{j=1}^{n_g}
        x_{g,j}x_{g,j}^T.
\end{equation}
As for the second factor we assume precision matrices are independent:
\begin{equation}
    f_{\mathcal{K} \mid \Gamma = c}(\kappa) = \prod_{g=1}^m f_{K_g \mid \Gamma = c} (k_g),
\end{equation}
Combining all of these we get:
\begin{equation}
    \begin{align}
        \idotsint\limits_{\mathcal{P}_c^m}
    f_{\mathbbm{X} \mid \mathcal{K} = \kappa, \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x})\,
    f_{\mathcal{K} \mid \Gamma = c}(\kappa)
    \, d \kappa = \\ =\idotsint\limits_{\mathcal{P}_c^m} \prod_{g=1}^m \left|K_g\right|^{\frac{n_g}{2}}
        \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_{g}U_g\right)\right) f_{K_g \mid \Gamma = c}(k_g) \, d \kappa = \\ =
        \prod_{g=1} ^m \int\limits_{\mathcal{P}_c} \left|K_g\right|^{\frac{n_g}{2}}
        \exp\left(-\frac{1}{2}\sum_{g=1}^mtr\left(K_{g}U_g\right)\right) f_{K_g \mid \Gamma = c}(k_g) \, dk_g.
    \end{align}
\end{equation}
Applying results from \cite{JSSv112i07} for each factor we arrive at the result:
\begin{equation}\label{UltimateAPosterioriFormula}
    \mathbbm{P}\left(\Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right) \propto
    f_{\mathbbm{X} \; \mid \; \Gamma = c, \; \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) \propto
    \prod_{g=1}^m\frac{I_c\left(\delta + n_g, D+U_g\right)}{I_c\left(\delta, D\right)}.
\end{equation}

\chapter{Baseline regularization strategy}
\label{ch:baseline_regularization}

\section{Mathematical Formulation}

The objective of this procedure is to transform the empirical covariance matrix $\mathbf{A}$ into a regularized matrix $\tilde{\mathbf{A}}$ such that its smallest eigenvalue, $\lambda_{\min}(\tilde{\mathbf{A}})$, is no less than a specific target threshold $\tau$. For the purposes of this study, we set $\tau = 0.05$.

If the smallest eigenvalue of the original matrix, denoted as $\lambda = \lambda_{\min}(\mathbf{A})$, already satisfies $\lambda \ge \tau$, no modification is performed. However, if $\lambda < \tau$, we apply a linear shrinkage towards the identity matrix $\mathbf{I}$. The regularized matrix is defined as:

\begin{equation}
    \tilde{\mathbf{A}} = \frac{\mathbf{A} + s\mathbf{I}}{1 + s},
\end{equation}
where $s$ is a non-negative scalar scaling factor.

\section{Derivation of the Scaling Factor}

The parameter $s$ is derived analytically to ensure the new smallest eigenvalue exactly matches the target $\tau$. The derivation proceeds as follows:

\begin{enumerate}
    \item Let the eigenvalues of $\mathbf{A}$ be denoted by $\lambda_i$. The smallest eigenvalue is $\lambda$.
    \item Adding a multiple of the identity matrix shifts the spectrum. The eigenvalues of $\mathbf{A} + s\mathbf{I}$ are $\lambda_i + s$. Consequently, the smallest eigenvalue becomes $\lambda + s$.
    \item Dividing by the scalar $(1+s)$ scales the eigenvalues. Thus, the smallest eigenvalue of the normalized matrix $\tilde{\mathbf{A}}$ is given by:
    \begin{equation}
        \lambda_{\min}(\tilde{\mathbf{A}}) = \frac{\lambda + s}{1 + s}.
    \end{equation}
\end{enumerate}

To enforce the condition $\lambda_{\min}(\tilde{\mathbf{A}}) = \tau$, we solve the following equation for $s$:

\begin{equation}
    \frac{\lambda + s}{1 + s} = \tau.
\end{equation}

Rearranging the terms:
\begin{align*}
    \lambda + s &= \tau(1 + s) \\
    \lambda + s &= \tau + \tau s \\
    s - \tau s &= \tau - \lambda \\
    s(1 - \tau) &= \tau - \lambda.
\end{align*}

This yields the closed-form solution for the scaling factor:
\begin{equation}
    s = \frac{\tau - \lambda}{1 - \tau}.
\end{equation}

This transformation aims to ensure that \texttt{LDAmod} and \texttt{QDAmod} remain numerically solvable even in high-dimensional scenarios where standard implementations would fail due to singularity, providing a robust baseline for benchmarking the performance of \texttt{gipsDA}.




% ----------------------- LIST OF SYMBOLS AND ABBREVIATIONS ------------------
%\chapter*{List of symbols and abbreviations}
%
%\begin{tabular}{cl}
%nzw. & nadzwyczajny \\
%* & star operator \\
%$\widetilde{}$ & tilde
%\end{tabular}
%\\
%If you don't need it, delete it.
%\thispagestyle{empty}


% ----------------------------  LIST OF FIGURES --------------------------------
\listoffigures
\thispagestyle{empty}


% -----------------------------  LIST OF TABLES --------------------------------
\renewcommand{\listtablename}{List of tables}
\listoftables
\thispagestyle{empty}

% -----------------------------  LIST OF APPENDICES ---------------------------
%\chapter*{List of appendices}
%\begin{enumerate}
%\item Appendix 1
%\item Appendix 2
%\item In case of no appendices, delete this part.
%\item texy probabilistyczne
%ogólny ogólnik
%\begin{equation*}
%\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) =
%\frac{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right)}{\mathbbm{P}\left(\mathbbm{X} = \mathbbm{x}, \mathbbm{Y} = \mathbbm{y}\right)} \propto \mathbbm{P}\left(\mathbbm{X} = \mathbbm{x} \mid \Gamma = c, \mathbbm{Y} = \mathbbm{y} \right)
%\end{equation*}
%wzór japońcowy
%\begin{equation*}
%I_c\left(\delta, D\right) = \int \limits_{P_c} |k|^{\frac{\delta - 2}{2}} \exp\left(-\frac{1}{2} Tr\left(Dk\right)\right) \; dk
%\end{equation*}
%powiązanie rozkładu z japańcem
%\begin{equation*}
%\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}\right) \propto \frac{I_c\left(\delta + n, D+U\right)}{I_c\left(\delta, D\right)}
%\end{equation*}
%uszczegółowienie ogólnego ogólnika (ogólnik)
%\begin{equation*}
%\mathbbm{P}\left( \Gamma = c \mid \mathbbm{X} = \mathbbm{x}, \mathbbm{Y}=\mathbbm{y}\right) \propto f_{\mathbbm{X} \; \mid \; \Gamma = c, \; \mathbbm{Y} = \mathbbm{y}}(\mathbbm{x}) = \overset{m}{\idotsint} f(\mathbbm{x})\, d\mathbbm{x}
%\end{equation*}

%\end{enumerate%}



\thispagestyle{empty}





\end{document}
